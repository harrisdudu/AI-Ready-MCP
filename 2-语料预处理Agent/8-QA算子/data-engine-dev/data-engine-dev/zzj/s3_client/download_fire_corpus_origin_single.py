import boto3
from botocore.exceptions import NoCredentialsError, ClientError
import os
import concurrent.futures
from concurrent.futures import ProcessPoolExecutor
import tqdm
import pandas as pd
import logging
import file_utils

# é…ç½®æ—¥å¿—ï¼Œç”¨äºè®°å½•å¤±è´¥çš„æ–‡ä»¶
logging.basicConfig(filename='download_failures.log', level=logging.ERROR,
                    format='%(asctime)s - %(levelname)s - %(message)s')

# æ”¯æŒçš„æ–‡ä»¶æ‰©å±•å
SUPPORTED_EXTENSIONS = ["pdf", "epub", "azw","azw3","mobi"]

class S3Util:
    def __init__(self, endpoint, ak, sk):
        self.endpoint = endpoint
        self.ak = ak
        self.sk = sk
        self.s3_client = self._create_s3_client()

    def _create_s3_client(self):
        """Create an S3 client."""
        return boto3.client(
            's3',
            endpoint_url=self.endpoint,
            aws_access_key_id=self.ak,
            aws_secret_access_key=self.sk
        )

    def download_file(self, bucket_name, object_key, download_dir):
        """Download a single file from S3 directly into the specified download directory."""
        filename = os.path.basename(object_key)
        download_path = os.path.join(download_dir, filename)

        # Create download directory if it does not exist
        os.makedirs(download_dir, exist_ok=True)

        try:
            response = self.s3_client.head_object(Bucket=bucket_name, Key=object_key)
            file_size = response['ContentLength']

            with tqdm.tqdm(total=file_size, unit='B', unit_scale=True, desc=f"Downloading {filename}", leave=False) as pbar:
                with open(download_path, 'wb') as file:
                    def progress_callback(bytes_downloaded):
                        pbar.update(bytes_downloaded)

                    self.s3_client.download_fileobj(bucket_name, object_key, file, Callback=progress_callback)
            return True, object_key, None  # æˆåŠŸ
        except Exception as e:
            error_msg = f"Failed to download {object_key}: {e}"
            print(error_msg)
            logging.error(error_msg)
            return False, object_key, str(e)  # å¤±è´¥

def worker(args):
    bucket_name, s3_path, download_dir, s3_util_params = args
    # s3_path = "P2025062300001_zlib_ä¸‹è½½æ•°æ®é›†/pilimi-zlib-11000000-11039999/11013680.mobi"
    endpoint, ak, sk = s3_util_params
    s3_util = S3Util(endpoint, ak, sk)
    results = []
    if s3_path.split(".")[-1] in SUPPORTED_EXTENSIONS:
        object_key = s3_path
        success, key, error = s3_util.download_file(bucket_name, object_key, download_dir)
        if success:
            return success, key, None  # æˆåŠŸä¸€ä¸ªå°±è¿”å›
        else:
            results.append((key, error))
    
    # å¦‚æœæ‰€æœ‰æ ¼å¼éƒ½å¤±è´¥äº†ï¼Œè¿”å›æœ€åä¸€ä¸ªé”™è¯¯
    return False, s3_path, results

if __name__ == "__main__":
    debug=False
    # S3 é…ç½®ä¿¡æ¯
    ENDPOINT = "http://172.20.90.11:8009"
    AK = "123456"
    SK = "inspuR12345"
    BUCKET_NAME = "corpus-origin"
    DOWNLOAD_DIR = "/mnt/data/zzj/fire_s3/data/500wä¹¦å•æå–/download"
    # DOWNLOAD_DIR = "download"
    origin_failed_path = "/mnt/data/zzj/fire_s3/data/origin_failed.jsonl"
    origin_successed_path = "/mnt/data/zzj/fire_s3/data/origin_successed.jsonl"
    os.makedirs(DOWNLOAD_DIR, exist_ok=True)

    file_path = "/mnt/data/zzj/fire_s3/data/500wä¹¦å•æå–.xlsx"
    df = pd.read_excel(file_path)
    arry_s3_path = df["s3_path"].tolist()
    arry_s3_path.sort()
    # arry_s3_path = df["s3_path"].dropna().astype(str)
    # filtered_s3_paths = arry_s3_path[arry_s3_path.str.endswith(tuple(SUPPORTED_EXTENSIONS))].tolist()

    s3_util_params = (ENDPOINT, AK, SK)

    tasks = [(BUCKET_NAME, s3_path.strip(), DOWNLOAD_DIR, s3_util_params) for s3_path in arry_s3_path]

    print(f"å¼€å§‹ä¸‹è½½ {len(arry_s3_path)} ä¸ª s3_path å¯¹åº”çš„æ–‡ä»¶ï¼ˆå¤šè¿›ç¨‹ï¼‰...")

    # with ProcessPoolExecutor(max_workers=8) as executor:
    #     results = list(tqdm.tqdm(executor.map(worker, tasks), total=len(tasks), desc="Overall Progress"))
    if debug:
        # å•æ¬¡è°ƒç”¨è°ƒè¯•æ¨¡å¼ - é€ä¸ªæ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
        print("ğŸ”§ è°ƒè¯•æ¨¡å¼ï¼šé€ä¸ªæ‰§è¡Œæ‰€æœ‰ä»»åŠ¡ï¼ˆå•çº¿ç¨‹ï¼‰")
        results = []
        for i, task in enumerate(tqdm.tqdm(tasks, desc="Debug Processing")):
            try:
                print(f"\nğŸ” å¤„ç†ä»»åŠ¡ {i+1}/{len(tasks)}: {task[1]}")
                result = worker(task)
                results.append(result)
                if result[0]:  # æˆåŠŸ
                    print(f"   âœ… æˆåŠŸä¸‹è½½: {result[1]}")
                else:  # å¤±è´¥
                    print(f"   âŒ ä¸‹è½½å¤±è´¥: {result[1]}, é”™è¯¯: {result[2]}")
            except Exception as e:
                print(f"   âš ï¸  ä»»åŠ¡å¼‚å¸¸: {e}")
                results.append((False, task[1], str(e)))
        
        # ç»Ÿè®¡è°ƒè¯•ç»“æœ
        failed_downloads = [res for res in results if not res[0]]
        print(f"\nğŸ”§ è°ƒè¯•å®Œæˆã€‚æ€»ä»»åŠ¡æ•°: {len(tasks)}, æˆåŠŸ: {len(results)-len(failed_downloads)}, å¤±è´¥: {len(failed_downloads)}")
        
        # ä¿å­˜å¤±è´¥ç»“æœ
        if failed_downloads:
            print("\nâŒ è°ƒè¯•æ¨¡å¼ä¸‹çš„å¤±è´¥æ–‡ä»¶ï¼š")
            for _, s3_path, errors in failed_downloads:
                print(f"  - s3_path: {s3_path}, Errors: {errors}")
            file_utils.save_to_jsonl(failed_downloads, origin_failed_path)
    else:
        # åˆ›å»ºæ€»ä½“è¿›åº¦æ¡
        with tqdm.tqdm(total=len(tasks), desc="Total Files Progress", unit="files") as total_pbar:
            def update_progress(future):
                """æ›´æ–°æ€»ä½“è¿›åº¦æ¡çš„å›è°ƒå‡½æ•°"""
                total_pbar.update(1)

            with ProcessPoolExecutor(max_workers=128) as executor:
                # æäº¤æ‰€æœ‰ä»»åŠ¡
                futures = [executor.submit(worker, task) for task in tasks]
                
                # ä¸ºæ¯ä¸ªfutureæ·»åŠ å®Œæˆå›è°ƒ
                for future in futures:
                    future.add_done_callback(lambda f: total_pbar.update(1))
                
                # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
                results = []
                for future in tqdm.tqdm(concurrent.futures.as_completed(futures), 
                                    total=len(futures), 
                                    desc="Processing Files", 
                                    leave=False):
                    try:
                        result = future.result()
                        results.append(result)
                    except Exception as e:
                        print(f"Task failed with exception: {e}")
                        results.append((False, "unknown", str(e)))

        failed_downloads = [res for res in results if not res[0]]
        successed_downloads = [res for res in results if res[0]]

        print(f"\nâœ… ä¸‹è½½å®Œæˆã€‚æ€»å…±å¤±è´¥æ–‡ä»¶æ•°: {len(failed_downloads)}")
        if failed_downloads:
            print("\nâŒ ä»¥ä¸‹æ–‡ä»¶ä¸‹è½½å¤±è´¥ï¼š")
            for _, s3_path, errors in failed_downloads:
                print(f"  - s3_path: {s3_path}, Errors: {errors}")
            file_utils.save_to_jsonl(failed_downloads,origin_failed_path)
        file_utils.save_to_jsonl(successed_downloads,origin_successed_path)