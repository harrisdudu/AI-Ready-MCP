# coding:utf-8
"""
æ‰¹é‡ Markdown è„±æ•å¤„ç†å™¨ï¼ˆå§“åç”±æ¨¡å‹æå–ï¼Œå·ç ç”±è§„åˆ™æå–ï¼‰
åŠŸèƒ½ï¼šåˆ†å—è°ƒç”¨å¤§æ¨¡å‹æå–å§“åï¼Œå…¨æ–‡æ­£åˆ™æå–å·ç ï¼Œåˆå¹¶åè„±æ•å†™å›
æ—¥å¿—ï¼šJSONL æ ¼å¼ï¼Œinput_path ä¸ºçœŸå®è·¯å¾„
æ‰€æœ‰å¤„ç†ä¿¡æ¯å‡æ‰“å°è¾“å‡º  
"""

import os
import glob
import asyncio
import multiprocessing
import queue
import json
import re
from typing import Any, Dict, Optional, Callable, Iterator, List, Tuple
import httpx
import uuid
from tenacity import retry, stop_after_attempt, wait_exponential
from volcenginesdkarkruntime import AsyncArk
from volcenginesdkarkruntime._constants import CLIENT_REQUEST_HEADER
from tqdm import tqdm
from datetime import datetime

# ==================== æ—¥å¿—å¤„ç†å‡½æ•° ====================

def load_existing_results_full(log_file: str) -> Dict[str, Dict]:
    results = {}
    if os.path.exists(log_file):
        with open(log_file, 'r', encoding='utf-8') as f:
            for line in f:
                try:
                    entry = json.loads(line.strip())
                    input_path = entry.get("input_path")
                    if input_path:
                        results[input_path] = entry
                except Exception as e:
                    print(f"[è­¦å‘Š] æ— æ³•è§£ææ—¥å¿—è¡Œ: {line.strip()[:100]}... é”™è¯¯: {e}")
    return results

def update_or_append_to_jsonl(log_file: str, result: Dict[str, Any], cache_dict: Dict[str, Dict]):
    input_path = result.get("input_path")
    if not input_path or input_path == "unknown":
        print(f"[è­¦å‘Š] å°è¯•è®°å½•æ—¥å¿—ä½†ç¼ºå¤±æœ‰æ•ˆ input_path: {result}")
        return

    cache_dict[input_path] = result

    temp_log_file = log_file + ".tmp"
    try:
        with open(temp_log_file, 'w', encoding='utf-8') as f:
            for entry in cache_dict.values():
                f.write(json.dumps(entry, ensure_ascii=False) + '\n')
        os.replace(temp_log_file, log_file)
    except Exception as e:
        print(f"[é”™è¯¯] å†™å…¥æ—¥å¿—æ–‡ä»¶å¤±è´¥: {e}")
        if os.path.exists(temp_log_file):
            try:
                os.remove(temp_log_file)
            except OSError as rm_e:
                print(f"[é”™è¯¯] åˆ é™¤ä¸´æ—¶æ—¥å¿—æ–‡ä»¶å¤±è´¥: {rm_e}")


# ==================== åˆ†å—å‡½æ•° ====================

def split_text_into_chunks(text: str, chunk_size: int = 4096) -> List[Tuple[int, int, str]]:
    chunks = []
    start = 0
    while start < len(text):
        end = start + chunk_size
        if end >= len(text):
            chunk = text[start:]
            chunks.append((start, len(text), chunk))
            break
        else:
            cut_point = end
            look_ahead = text[end - 50:end + 50]
            last_break = -1
            for sep in ['\n\n', '\n', 'ã€‚', ' ', '']:
                idx = look_ahead.rfind(sep)
                if idx != -1 and start + end - 50 + idx > start + 100:
                    last_break = start + end - 50 + idx + len(sep)
                    break
            if last_break == -1 or last_break <= start:
                last_break = end
            chunk = text[start:last_break]
            chunks.append((start, last_break, chunk))
            start = last_break
    return chunks


# ==================== è§„åˆ™æå–å‡½æ•° ====================

def extract_mobiles(content: str) -> List[str]:
    """æå–æ‰‹æœºå·"""
    return re.findall(r'1\d{10}', content)

def extract_phones(content: str) -> List[str]:
    """æå–å›ºè¯ï¼ˆåŒºå·-å·ç æ ¼å¼ï¼‰"""
    return re.findall(r'\d{3,4}-\d{7,8}', content)

def extract_id_cards(content: str) -> List[str]:
    """æå–èº«ä»½è¯å·"""
    return [id_num.upper() for id_num in re.findall(r'\d{17}[\dXx]', content)]


# ==================== è„±æ•å‡½æ•° ====================

def desensitize_content(original_content: str, sensitive_items: List[str]) -> str:
    desensitized_content = original_content
    sorted_items = sorted(set(sensitive_items), key=len, reverse=True)

    for item in sorted_items:
        if not item or item.isspace():
            continue
        escaped_item = re.escape(item)

        if re.fullmatch(r'1\d{10}', item):
            replacement = f"{item[:3]}****{item[-4:]}"
        elif re.fullmatch(r'\d{3,4}-\d{7,8}', item):
            replacement = "*******"
        elif re.fullmatch(r'\d{17}[\dXx]', item):
            replacement = f"{item[:6]}********{item[-4:]}"
        else:
            replacement = item[0] + "*" * (len(item) - 1) if len(item) > 2 else (item[0] + "*")

        desensitized_content = re.sub(escaped_item, replacement, desensitized_content)

    return desensitized_content


# ==================== æ–‡ä»¶è¯»å– ====================

def read_md_files(md_root: str, skip_paths: set, target_files: Optional[set] = None) -> Iterator[Tuple[str, str]]:
    files_to_scan = list(target_files) if target_files is not None else glob.glob(os.path.join(md_root, "**/*.md"), recursive=True)
    filtered_files = [f for f in files_to_scan if f not in skip_paths]

    for md_path in filtered_files:
        try:
            with open(md_path, "r", encoding="utf-8") as f:
                content = f.read()
            yield md_path, content
        except Exception as e:
            print(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥ï¼Œè·³è¿‡: {md_path}, é”™è¯¯: {e}")


# ==================== è¾“å…¥ç”Ÿæˆå™¨ï¼ˆä½¿ç”¨ custom_id ç¼–ç  chunk ä¿¡æ¯ï¼‰====================

def md_input_generator_for_desensitization(md_root_dir: str, existing_log_cache: Dict[str, Dict], target_files: Optional[set] = None):
    successfully_processed_paths = {
        path for path, entry in existing_log_cache.items()
        if entry.get("is_success", False)
    }

    for path, content in read_md_files(md_root_dir, successfully_processed_paths, target_files):
        chunks = split_text_into_chunks(content, chunk_size=4096)
        print(f"ğŸ“„ {path} -> é•¿åº¦: {len(content)} å­—ç¬¦ï¼Œåˆ†æˆ {len(chunks)} ä¸ª chunk")  # âœ… æ‰“å°å®Œæ•´è·¯å¾„

        for idx, (start, end, chunk) in enumerate(chunks):
            # ä½¿ç”¨ custom_id ç¼–ç ï¼šè·¯å¾„::chunk_ç´¢å¼•
            custom_id = f"{path}::chunk_{idx}"

            yield {
                "messages": [
                    {
                        "role": "system",
                        "content": """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£ä¿¡æ¯æå–ä¸“å®¶ï¼Œæ“…é•¿æ™ºèƒ½è¯†åˆ«æ–‡æ¡£ä¸­çš„å§“åç±»æ•æ„Ÿä¿¡æ¯ã€‚ä½ çš„ä»»åŠ¡æ˜¯å‡†ç¡®æå–ä¸èŒä½ç›¸å…³çš„ä¸ªäººå§“åï¼Œä»¥åŠç¬¦åˆå¸¸è§å§“åç‰¹å¾çš„æ™®é€šä¸ªäººå§“åï¼Œå¹¶ä»¥æŒ‡å®šçš„ JSON æ ¼å¼åˆ—è¡¨è¿”å›ã€‚"""
                    },
                    {
                        "role": "user",
                        "content": f"""
è¯·ä»”ç»†åˆ†æä»¥ä¸‹ Markdown å†…å®¹ç‰‡æ®µï¼Œæ ¹æ®è§„åˆ™æå–éœ€è¦è„±æ•çš„**å§“åç±»**æ•æ„Ÿä¿¡æ¯ã€‚

## æå–è§„åˆ™ï¼š
- åªæå–ï¼šæ™®é€šä¸ªäººå§“åï¼ˆ2-4ä¸ªæ±‰å­—ï¼‰å’Œ èŒä½ç›¸å…³å§“åï¼ˆå¦‚ï¼šç»„é•¿ã€å‰¯ç»„é•¿ã€è´Ÿè´£äººã€è”ç³»äººã€é¡¹ç›®ç»ç†ç­‰ï¼‰
- å¿…é¡»æå–çš„èŒä½ï¼šç»„é•¿ã€å‰¯ç»„é•¿ã€ç»„å‘˜ã€ä¸»ä»»ã€å‰¯ä¸»ä»»ã€æˆå‘˜ã€æ ¸å¿ƒæˆå‘˜ã€ç»åŠäººã€è´Ÿè´£äººã€æ‰§è¡Œäººã€è”ç³»äººã€å¯¹æ¥äººã€é¡¹ç›®ç»ç†ã€æŠ€æœ¯è´Ÿè´£äººç­‰
- ä¸æå–ï¼šæ‰‹æœºå·ã€ç”µè¯ã€èº«ä»½è¯å·ã€å…¬å¸åã€åœ°åã€å†å²äººç‰©ã€å…¬ä¼—äººç‰©

## è¾“å‡ºæ ¼å¼ï¼š
**åªè¿”å›ä¸€ä¸ª JSON æ•°ç»„ï¼Œä»…åŒ…å«å§“åå­—ç¬¦ä¸²ã€‚ä¸è¦åŒ…å«ä»»ä½•å…¶ä»–å†…å®¹ã€‚**
ä¾‹å¦‚ï¼š["å¼ ä¸‰", "æå››"]

åŸå§‹å†…å®¹ç‰‡æ®µï¼ˆæ–‡ä»¶ {os.path.basename(path)} çš„ç¬¬ {idx+1} å—ï¼‰ï¼š
---
{chunk}
---
è¯·å¼€å§‹æå–å§“åï¼š
"""
                    }
                ],
                "thinking":{
                    "type": "disabled", # ä¸ä½¿ç”¨æ·±åº¦æ€è€ƒèƒ½åŠ›
                    # "type": "enabled", # ä½¿ç”¨æ·±åº¦æ€è€ƒèƒ½åŠ›
                    # "type": "auto", # æ¨¡å‹è‡ªè¡Œåˆ¤æ–­æ˜¯å¦ä½¿ç”¨æ·±åº¦æ€è€ƒèƒ½åŠ›
                },
                "temperature": 0.2,
                "input_path": path,
                "custom_id": custom_id,  # âœ… å…³é”®ï¼šchunk ä¿¡æ¯ç¼–ç åœ¨ custom_id ä¸­
                "extra_headers": {
                    CLIENT_REQUEST_HEADER: str(uuid.uuid4())
                }
            }


# ==================== æ‰¹å¤„ç†æ¡†æ¶ï¼ˆé€ä¼  custom_idï¼‰====================

class DoubaoBatchProcessor:
    def __init__(
        self,
        input_generator_func: Callable[..., Iterator[Dict[str, Any]]],
        input_generator_args: Dict[str, Any],
        num_workers: int = 4,
        max_concurrency_per_process: int = 32,
        model: str = "doubao-pro-32k",
        api_key: Optional[str] = None,
    ):
        self.input_generator_func = input_generator_func
        self.input_generator_args = input_generator_args
        self.num_workers = num_workers if num_workers > 0 else 1
        self.max_concurrency_per_process = max_concurrency_per_process
        self.model = model
        if api_key is None:
            api_key = os.getenv("ARK_API_KEY")
            if not api_key:
                raise ValueError("è¯·è®¾ç½® ARK_API_KEY ç¯å¢ƒå˜é‡")
        self.api_key = api_key

    def run(self, output_handler: Callable[[Dict[str, Any]], None]) -> None:
        print("ğŸš€ å¯åŠ¨ Doubao æ‰¹é‡å¤„ç†å™¨")
        print(f"   å·¥ä½œè¿›ç¨‹æ•°: {self.num_workers}")
        print(f"   æ¯è¿›ç¨‹å¹¶å‘æ•°: {self.max_concurrency_per_process}")

        manager = multiprocessing.Manager()
        in_queue: multiprocessing.Queue[Optional[Dict[str, Any]]] = manager.Queue(maxsize=1024)
        out_queue: multiprocessing.Queue[Optional[Dict[str, Any]]] = manager.Queue(maxsize=1024)

        processes = []

        p_in_args = (self.input_generator_func, self.input_generator_args, in_queue, self.num_workers)
        p_in = multiprocessing.Process(target=self._input_producer, args=p_in_args)
        p_in.start()
        processes.append(p_in)

        for i in range(self.num_workers):
            p = multiprocessing.Process(
                target=self._worker_process,
                args=(i, self.max_concurrency_per_process, self.api_key, in_queue, out_queue),
            )
            p.start()
            processes.append(p)

        finished_workers = 0
        try:
            while finished_workers < self.num_workers:
                try:
                    result = out_queue.get(timeout=600)
                    if result is None:
                        finished_workers += 1
                        continue
                    output_handler(result)
                except (queue.Empty, EOFError):
                    alive_workers = sum(1 for p in processes[1:] if p.is_alive())
                    if alive_workers == 0:
                        break
        except KeyboardInterrupt:
            print("\nğŸ›‘ ç”¨æˆ·ä¸­æ–­")
        finally:
            print("â³ æ­£åœ¨æ¸…ç†å’Œå…³é—­è¿›ç¨‹...")
            for p in processes:
                if p.is_alive():
                    p.terminate()
            for p in processes:
                p.join(timeout=5)
            print("âœ… æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼")

    @staticmethod
    def _input_producer(generator_func, generator_args, in_queue, num_workers):
        try:
            records = list(generator_func(**generator_args))
            print(f"ğŸ“‹ æ€»å…±éœ€è¦å¤„ç† {len(records)} ä¸ª chunk ä»»åŠ¡")
            for record in tqdm(records, desc="åˆ†å‘ä»»åŠ¡"):
                try:
                    in_queue.put(record, block=True, timeout=300)
                except Exception as e:
                    print(f"âŒ ä»»åŠ¡å…¥é˜Ÿå¤±è´¥: {e}")
                    break
        except Exception as e:
            print(f"âŒ è¾“å…¥ç”Ÿäº§è€…å‡ºé”™: {e}")
        finally:
            for _ in range(num_workers):
                in_queue.put(None)

    def _worker_process(self, worker_id, max_concurrency, api_key, in_queue, out_queue):
        try:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            client = AsyncArk(api_key=api_key, http_client=self._make_client(max_concurrency), timeout=7200)

            async def work():
                sem = asyncio.Semaphore(max_concurrency)
                tasks = []
                while True:
                    try:
                        record = await asyncio.to_thread(in_queue.get, block=True, timeout=60)
                        if record is None:
                            break
                        if "model" not in record:
                            record["model"] = self.model
                        await sem.acquire()
                        task = loop.create_task(self._process_task(client, record, out_queue, sem))
                        tasks.append(task)
                    except (queue.Empty, EOFError):
                        break
                if tasks:
                    await asyncio.gather(*tasks, return_exceptions=True)
                await client._client.aclose()
            loop.run_until_complete(work())
        except Exception as e:
            print(f"âŒ Worker {worker_id} å¼‚å¸¸: {e}")
        finally:
            out_queue.put(None)

    @staticmethod
    async def _process_task(client, record, out_queue, sem):
        api_params = record.copy()
        input_path = api_params.pop("input_path", "unknown")
        custom_id = api_params.pop("custom_id", "")  # âœ… ä¿ç•™ custom_id
        extra_headers = api_params.pop("extra_headers", {})

        try:
            response = await client.chat.completions.create(**api_params, extra_headers=extra_headers)
            result_dict = response.to_dict()
            result_dict["input_path"] = input_path
            result_dict["custom_id"] = custom_id  # âœ… ç¡®ä¿ä¼ å›
            result_dict["extra_headers"] = extra_headers
            await asyncio.to_thread(out_queue.put, result_dict)
        except Exception as e:
            error_result = {
                "error": str(e),
                "input_path": input_path,
                "custom_id": custom_id,
                "extra_headers": extra_headers,
                "input": record.get("messages", []),
            }
            await asyncio.to_thread(out_queue.put, error_result)
        finally:
            sem.release()

    @staticmethod
    def _make_client(max_concurrency):
        return httpx.AsyncClient(
            limits=httpx.Limits(max_connections=max_concurrency, max_keepalive_connections=max_concurrency),
            timeout=httpx.Timeout(7200.0)
        )


# ==================== ç»“æœå¤„ç†å™¨ï¼ˆä» custom_id è§£æ chunk_indexï¼‰====================

class ResultHandler:
    def __init__(self, log_file_path: str, log_cache: Dict, progress_bar: tqdm):
        self.log_file_path = log_file_path
        self.log_cache = log_cache
        self.progress_bar = progress_bar
        self.file_name_to_chunks: Dict[str, List[List[str]]] = {}
        self.file_name_to_full_content: Dict[str, str] = {}

    def handle(self, result: Dict[str, Any]):
        input_path = result.get("input_path", "unknown")
        custom_id = result.get("custom_id", "unknown::chunk_unknown")

        # è§£æ custom_id è·å– chunk_index
        if "::chunk_" in custom_id:
            try:
                _, chunk_index = custom_id.rsplit("::", 1)
            except:
                chunk_index = "unknown"
        else:
            chunk_index = "0"

        is_success = False
        error_msg = ""
        extracted_names = []

        if input_path == "unknown":
            error_msg = "æ— æ•ˆ input_path"
        elif "error" in result:
            error_msg = str(result['error'])
        else:
            try:
                resp = result.get("choices", [{}])[0].get("message", {}).get("content", "").strip()
                if resp.startswith("```json"):
                    resp = resp[7:-3].strip()
                names = json.loads(resp)
                if isinstance(names, list):
                    extracted_names = [n.strip() for n in names if isinstance(n, str) and 2 <= len(n.strip()) <= 10]
                # ç¼“å­˜åˆ°åŸå§‹è·¯å¾„
                if input_path not in self.file_name_to_chunks:
                    self.file_name_to_chunks[input_path] = []
                self.file_name_to_chunks[input_path].append(extracted_names)
                is_success = True
            except Exception as e:
                error_msg = f"è§£ææ¨¡å‹å“åº”å¤±è´¥: {e}, åŸå§‹: {resp[:150]}..."

        # è®°å½• chunk æ—¥å¿—
        log_entry = {
            "input_path": input_path,  # âœ… å®Œæ•´è·¯å¾„
            "chunk_index": chunk_index,
            "is_success": is_success,
            "error_message": error_msg,
            "extracted_names_count": len(extracted_names),
        }
        update_or_append_to_jsonl(self.log_file_path, log_entry, self.log_cache)

        # æ›´æ–°è¿›åº¦æ¡
        if self.progress_bar:
            self.progress_bar.update(1)

        # âœ… æ‰“å°å¤„ç†çŠ¶æ€
        status = "âœ…" if is_success else "âŒ"
        print(f"{status} [{input_path}][å—{chunk_index}] å§“åæ•°: {len(extracted_names)}{f' | é”™è¯¯: {error_msg}' if error_msg else ''}")

        # å°è¯•åˆå¹¶
        self._try_merge_file(input_path)

    def _try_merge_file(self, input_path: str):
        if input_path not in self.file_name_to_chunks:
            return

        if input_path not in self.file_name_to_full_content:
            try:
                with open(input_path, "r", encoding="utf-8") as f:
                    self.file_name_to_full_content[input_path] = f.read()
                print(f"ğŸ“„ å·²åŠ è½½å…¨æ–‡: {input_path}")
            except Exception as e:
                print(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥: {input_path}, é”™è¯¯: {e}")
                return

        full_content = self.file_name_to_full_content[input_path]

        # âœ… åˆ†åˆ«æå–æ‰‹æœºå·ã€å›ºè¯ã€èº«ä»½è¯
        extracted_mobiles = list(set(extract_mobiles(full_content)))
        extracted_phones = list(set(extract_phones(full_content)))
        extracted_id_cards = list(set(extract_id_cards(full_content)))

        # æ¨¡å‹æå–çš„å§“åï¼ˆå·²ç¼“å­˜ï¼‰
        model_names = [name for chunk in self.file_name_to_chunks[input_path] for name in chunk]
        extracted_names = list(set(model_names))  # å»é‡

        # åˆå¹¶æ‰€æœ‰æ•æ„Ÿé¡¹ç”¨äºè„±æ•
        all_sensitive = list(set(extracted_names + extracted_mobiles + extracted_phones + extracted_id_cards))

        try:
            desensitized = desensitize_content(full_content, all_sensitive)
            with open(input_path, "w", encoding="utf-8") as f:
                f.write(desensitized)

            # âœ… æ„å»ºæœ€ç»ˆæ—¥å¿—ï¼ŒåŒ…å«è¯¦ç»†æå–ä¿¡æ¯
            final_log = {
                "input_path": input_path,  # âœ… å®Œæ•´è·¯å¾„
                "is_success": True,
                "error_message": "",
                "total_sensitive_count": len(all_sensitive),
                "extracted_names_count": len(extracted_names),
                "rule_based_count": len(extracted_mobiles) + len(extracted_phones) + len(extracted_id_cards),
                "extracted_names": extracted_names,           # âœ… æ¨¡å‹æå–çš„å§“å
                "extracted_mobiles": extracted_mobiles,       # âœ… æ‰‹æœºå·
                "extracted_phones": extracted_phones,         # âœ… å›ºè¯
                "extracted_id_cards": extracted_id_cards,     # âœ… èº«ä»½è¯
                "timestamp": datetime.now().isoformat(),      # å¯é€‰ï¼šåŠ æ—¶é—´æˆ³
            }
            update_or_append_to_jsonl(self.log_file_path, final_log, self.log_cache)
            print(f"ğŸ“„ã€å®Œæˆã€‘{input_path} è„±æ•å†™å›")
            print(f"   å§“å: {extracted_names}")
            print(f"   æ‰‹æœºå·: {extracted_mobiles}")
            print(f"   å›ºè¯: {extracted_phones}")
            print(f"   èº«ä»½è¯: {extracted_id_cards}")

        except Exception as e:
            print(f"âŒ å†™å…¥å¤±è´¥: {input_path}, é”™è¯¯: {e}")


# ==================== ä¸»ç¨‹åºå…¥å£ ====================

if __name__ == "__main__":
    multiprocessing.set_start_method("spawn", force=True)

    # ==================== é…ç½®ä¿¡æ¯ ====================
    DEBUG_MODE = False  # âœ… æ§åˆ¶æ˜¯å¦æ‰“å°è¯¦ç»†æ—¥å¿—
    DEBUG_COUNT = 10   # DEBUG æ¨¡å¼ä¸‹å¤„ç†çš„æ–‡ä»¶æ•°ä¸Šé™
    MD_ROOT_DIR = "/mnt/data/zzj/data_clean_fire/data/output"
    NUM_WORKERS = 32
    MAX_CONCURRENCY_PER_PROCESS = 64
    LOG_FILE_PATH = os.path.join(MD_ROOT_DIR, "desensitize_results.jsonl")
    MODEL_NAME = "doubao-seed-1-6-flash-250715"

    print(f"ğŸ“‚ å¤„ç†ç›®å½•: {MD_ROOT_DIR}")
    if not os.path.isdir(MD_ROOT_DIR):
        print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {MD_ROOT_DIR}")
        exit(1)

    log_cache = load_existing_results_full(LOG_FILE_PATH)
    all_md_files = set(glob.glob(os.path.join(MD_ROOT_DIR, "**/*.md"), recursive=True))
    done_paths = {p for p, e in log_cache.items() if e.get("is_success", False)}
    todo_files = all_md_files - done_paths

    print(f"ğŸ“š å·²åŠ è½½ {len(log_cache)} æ¡å†å²æ—¥å¿—")
    print(f"âœ… å·²æˆåŠŸå¤„ç†: {len(done_paths)} ä¸ªæ–‡ä»¶")
    print(f"ğŸ“Œ å¾…å¤„ç†: {len(todo_files)} ä¸ªæ–‡ä»¶")

    # âœ… DEBUG æ¨¡å¼ä¸‹åªå¤„ç†å‰ N ä¸ªæ–‡ä»¶
    if DEBUG_MODE:
        todo_files = set(sorted(list(todo_files))[:DEBUG_COUNT])
        print(f"ğŸ DEBUG æ¨¡å¼: åªå¤„ç†å‰ {len(todo_files)} ä¸ªæ–‡ä»¶")

    if not todo_files:
        print("ğŸ‰ æ‰€æœ‰æ–‡ä»¶å‡å·²å¤„ç†å®Œæ¯•ï¼Œæ— éœ€æ‰§è¡Œã€‚")
        exit(0)

    with tqdm(total=len(todo_files), desc="å¤„ç†è¿›åº¦", unit="æ–‡ä»¶") as pbar:
        handler = ResultHandler(LOG_FILE_PATH, log_cache, pbar)
        processor = DoubaoBatchProcessor(
            input_generator_func=md_input_generator_for_desensitization,
            input_generator_args={
                "md_root_dir": MD_ROOT_DIR,
                "existing_log_cache": log_cache,
                "target_files": todo_files
            },
            num_workers=NUM_WORKERS,
            max_concurrency_per_process=MAX_CONCURRENCY_PER_PROCESS,
            model=MODEL_NAME,
            api_key=os.getenv("ARK_API_KEY")
        )
        processor.run(output_handler=handler.handle)

    print(f"âœ… æ‰¹é‡è„±æ•å¤„ç†å®Œæˆï¼æ—¥å¿—å·²ä¿å­˜è‡³: {LOG_FILE_PATH}")