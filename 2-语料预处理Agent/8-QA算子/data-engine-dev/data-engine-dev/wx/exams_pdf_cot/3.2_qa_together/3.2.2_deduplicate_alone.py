#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import json
import time
import argparse
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import threading
from tqdm import tqdm
import pandas as pd

try:
    from sentence_transformers import SentenceTransformer
except Exception:
    raise SystemExit("âŒ è¯·å…ˆå®‰è£… sentence-transformers: pip install sentence-transformers")

# ========= é…ç½®å¸¸é‡ =========
DEFAULT_EMBEDDING_MODEL = "/home/wangxi/workspace/gongye/yijizaojia/Qwen3-Embedding-0.6B"  # æœ¬åœ°æ¨¡å‹è·¯å¾„
DEFAULT_SIMILARITY_THRESHOLD = 0.99  # ä½™å¼¦ç›¸ä¼¼åº¦é˜ˆå€¼

class EmbeddingDeduplicator:
    def __init__(self, model_name: str = DEFAULT_EMBEDDING_MODEL, 
                 similarity_threshold: float = DEFAULT_SIMILARITY_THRESHOLD):
        # æ”¯æŒæœ¬åœ°æ¨¡å‹è·¯å¾„
        if Path(model_name).exists():
            print(f"ğŸ” åŠ è½½æœ¬åœ°embeddingæ¨¡å‹ï¼š{model_name}")
            self.model = SentenceTransformer(model_name)
        else:
            print(f"ğŸ” ä¸‹è½½embeddingæ¨¡å‹ï¼š{model_name}")
            self.model = SentenceTransformer(model_name)
        self.threshold = similarity_threshold
        
    def deduplicate_batch(self, questions_data: List[Dict[str, Any]]) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:
        """
        æ‰¹é‡å»é‡å¤„ç†
        è¿”å›ï¼š(å»é‡åçš„æ•°æ®, é‡å¤ä¿¡æ¯åˆ—è¡¨)
        """
        # æå–é—®é¢˜æ–‡æœ¬
        questions = [item.get('question', '') for item in questions_data]
        
        # è¿‡æ»¤ç©ºé—®é¢˜
        valid_indices = [i for i, q in enumerate(questions) if q.strip()]
        valid_questions = [questions[i] for i in valid_indices]
        valid_data = [questions_data[i] for i in valid_indices]
        
        if not valid_questions:
            return [], []
        
        print(f"ğŸ”„ è®¡ç®—embeddingå‘é‡...")
        # æ‰¹é‡è®¡ç®—embedding
        embeddings = self.model.encode(valid_questions, normalize_embeddings=True)
        
        print(f"ğŸ”„ è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ...")
        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        similarity_matrix = embeddings @ embeddings.T
        
        # è®¾ç½®å¯¹è§’çº¿ä¸º0ï¼ˆé¿å…è‡ªå·±ä¸è‡ªå·±æ¯”è¾ƒï¼‰
        np.fill_diagonal(similarity_matrix, 0)
        
        print(f"ğŸ”„ æ‰§è¡Œå»é‡...")
        # å»é‡å¤„ç†
        unique_indices = []
        duplicate_info = []
        processed = set()
        
        for i in range(len(valid_questions)):
            if i in processed:
                continue
                
            # æ‰¾åˆ°ä¸å½“å‰é—®é¢˜ç›¸ä¼¼çš„æ‰€æœ‰é—®é¢˜
            similar_indices = np.where(similarity_matrix[i] >= self.threshold)[0]
            
            if len(similar_indices) > 0:
                # æœ‰é‡å¤ï¼Œè®°å½•é‡å¤ä¿¡æ¯
                current_question = valid_questions[i]
                current_data = valid_data[i]
                
                for similar_idx in similar_indices:
                    if similar_idx not in processed:
                        duplicate_question = valid_questions[similar_idx]
                        duplicate_data = valid_data[similar_idx]
                        similarity = float(similarity_matrix[i][similar_idx])
                        
                        duplicate_info.append({
                            'current_question': current_question,
                            'current_data': current_data,
                            'duplicate_question': duplicate_question,
                            'duplicate_data': duplicate_data,
                            'similarity': similarity,
                            'current_index': valid_indices[i],
                            'duplicate_index': valid_indices[similar_idx]
                        })
                        
                        processed.add(similar_idx)
                
                # å°†å½“å‰é—®é¢˜åŠ å…¥å”¯ä¸€åˆ—è¡¨
                unique_indices.append(i)
                processed.add(i)
            else:
                # æ— é‡å¤ï¼Œç›´æ¥åŠ å…¥å”¯ä¸€åˆ—è¡¨
                unique_indices.append(i)
                processed.add(i)
        
        # è¿”å›å»é‡åçš„æ•°æ®
        unique_data = [valid_data[i] for i in unique_indices]
        
        return unique_data, duplicate_info

def load_jsonl(file_path: str) -> List[Dict[str, Any]]:
    """åŠ è½½JSONLæ–‡ä»¶"""
    data = []
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if line:
                data.append(json.loads(line))
    return data

def save_jsonl(data: List[Dict[str, Any]], file_path: str):
    """ä¿å­˜æ•°æ®åˆ°JSONLæ–‡ä»¶"""
    with open(file_path, 'w', encoding='utf-8') as f:
        for item in data:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')

def main():
    parser = argparse.ArgumentParser(description='å¯¹JSONLæ–‡ä»¶ä¸­çš„é—®é¢˜è¿›è¡Œå»é‡')
    parser.add_argument('--input', type=str, 
                       default='/home/wangxi/workspace/gongye/zejun/out_qas_t/combined_questions.jsonl',
                       help='è¾“å…¥JSONLæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output', type=str, 
                       default='/home/wangxi/workspace/gongye/zejun/out_qas_t/deduplicated_questions.jsonl',
                       help='è¾“å‡ºJSONLæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--stats', type=str, 
                       default='/home/wangxi/workspace/gongye/zejun/out_qas_t/deduplication_stats.json',
                       help='ç»Ÿè®¡ä¿¡æ¯è¾“å‡ºæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--model', type=str, 
                       default=DEFAULT_EMBEDDING_MODEL,
                       help='Embeddingæ¨¡å‹è·¯å¾„æˆ–åç§°')
    parser.add_argument('--threshold', type=float, 
                       default=DEFAULT_SIMILARITY_THRESHOLD,
                       help='ç›¸ä¼¼åº¦é˜ˆå€¼')
    
    args = parser.parse_args()
    
    print(f"ğŸ“– åŠ è½½è¾“å…¥æ–‡ä»¶ï¼š{args.input}")
    input_data = load_jsonl(args.input)
    print(f"ğŸ“Š æ€»é¢˜ç›®æ•°é‡ï¼š{len(input_data)}")
    
    # åˆå§‹åŒ–å»é‡å™¨
    deduplicator = EmbeddingDeduplicator(
        model_name=args.model,
        similarity_threshold=args.threshold
    )
    
    # æ‰¹é‡å»é‡å¤„ç†
    start_time = time.time()
    unique_questions, duplicate_info = deduplicator.deduplicate_batch(input_data)
    end_time = time.time()
    
    # ä¿å­˜å»é‡åçš„æ•°æ®
    print(f"ğŸ’¾ ä¿å­˜å»é‡åçš„æ•°æ®ï¼š{args.output}")
    save_jsonl(unique_questions, args.output)
    
    # ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
    stats = {
        'input_total': len(input_data),
        'output_unique': len(unique_questions),
        'duplicates_found': len(duplicate_info),
        'deduplication_rate': len(duplicate_info) / len(input_data) * 100,
        'similarity_threshold': args.threshold,
        'embedding_model': args.model,
        'processing_time': end_time - start_time,
        'duplicate_details': duplicate_info
    }
    
    # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
    print(f"ğŸ“ˆ ä¿å­˜ç»Ÿè®¡ä¿¡æ¯ï¼š{args.stats}")
    with open(args.stats, 'w', encoding='utf-8') as f:
        json.dump(stats, f, ensure_ascii=False, indent=2)
    
    # æ‰“å°ç»Ÿè®¡æ‘˜è¦
    print("\n" + "="*50)
    print("ğŸ“Š å»é‡ç»Ÿè®¡æ‘˜è¦")
    print("="*50)
    print(f"è¾“å…¥é¢˜ç›®æ€»æ•°ï¼š{stats['input_total']}")
    print(f"å»é‡åé¢˜ç›®æ•°ï¼š{stats['output_unique']}")
    print(f"å‘ç°é‡å¤é¢˜ç›®ï¼š{stats['duplicates_found']}")
    print(f"å»é‡ç‡ï¼š{stats['deduplication_rate']:.2f}%")
    print(f"ç›¸ä¼¼åº¦é˜ˆå€¼ï¼š{stats['similarity_threshold']}")
    print(f"Embeddingæ¨¡å‹ï¼š{stats['embedding_model']}")
    print(f"å¤„ç†æ—¶é—´ï¼š{stats['processing_time']:.2f}ç§’")
    
    # ç›¸ä¼¼åº¦åˆ†å¸ƒç»Ÿè®¡
    if duplicate_info:
        similarities = [info['similarity'] for info in duplicate_info]
        print(f"\nç›¸ä¼¼åº¦åˆ†å¸ƒï¼š")
        print(f"  æœ€é«˜ç›¸ä¼¼åº¦ï¼š{max(similarities):.4f}")
        print(f"  æœ€ä½ç›¸ä¼¼åº¦ï¼š{min(similarities):.4f}")
        print(f"  å¹³å‡ç›¸ä¼¼åº¦ï¼š{np.mean(similarities):.4f}")
        print(f"  ä¸­ä½æ•°ç›¸ä¼¼åº¦ï¼š{np.median(similarities):.4f}")
    
    print(f"\nâœ… å»é‡å®Œæˆï¼")
    print(f"  å»é‡åæ•°æ®ï¼š{args.output}")
    print(f"  ç»Ÿè®¡ä¿¡æ¯ï¼š{args.stats}")

if __name__ == "__main__":
    main()
