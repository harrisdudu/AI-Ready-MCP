#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import json
import time
import argparse
import random
from dataclasses import dataclass, asdict
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

from tqdm import tqdm
import pandas as pd
import concurrent.futures as cf

# ========= Ark é…ç½® =========
API_KEY  = "c702676e-a69f-4ff0-a672-718d0d4723ed"
MODEL_ID = "doubao-seed-1-6-250615"

try:
    from volcenginesdkarkruntime import Ark
except Exception:
    raise SystemExit("âŒ è¯·å…ˆå®‰è£… Ark SDK: pip install 'volcengine-python-sdk[ark]'")

try:
    import sentence_transformers
    from sentence_transformers import SentenceTransformer
except Exception:
    raise SystemExit("âŒ è¯·å…ˆå®‰è£… sentence-transformers: pip install sentence-transformers")

# ========= å‚æ•°å¸¸é‡ =========
DEFAULT_WINDOW_LINES = 80    # ç¨å¾®ä¿å®ˆï¼Œå‡å°‘é•¿çª—è¶…æ—¶
DEFAULT_STRIDE_LINES = 40
SUPPORTED_TYPES = {"single", "multiple", "judge", "fill"}

# å•æ¬¡è¯·æ±‚æœ€å¤§å­—ç¬¦é¢„ç®—ï¼ˆé¿å…è¶…å¤§çª—å£å¯¼è‡´è¶…æ—¶/è¶…é•¿ï¼‰
MAX_CHARS_PER_PROMPT = 9000
# Ark é»˜è®¤è¯·æ±‚è¶…æ—¶ï¼ˆç§’ï¼‰
DEFAULT_ARK_TIMEOUT = 120

# Embedding é…ç½®
DEFAULT_EMBEDDING_MODEL = "/home/wangxi/workspace/gongye/yijizaojia/Qwen3-Embedding-0.6B"  # æœ¬åœ°æ¨¡å‹è·¯å¾„
DEFAULT_SIMILARITY_THRESHOLD = 0.85  # ä½™å¼¦ç›¸ä¼¼åº¦é˜ˆå€¼

# ========= System/User Prompt =========
SYSTEM_PROMPT = """\
ä½ æ˜¯ä¸€ä¸ªç²¾ç¡®çš„è¯•é¢˜æŠ½å–åŠ©æ‰‹ã€‚
æˆ‘å°†ç»™ä½ ä¸€æ®µä»è€ƒè¯• PDF è½¬æˆ Markdown çš„æ–‡æœ¬ç‰‡æ®µã€‚
ä½ çš„ä»»åŠ¡ï¼šåªæŠ½å–å››ç§ç±»å‹çš„å®Œæ•´è¯•é¢˜ï¼š
- singleï¼ˆå•é€‰é¢˜ï¼‰
- multipleï¼ˆå¤šé€‰é¢˜ï¼‰
- judgeï¼ˆåˆ¤æ–­é¢˜ï¼‰
- fillï¼ˆå¡«ç©ºé¢˜ï¼‰

è§„åˆ™ï¼š
1) æ¯é“é¢˜å¿…é¡»åŒ…å«ï¼š
   - "id"ï¼šåœ¨å½“å‰ç‰‡æ®µå†…çš„å±€éƒ¨é€’å¢ç¼–å·ï¼ˆå­—ç¬¦ä¸²æˆ–æ•°å­—ï¼‰
   - "type"ï¼šå–å€¼åªèƒ½æ˜¯ ["single","multiple","judge","fill"]
   - "question"ï¼šé¢˜å¹²çš„å®Œæ•´æ–‡å­—ï¼ˆä¿ç•™é¢˜å¹²ä¸­çš„å›¾ç‰‡é“¾æ¥æˆ–å…¬å¼ï¼‰å’Œé€‰é¡¹åˆ—è¡¨ï¼ˆæŒ‰åŸé¡ºåºï¼‰
   - "answer"ï¼šç®€æ´çš„ç­”æ¡ˆæ–‡å­—ï¼›é€‰æ‹©é¢˜ç”¨å­—æ¯ï¼ˆå¦‚ "A" æˆ– "ACD"ï¼‰ï¼›åˆ¤æ–­é¢˜ç”¨â€œå¯¹/é”™â€æˆ–â€œTrue/Falseâ€ï¼›å¡«ç©ºé¢˜ç”¨å®é™…å¡«ç©ºå†…å®¹ï¼ˆå¦‚æœ‰ï¼‰
   - "explanation"ï¼šè§£æè¯´æ˜ï¼ˆå¦‚æ— åˆ™ä¸ºç©ºå­—ç¬¦ä¸²ï¼‰
   - "knowledge_points"ï¼šçŸ¥è¯†ç‚¹æ ‡ç­¾ï¼ˆå¦‚æ— åˆ™ä¸ºç©ºå­—ç¬¦ä¸²ï¼‰
2) åªè¾“å‡ºå®Œæ•´çš„è¯•é¢˜ï¼šå¿…é¡»æœ‰å®Œæ•´çš„é—®é¢˜å’Œç­”æ¡ˆã€‚
3) å¦‚æœé¢˜ç›®è¢«æˆªæ–­æˆ–ä¸å®Œæ•´ï¼Œåˆ™ä¸è¦è¾“å‡ºã€‚
4) ä¸¥æ ¼è¾“å‡º JSON æ ¼å¼ï¼ŒUTF-8 ç¼–ç ï¼Œä¸è¦åŒ…å« Markdownã€ä»£ç å—å›´æ æˆ–å¤šä½™æ–‡å­—ã€‚
5) ä¸è¦ç¼–é€ å†…å®¹ã€‚ç­”æ¡ˆã€è§£ææˆ–çŸ¥è¯†ç‚¹ç¼ºå¤±æ—¶ç”¨ç©ºå­—ç¬¦ä¸²è¡¨ç¤ºã€‚
6) å¦‚æœç‰‡æ®µä¸­æ²¡æœ‰ç¬¦åˆæ¡ä»¶çš„è¯•é¢˜ï¼Œè¿”å›ç©ºæ•°ç»„ []ã€‚

è¾“å‡ºæ ¼å¼ï¼š
JSON æ•°ç»„ï¼Œæ•°ç»„ä¸­æ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªè¯•é¢˜å¯¹è±¡ã€‚
"""

CHUNK_PROMPT_TEMPLATE = """\
ä¸‹é¢æ˜¯å…¨æ–‡çš„ç¬¬ {start_line} è¡Œåˆ°ç¬¬ {end_line} è¡Œçš„ Markdown å†…å®¹ã€‚
è¯·ä»…æå–å…¶ä¸­ç¬¦åˆè¦æ±‚ä¸”å®Œæ•´çš„è¯•é¢˜ï¼ˆé¢˜å‹é™å®šåœ¨ single/multiple/judge/fillï¼‰ã€‚

--- å¼€å§‹ç‰‡æ®µ ---
{chunk_text}
--- ç»“æŸç‰‡æ®µ ---

è¯·è®°ä½ï¼š
- åªä¿ç•™ ["single","multiple","judge","fill"] è¿™å››ç§é¢˜å‹ã€‚
- åªè¿”å›ä¸¥æ ¼ JSON æ•°ç»„ï¼Œä¸è¦è¾“å‡ºä»»ä½•é¢å¤–æ–‡å­—ã€‚
"""

# ========= æ•°æ®ç»“æ„ =========
@dataclass
class QAItem:
    qid: Optional[str]
    type: str
    question: str
    answer: str
    explanation: str
    knowledge_points: str
    source_window: Tuple[int, int]
    window_local_id: Optional[str]
    source_file: str = ""

@dataclass
class ExtractStats:
    windows: int = 0
    raw_questions: int = 0
    kept_questions: int = 0
    dedup_dropped: int = 0
    embedding_dedup_dropped: int = 0  # embeddingç›¸ä¼¼åº¦å»é‡ä¸¢å¼ƒçš„æ•°é‡

@dataclass
class DedupLog:
    """å»é‡æ—¥å¿—è®°å½•"""
    timestamp: str
    source_file: str
    window_range: str
    question_text: str
    dedup_type: str  # "md5" æˆ– "embedding"
    reason: str  # å»é‡åŸå› æè¿°
    similarity_score: Optional[float] = None  # ä»…embeddingå»é‡æ—¶æœ‰å€¼
    duplicate_with: Optional[str] = None  # ä¸å“ªä¸ªé¢˜ç›®é‡å¤ï¼ˆé¢˜ç›®é¢„è§ˆï¼‰

# ========= Ark è°ƒç”¨ =========
def call_ark(prompt: str, api_key: str, model_id: str,
             temperature: float = 0.0, top_p: float = 0.9,
             timeout: int = DEFAULT_ARK_TIMEOUT) -> str:
    time.sleep(random.uniform(0.05, 0.20))  # è½»å¾®æŠ–åŠ¨
    client = Ark(api_key=api_key, timeout=timeout)
    resp = client.chat.completions.create(
        model=model_id,
        messages=[
            {"role":"system","content":SYSTEM_PROMPT},
            {"role":"user","content":prompt},
        ],
        temperature=temperature,
        top_p=top_p,
    )
    try:
        return resp.choices[0].message.content
    except Exception:
        return str(resp)

# ========= JSON è§£æ + æ¸…æ´— =========
_HEX = set("0123456789abcdefABCDEF")

def _fix_invalid_backslashes(s: str) -> str:
    """
    ä¿®å¤æ¨¡å‹è¿”å›ä¸­çš„éæ³•åæ–œæ ï¼šå¯¹éåˆæ³•è½¬ä¹‰å‰çš„åæ–œæ è¿›è¡ŒäºŒæ¬¡è½¬ä¹‰ã€‚
    ä»…ç”¨äºè®© JSON å¯è¢«è§£æï¼Œä¸æ”¹åŠ¨å…¶ä»–å†…å®¹ã€‚
    """
    out = []
    i = 0
    n = len(s)
    while i < n:
        ch = s[i]
        if ch != '\\':
            out.append(ch)
            i += 1
            continue
        # ch is backslash
        if i + 1 >= n:
            out.append('\\\\'); i += 1; continue
        nxt = s[i+1]
        if nxt in '"\\/bfnrt':
            out.append('\\' + nxt); i += 2; continue
        if nxt == 'u':
            # \uXXXX
            if i + 5 < n and all(c in _HEX for c in s[i+2:i+6]):
                out.append(s[i:i+6]); i += 6; continue
            else:
                out.append('\\\\u'); i += 2; continue
        # éæ³•è½¬ä¹‰ -> åŒåæ–œæ 
        out.append('\\\\' + nxt)
        i += 2
    return ''.join(out)

def _strip_code_fences(s: str) -> str:
    s = s.strip()
    fences = ("```json", "```JSON", "```", "~~~json", "~~~JSON", "~~~")
    for f in fences:
        if s.startswith(f):
            s = s[len(f):].strip()
        if s.endswith(f):
            s = s[:-len(f)].strip()
    return s

def force_json_load_with_sanitize(s: str) -> Tuple[Any, bool]:
    """
    è¿”å› (obj, sanitized_used)
    å…ˆæŒ‰å¸¸è§„è·¯å¾„è§£æï¼›å¤±è´¥åï¼š
      - æˆªå–é¦–ä¸ª [ åˆ°æœ€åä¸€ä¸ª ] çš„æ ¸å¿ƒ
      - å»ä»£ç å›´æ 
      - ä¿®å¤éæ³•åæ–œæ 
    """
    s0 = s.strip()
    try:
        if s0.startswith('[') and s0.endswith(']'):
            return json.loads(s0), False
    except Exception:
        pass

    first = s0.find('['); last = s0.rfind(']')
    core = s0 if (first == -1 or last == -1 or first >= last) else s0[first:last+1]
    core = _strip_code_fences(core)

    # ç¬¬ä¸€æ¬¡å°è¯•
    try:
        return json.loads(core), False
    except Exception:
        pass

    # æ¸…æ´—éæ³• \
    fixed = _fix_invalid_backslashes(core)
    return json.loads(fixed), True  # å¦‚æœä»å¼‚å¸¸ï¼Œä¼šæŠ›ç»™ä¸Šå±‚

# ========= å·¥å…· =========
def norm_text(s: str) -> str:
    return " ".join(s.replace("\u3000", " ").replace("\r", "").split())

def type_filter(qtype: str) -> bool:
    return qtype in SUPPORTED_TYPES

def hash_key(question: str) -> str:
    import hashlib
    base = norm_text(question)
    return hashlib.md5(base.encode("utf-8")).hexdigest()

# ========= å»é‡æ—¥å¿—è®°å½•å™¨ =========
class DedupLogger:
    def __init__(self, log_file: Path):
        self.log_file = log_file
        self.log_file.parent.mkdir(parents=True, exist_ok=True)
        self.lock = threading.Lock()
        
    def log_dedup(self, dedup_log: DedupLog):
        """è®°å½•å»é‡æ—¥å¿—"""
        with self.lock:
            with self.log_file.open("a", encoding="utf-8") as f:
                f.write(json.dumps(asdict(dedup_log), ensure_ascii=False) + "\n")
    
    def log_md5_dedup(self, source_file: str, window_range: str, question: str, 
                     duplicate_with: str):
        """è®°å½•MD5å»é‡"""
        dedup_log = DedupLog(
            timestamp=time.strftime("%Y-%m-%d %H:%M:%S"),
            source_file=source_file,
            window_range=window_range,
            question_text=question[:200] + "..." if len(question) > 200 else question,
            dedup_type="md5",
            reason="MD5å“ˆå¸Œå€¼é‡å¤",
            duplicate_with=duplicate_with[:200] + "..." if len(duplicate_with) > 200 else duplicate_with
        )
        self.log_dedup(dedup_log)
    
    def log_embedding_dedup(self, source_file: str, window_range: str, question: str,
                          duplicate_with: str, similarity_score: float):
        """è®°å½•embeddingå»é‡"""
        # ç¡®ä¿similarity_scoreæ˜¯PythonåŸç”Ÿfloatç±»å‹
        if hasattr(similarity_score, 'item'):
            similarity_score = float(similarity_score.item())
        else:
            similarity_score = float(similarity_score)
            
        dedup_log = DedupLog(
            timestamp=time.strftime("%Y-%m-%d %H:%M:%S"),
            source_file=source_file,
            window_range=window_range,
            question_text=question[:200] + "..." if len(question) > 200 else question,
            dedup_type="embedding",
            reason=f"embeddingç›¸ä¼¼åº¦ {similarity_score:.4f} è¶…è¿‡é˜ˆå€¼",
            similarity_score=similarity_score,
            duplicate_with=duplicate_with[:200] + "..." if len(duplicate_with) > 200 else duplicate_with
        )
        self.log_dedup(dedup_log)

# ========= Embedding ç›¸å…³å·¥å…· ========
import threading
import time

class EmbeddingDeduplicator:
    def __init__(self, model_name: str = DEFAULT_EMBEDDING_MODEL, 
                 similarity_threshold: float = DEFAULT_SIMILARITY_THRESHOLD):
        # æ”¯æŒæœ¬åœ°æ¨¡å‹è·¯å¾„
        if Path(model_name).exists():
            print(f"ğŸ” åŠ è½½æœ¬åœ°embeddingæ¨¡å‹ï¼š{model_name}")
            self.model = SentenceTransformer(model_name)
        else:
            print(f"ğŸ” ä¸‹è½½embeddingæ¨¡å‹ï¼š{model_name}")
            self.model = SentenceTransformer(model_name)
        self.threshold = similarity_threshold
        self.embeddings = []
        self.questions = []
        self.lock = threading.Lock()  # çº¿ç¨‹é”
        
    def add_question(self, question: str) -> Tuple[bool, Optional[Tuple[str, float]]]:
        """
        æ·»åŠ é—®é¢˜ï¼Œå¦‚æœä¸å·²æœ‰é—®é¢˜ç›¸ä¼¼åº¦è¶…è¿‡é˜ˆå€¼åˆ™è¿”å›(False, (é‡å¤é¢˜ç›®, ç›¸ä¼¼åº¦))
        å¦åˆ™è¿”å›(True, None)ï¼ˆè¡¨ç¤ºæ–°å¢ï¼‰
        """
        with self.lock:  # çº¿ç¨‹å®‰å…¨
            if not self.questions:
                # ç¬¬ä¸€ä¸ªé—®é¢˜ï¼Œç›´æ¥æ·»åŠ 
                embedding = self.model.encode([question])[0]
                self.embeddings.append(embedding)
                self.questions.append(question)
                return True, None
                
            # è®¡ç®—ä¸æ‰€æœ‰å·²æœ‰é—®é¢˜çš„ç›¸ä¼¼åº¦
            new_embedding = self.model.encode([question])[0]
            similarities = cosine_similarity([new_embedding], self.embeddings)[0]
            
            # å¦‚æœæœ€å¤§ç›¸ä¼¼åº¦è¶…è¿‡é˜ˆå€¼ï¼Œè®¤ä¸ºæ˜¯é‡å¤
            max_similarity_idx = np.argmax(similarities)
            max_similarity = similarities[max_similarity_idx]
            
            if max_similarity >= self.threshold:
                duplicate_question = self.questions[max_similarity_idx]
                return False, (duplicate_question, max_similarity)
                
            # ä¸é‡å¤ï¼Œæ·»åŠ åˆ°åˆ—è¡¨
            self.embeddings.append(new_embedding)
            self.questions.append(question)
            return True, None

def make_windows(lines: List[str], window: int, stride: int) -> List[Tuple[int, int, str]]:
    n = len(lines); out = []; i = 0
    while i < n:
        s = i; e = min(i + window - 1, n - 1)
        out.append((s+1, e+1, "\n".join(lines[s:e+1])))
        if e == n - 1: break
        i += stride
    return out

def compress_chunk_text(s: str, max_chars: int = MAX_CHARS_PER_PROMPT) -> str:
    lines = s.replace("\r", "").splitlines()
    out = []; blank = 0
    for ln in lines:
        ln = " ".join(ln.split())
        if not ln:
            blank += 1
            if blank > 1: continue
        else:
            blank = 0
        out.append(ln)
    s2 = "\n".join(out).strip()
    return s2[:max_chars] if len(s2) > max_chars else s2

# ========= Debug è½ç›˜ =========
def write_text(path: Path, content: str):
    path.parent.mkdir(parents=True, exist_ok=True)
    with path.open("w", encoding="utf-8") as f:
        f.write(content)

# ========= å•çª—å£è‡ªé€‚åº”è°ƒç”¨ï¼ˆå¸¦æ—¥å¿—ï¼‰ =========
def send_chunk_adaptive(s_line: int, e_line: int, chunk_text: str,
                        max_retries: int, retry_backoff: float, timeout: int,
                        debug_dir: Optional[Path], base_norm: str):
    """
    è¿”å›ï¼šraw_out, data(list), meta(dict)
    meta: {prompt_len, response_len, parse_error, sanitized_used, compressed_chunk}
    """
    compressed = compress_chunk_text(chunk_text, MAX_CHARS_PER_PROMPT)
    prompt = CHUNK_PROMPT_TEMPLATE.format(
        start_line=s_line, end_line=e_line, chunk_text=compressed
    )
    meta = {
        "prompt_len": len(prompt), "response_len": 0,
        "parse_error": "", "sanitized_used": False,
        "compressed_chunk": compressed
    }

    # è°ƒè¯•ç›®å½•
    wdir = None
    if debug_dir:
        wdir = debug_dir / base_norm / f"{s_line}-{e_line}"
        write_text(wdir / "prompt.txt", prompt)  # é€ç»™æ¨¡å‹çš„å®Œæ•´ç”¨æˆ·æ¶ˆæ¯ï¼ˆå«ç‰‡æ®µï¼‰

    # ç›´æ¥é‡è¯•
    attempt = 0
    last_err = None
    while attempt <= max_retries:
        try:
            raw = call_ark(prompt, api_key=API_KEY, model_id=MODEL_ID, timeout=timeout)
            if debug_dir:
                write_text(wdir / "response.txt", raw)
            meta["response_len"] = len(raw)
            obj, sanitized_used = force_json_load_with_sanitize(raw)
            meta["sanitized_used"] = sanitized_used
            if debug_dir and sanitized_used:
                # ä¿å­˜æ¸…æ´—åçš„ JSON æ–‡æœ¬
                write_text(wdir / "sanitized.json", json.dumps(obj, ensure_ascii=False, indent=2))
            return raw, obj, meta
        except Exception as e:
            last_err = str(e)
            meta["parse_error"] = last_err
            if debug_dir:
                write_text(wdir / "parse_error.txt", last_err)
            if attempt == max_retries:
                break
            time.sleep((retry_backoff ** attempt) + random.uniform(0.1, 0.5))
            attempt += 1

    # å®åœ¨å¤±è´¥
    return f"<<FAILED {s_line}-{e_line}>> {last_err}", [], meta

# ========= ä¿å­˜åŸæ–‡ç‰‡æ®µå¯¹ç…§ =========
def save_raw_chunks(raw_records: List[Dict[str, Any]], path: Path):
    path.parent.mkdir(parents=True, exist_ok=True)
    with path.open("w", encoding="utf-8") as f:
        for rec in raw_records:
            f.write(json.dumps(rec, ensure_ascii=False) + "\n")

# ========= æŠ½å–å•æ–‡ä»¶ =========
def extract_from_md(md_path: Path, window: int, stride: int,
                    max_retries: int, retry_backoff: float, timeout: int,
                    source_file_stem: str,
                    debug_dir: Optional[Path],
                    embedding_dedup: Optional[EmbeddingDeduplicator] = None,
                    dedup_logger: Optional[DedupLogger] = None) -> Tuple[List[QAItem], ExtractStats, List[Dict[str, Any]]]:
    text = md_path.read_text(encoding="utf-8", errors="ignore")
    lines = text.splitlines()
    windows = make_windows(lines, window=window, stride=stride)

    all_items: List[QAItem] = []
    seen = set()
    seen_questions = {}  # è®°å½•å·²è§è¿‡çš„é¢˜ç›®ï¼Œç”¨äºå»é‡æ—¥å¿—
    stats = ExtractStats(windows=len(windows))
    raw_records: List[Dict[str, Any]] = []

    pbar = tqdm(total=len(windows), desc=f"æŠ½å– {md_path.name}", ncols=100)
    for (s_line, e_line, chunk) in windows:
        raw_out, data, meta = send_chunk_adaptive(
            s_line=s_line, e_line=e_line, chunk_text=chunk,
            max_retries=max_retries, retry_backoff=retry_backoff, timeout=timeout,
            debug_dir=debug_dir, base_norm=source_file_stem
        )

        raw_records.append({
            "window": f"{s_line}-{e_line}",
            "chunk_text": chunk,
            "compressed_chunk": meta["compressed_chunk"],
            "prompt_length": meta["prompt_len"],
            "model_output": raw_out,
            "response_length": meta["response_len"],
            "sanitized_used": meta["sanitized_used"],
            "parse_error": meta["parse_error"],
            "parsed_questions": data
        })

        stats.raw_questions += len(data)
        for obj in data:
            question = norm_text(str(obj.get("question") or ""))
            answer = norm_text(str(obj.get("answer") or ""))
            explanation = norm_text(str(obj.get("explanation") or ""))
            knowledge = norm_text(str(obj.get("knowledge_points") or ""))
            qtype = norm_text(str(obj.get("type") or ""))

            # é¢˜å‹è¿‡æ»¤
            if not qtype or not type_filter(qtype):
                continue

            # ======= å…³é”®æ”¹åŠ¨ï¼šç­”æ¡ˆä¸ºç©ºç›´æ¥å‰”é™¤ =======
            if not answer:
                continue
            # =======================================

            # hash å»é‡
            key = hash_key(question)
            if key in seen:
                stats.dedup_dropped += 1
                # è®°å½•MD5å»é‡æ—¥å¿—
                if dedup_logger:
                    window_range = f"{s_line}-{e_line}"
                    duplicate_question = seen_questions.get(key, "æœªçŸ¥é¢˜ç›®")
                    dedup_logger.log_md5_dedup(source_file_stem, window_range, question, duplicate_question)
                continue
            seen.add(key)
            seen_questions[key] = question

            # Embedding ç›¸ä¼¼åº¦å»é‡
            if embedding_dedup is not None:
                is_unique, duplicate_info = embedding_dedup.add_question(question)
                if not is_unique:
                    stats.embedding_dedup_dropped += 1
                    # è®°å½•embeddingå»é‡æ—¥å¿—
                    if dedup_logger:
                        window_range = f"{s_line}-{e_line}"
                        duplicate_question, similarity_score = duplicate_info
                        dedup_logger.log_embedding_dedup(source_file_stem, window_range, question, 
                                                       duplicate_question, similarity_score)
                    continue

            all_items.append(QAItem(
                qid=None, type=qtype, question=question, answer=answer,
                explanation=explanation, knowledge_points=knowledge,
                source_window=(s_line, e_line), window_local_id=str(obj.get("id") or ""),
                source_file=source_file_stem
            ))
            stats.kept_questions += 1

        pbar.update(1)
    pbar.close()

    for i, q in enumerate(all_items, start=1):
        q.qid = f"{source_file_stem}__Q{i:05d}"
    return all_items, stats, raw_records

# ========= IO è¾…åŠ© =========
def save_items_per_file(items: List[QAItem], out_dir: Path, base: str):
    out_dir_file = out_dir / base
    out_dir_file.mkdir(parents=True, exist_ok=True)
    jsonl_path = out_dir_file / "questions.jsonl"
    with jsonl_path.open("w", encoding="utf-8") as f:
        for it in items:
            f.write(json.dumps(asdict(it), ensure_ascii=False) + "\n")
    rows = []
    for it in items:
        rows.append({
            "qid": it.qid, "source_file": it.source_file, "type": it.type,
            "question_preview": it.question[:160].replace("\n", " "),
            "has_answer": 1 if it.answer else 0,
            "has_explanation": 1 if it.explanation else 0,
            "has_kp": 1 if it.knowledge_points else 0,
            "source_window": f"{it.source_window[0]}-{it.source_window[1]}",
            "window_local_id": it.window_local_id or ""
        })
    pd.DataFrame(rows).to_csv(out_dir_file / "summary.csv", index=False, encoding="utf-8-sig")
    return jsonl_path, out_dir_file / "summary.csv"

def append_combined(items: List[QAItem], combined_jsonl: Path, combined_rows: List[dict]):
    combined_jsonl.parent.mkdir(parents=True, exist_ok=True)
    with combined_jsonl.open("a", encoding="utf-8") as f:
        for it in items:
            f.write(json.dumps(asdict(it), ensure_ascii=False) + "\n")
            combined_rows.append({
                "qid": it.qid, "source_file": it.source_file, "type": it.type,
                "question_preview": it.question[:160].replace("\n", " "),
                "has_answer": 1 if it.answer else 0,
                "has_explanation": 1 if it.explanation else 0,
                "has_kp": 1 if it.knowledge_points else 0,
            })

# ========= æ ‡æ³¨è¯»å– & æ–‡ä»¶æŸ¥æ‰¾ =========
def load_target_names(label_json_path: Path) -> set:
    targets = set()

    def add_name(v: str):
        v = (v or "").strip()
        if not v:
            return
        name = Path(v).name        # æœ€åä¸€å±‚æ–‡ä»¶å
        stem = Path(name).stem     # å»æ‰æ‰©å±•å
        if stem.endswith("_content_list"):
            stem = stem[:-len("_content_list")].rstrip("_- ")
        targets.add(stem)

    with open(label_json_path, "r", encoding="utf-8", errors="ignore") as f:
        for line in f:
            if not line.strip():
                continue
            try:
                obj = json.loads(line)
            except Exception:
                continue
            # åªå¤„ç† label ä¸º MIXED_TOGETHER çš„è®°å½•
            if obj.get("label") == "MIXED_TOGETHER":
                add_name(obj.get("file") or obj.get("path") or obj.get("File") or "")

    return targets

def find_md_files(md_root: Path, target_names: set) -> List[Path]:
    exts = {'.md', '.markdown'}
    files = []
    unmatched = []

    for p in md_root.rglob("*"):
        if p.is_file() and p.suffix.lower() in exts:
            stem = Path(p.name).stem  # ç›´æ¥å–å»æ‰©å±•åçš„éƒ¨åˆ†
            if stem in target_names:
                files.append(p)

    return sorted(files)

# ========= å•çª—å£å¤„ç†ï¼ˆç»™å¹¶å‘è°ƒç”¨ï¼‰ =========
def process_window(md_path: Path, s_line: int, e_line: int, chunk_text: str,
                   window: int, stride: int, max_retries: int, timeout: int,
                   out_root: Path, raw_root: Optional[Path], debug_dir: Optional[Path],
                   embedding_dedup: Optional[EmbeddingDeduplicator] = None,
                   dedup_logger: Optional[DedupLogger] = None) -> Tuple[str, int, int, List[QAItem], List[Dict[str, Any]]]:
    """
    å¤„ç†å•ä¸ªçª—å£ï¼Œè¿”å› (source_file_stem, s_line, e_line, items, raw_records)
    """
    base_norm = md_path.stem
    compressed = compress_chunk_text(chunk_text, MAX_CHARS_PER_PROMPT)
    prompt = CHUNK_PROMPT_TEMPLATE.format(
        start_line=s_line, end_line=e_line, chunk_text=compressed
    )
    
    # è°ƒè¯•ç›®å½•
    wdir = None
    if debug_dir:
        wdir = debug_dir / base_norm / f"{s_line}-{e_line}"
        write_text(wdir / "prompt.txt", prompt)

    # è°ƒç”¨Arkå¤„ç†çª—å£
    raw_out, data, meta = send_chunk_adaptive(
        s_line=s_line, e_line=e_line, chunk_text=chunk_text,
        max_retries=max_retries, retry_backoff=1.8, timeout=timeout,
        debug_dir=debug_dir, base_norm=base_norm
    )

    # å¤„ç†è¿”å›çš„æ•°æ®
    items = []
    seen = set()  # æ¯ä¸ªçª—å£å†…çš„å»é‡
    seen_questions = {}  # è®°å½•å·²è§è¿‡çš„é¢˜ç›®ï¼Œç”¨äºå»é‡æ—¥å¿—
    
    for obj in data:
        question = norm_text(str(obj.get("question") or ""))
        answer = norm_text(str(obj.get("answer") or ""))
        explanation = norm_text(str(obj.get("explanation") or ""))
        knowledge = norm_text(str(obj.get("knowledge_points") or ""))
        qtype = norm_text(str(obj.get("type") or ""))

        # é¢˜å‹è¿‡æ»¤
        if not qtype or not type_filter(qtype):
            continue

        # ç­”æ¡ˆä¸ºç©ºç›´æ¥å‰”é™¤
        if not answer:
            continue

        # hash å»é‡ï¼ˆçª—å£å†…ï¼‰
        key = hash_key(question)
        if key in seen:
            # è®°å½•MD5å»é‡æ—¥å¿—
            if dedup_logger:
                window_range = f"{s_line}-{e_line}"
                duplicate_question = seen_questions.get(key, "æœªçŸ¥é¢˜ç›®")
                dedup_logger.log_md5_dedup(base_norm, window_range, question, duplicate_question)
            continue
        seen.add(key)
        seen_questions[key] = question

        # Embedding ç›¸ä¼¼åº¦å»é‡
        if embedding_dedup is not None:
            is_unique, duplicate_info = embedding_dedup.add_question(question)
            if not is_unique:
                # è®°å½•embeddingå»é‡æ—¥å¿—
                if dedup_logger:
                    window_range = f"{s_line}-{e_line}"
                    duplicate_question, similarity_score = duplicate_info
                    dedup_logger.log_embedding_dedup(base_norm, window_range, question, 
                                                   duplicate_question, similarity_score)
                continue

        items.append(QAItem(
            qid=None, type=qtype, question=question, answer=answer,
            explanation=explanation, knowledge_points=knowledge,
            source_window=(s_line, e_line), window_local_id=str(obj.get("id") or ""),
            source_file=base_norm
        ))

    return base_norm, s_line, e_line, items, [{
        "window": f"{s_line}-{e_line}",
        "chunk_text": chunk_text,
        "compressed_chunk": meta["compressed_chunk"],
        "prompt_length": meta["prompt_len"],
        "model_output": raw_out,
        "response_length": meta["response_len"],
        "sanitized_used": meta["sanitized_used"],
        "parse_error": meta["parse_error"],
        "parsed_questions": data
    }]

# ========= å•æ–‡ä»¶å¤„ç†ï¼ˆç»™å¹¶å‘è°ƒç”¨ï¼‰ =========
def process_one(md_path: Path,
                window: int,
                stride: int,
                max_retries: int,
                timeout: int,
                out_root: Path,
                raw_root: Optional[Path],
                debug_dir: Optional[Path],
                embedding_dedup: Optional[EmbeddingDeduplicator] = None,
                dedup_logger: Optional[DedupLogger] = None) -> Tuple[str, List[QAItem], ExtractStats, List[Dict[str, Any]], Path, Path, Optional[Path]]:
    base_norm = md_path.stem
    items, stats, raw_records = extract_from_md(
        md_path=md_path,
        window=window, stride=stride,
        max_retries=max_retries, retry_backoff=1.8, timeout=timeout,
        source_file_stem=base_norm, debug_dir=debug_dir,
        embedding_dedup=embedding_dedup,
        dedup_logger=dedup_logger
    )

    jsonl_path, csv_path = save_items_per_file(items, out_root, base_norm)
    raw_path = None
    if raw_root:
        raw_path = raw_root / base_norm / "raw_chunks.jsonl"
        save_raw_chunks(raw_records, raw_path)

    return base_norm, items, stats, raw_records, jsonl_path, csv_path, raw_path


# ========= ä¸»ç¨‹åº =========
def main():
    ap = argparse.ArgumentParser(description="æ‰¹é‡ï¼šæŒ‰æ ‡æ³¨JSONç­›é€‰ .mdï¼ŒArk æŠ½å–è¯•é¢˜ï¼ˆå¸¦è¯·æ±‚/è¿”å›è°ƒè¯•æ—¥å¿—ï¼‰")
    ap.add_argument("md_dir", type=str, help="Markdown æ ¹ç›®å½•ï¼ˆé€’å½’ï¼‰")
    ap.add_argument("label_json", type=str, help="æ ‡æ³¨è¡Œå¼ JSONLï¼Œåªå¤„ç† label ä¸º MIXED_TOGETHER çš„è®°å½•")
    ap.add_argument("--out-dir", type=str, default="out_qas", help="è¾“å‡ºæ ¹ç›®å½•")
    ap.add_argument("--save-raw-dir", type=str, default="", help="ä¿å­˜åŸæ–‡ç‰‡æ®µå¯¹ç…§çš„ç›®å½•ï¼ˆæ¯æ–‡ä»¶ raw_chunks.jsonlï¼‰")
    ap.add_argument("--debug-log-dir", type=str, default="", help="ä¿å­˜æ¯ä¸ªçª—å£çš„ prompt/response/parse_error/sanitized.json")
    ap.add_argument("--window", type=int, default=DEFAULT_WINDOW_LINES, help="æ»‘çª—å¤§å°ï¼ˆè¡Œï¼‰")
    ap.add_argument("--stride", type=int, default=DEFAULT_STRIDE_LINES, help="æ»‘çª—æ­¥é•¿ï¼ˆè¡Œï¼‰")
    ap.add_argument("--max-retries", type=int, default=3, help="Ark è°ƒç”¨å¤±è´¥æœ€å¤§é‡è¯•æ¬¡æ•°")
    ap.add_argument("--timeout", type=int, default=DEFAULT_ARK_TIMEOUT, help="å•æ¬¡ Ark è¯·æ±‚è¶…æ—¶ï¼ˆç§’ï¼‰")
    ap.add_argument("--max-workers", type=int, default=256, help="å¹¶å‘çº¿ç¨‹æ•°")
    ap.add_argument("--concurrency-level", type=str, choices=["window", "file"], default="window", 
                    help="å¹¶å‘çº§åˆ«ï¼šwindow(çª—å£çº§å¹¶å‘) æˆ– file(æ–‡ä»¶çº§å¹¶å‘)")
    ap.add_argument("--enable-embedding-dedup", action="store_true", help="å¯ç”¨embeddingç›¸ä¼¼åº¦å»é‡")
    ap.add_argument("--embedding-model", type=str, default=DEFAULT_EMBEDDING_MODEL, help="embeddingæ¨¡å‹åç§°æˆ–æœ¬åœ°æ¨¡å‹è·¯å¾„")
    ap.add_argument("--similarity-threshold", type=float, default=DEFAULT_SIMILARITY_THRESHOLD, help="ä½™å¼¦ç›¸ä¼¼åº¦é˜ˆå€¼")
    ap.add_argument("--enable-dedup-log", action="store_true", help="å¯ç”¨å»é‡è¿‡ç¨‹æ—¥å¿—è®°å½•")
    args = ap.parse_args()

    md_root = Path(args.md_dir)
    label_json_path = Path(args.label_json)
    out_root = Path(args.out_dir)
    raw_root = Path(args.save_raw_dir) if args.save_raw_dir else None
    debug_dir = Path(args.debug_log_dir) if args.debug_log_dir else None

    if not md_root.exists():
        print(f"âŒ md_dir ä¸å­˜åœ¨ï¼š{md_root}"); return
    if not label_json_path.exists():
        print(f"âŒ label_json ä¸å­˜åœ¨ï¼š{label_json_path}"); return

    print("ğŸ“¥ è¯»å–æ ‡æ³¨ ...")
    target_names = load_target_names(label_json_path)
    print(f"âœ… ç›®æ ‡æ–‡ä»¶æ•°é‡ï¼š{len(target_names)}")

    print("ğŸ” æ‰«æ .md ...")
    md_files = find_md_files(md_root, target_names)
    print(f"ğŸ“„ å¾…å¤„ç†æ–‡ä»¶æ•°ï¼š{len(md_files)}")
    if not md_files:
        print("ğŸ›‘ æœªæ‰¾åˆ°å¯å¤„ç†çš„ md"); return

    out_root.mkdir(parents=True, exist_ok=True)
    combined_jsonl = out_root / "combined_questions.jsonl"
    combined_csv = out_root / "combined_summary.csv"
    
    # åˆ›å»ºcombinedæ–‡ä»¶å¹¶å†™å…¥è¡¨å¤´
    if combined_jsonl.exists(): combined_jsonl.unlink()
    with combined_jsonl.open("w", encoding="utf-8") as f:
        pass  # åˆ›å»ºç©ºæ–‡ä»¶
    
    # åˆ›å»ºCSVæ–‡ä»¶å¹¶å†™å…¥è¡¨å¤´
    csv_headers = ["qid", "source_file", "type", "question_preview", "has_answer", "has_explanation", "has_kp"]
    with combined_csv.open("w", encoding="utf-8-sig") as f:
        f.write(",".join(csv_headers) + "\n")

    # åˆå§‹åŒ–embeddingå»é‡å™¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    embedding_dedup = None
    if args.enable_embedding_dedup:
        print(f"ğŸ” åˆå§‹åŒ–embeddingå»é‡å™¨ï¼Œæ¨¡å‹ï¼š{args.embedding_model}ï¼Œé˜ˆå€¼ï¼š{args.similarity_threshold}")
        embedding_dedup = EmbeddingDeduplicator(
            model_name=args.embedding_model,
            similarity_threshold=args.similarity_threshold
        )

    # åˆå§‹åŒ–å»é‡æ—¥å¿—è®°å½•å™¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    dedup_logger = None
    if args.enable_dedup_log:
        dedup_log_file = out_root / "dedup_log.jsonl"
        dedup_logger = DedupLogger(dedup_log_file)
        print(f"ğŸ“ å¯ç”¨å»é‡æ—¥å¿—è®°å½•ï¼š{dedup_log_file}")
        if dedup_log_file.exists():
            dedup_log_file.unlink()  # æ¸…ç©ºä¹‹å‰çš„æ—¥å¿—

    # æ ¹æ®å¹¶å‘çº§åˆ«é€‰æ‹©å¤„ç†æ–¹å¼
    combined_rows: List[dict] = []
    
    if args.concurrency_level == "window":
        # çª—å£çº§å¹¶å‘å¤„ç†
        print(f"ğŸš€ ä½¿ç”¨çª—å£çº§å¹¶å‘ï¼Œæœ€å¤§çº¿ç¨‹æ•°ï¼š{args.max_workers}")
        
        # æ”¶é›†æ‰€æœ‰çª—å£ä»»åŠ¡
        all_window_tasks = []
        file_stats = {}  # ç”¨äºè·Ÿè¸ªæ¯ä¸ªæ–‡ä»¶çš„ç»Ÿè®¡ä¿¡æ¯
        
        print("ğŸ“‹ å‡†å¤‡çª—å£ä»»åŠ¡...")
        for md_path in md_files:
            base_norm = md_path.stem
            text = md_path.read_text(encoding="utf-8", errors="ignore")
            lines = text.splitlines()
            windows = make_windows(lines, window=args.window, stride=args.stride)
            
            file_stats[base_norm] = {
                'path': md_path,
                'windows': len(windows),
                'items': [],
                'raw_records': [],
                'stats': ExtractStats(windows=len(windows))
            }
            
            for s_line, e_line, chunk in windows:
                all_window_tasks.append((md_path, s_line, e_line, chunk))
        
        print(f"ğŸ“Š æ€»çª—å£æ•°ï¼š{len(all_window_tasks)}")
        
        # å…¨å±€çª—å£çº§å¹¶å‘å¤„ç†
        with cf.ThreadPoolExecutor(max_workers=args.max_workers) as executor:
            futures = {
                executor.submit(
                    process_window,
                    md_path=task[0], s_line=task[1], e_line=task[2], chunk_text=task[3],
                    window=args.window, stride=args.stride, max_retries=args.max_retries, timeout=args.timeout,
                    out_root=out_root, raw_root=raw_root, debug_dir=debug_dir,
                    embedding_dedup=embedding_dedup,
                    dedup_logger=dedup_logger
                ): (task[0].stem, task[1], task[2])  # ç”¨ (æ–‡ä»¶å, èµ·å§‹è¡Œ, ç»“æŸè¡Œ) ä½œä¸ºkey
                for task in all_window_tasks
            }

            # å¤„ç†å®Œæˆçš„çª—å£
            for fut in tqdm(cf.as_completed(futures), total=len(futures), ncols=100, desc="çª—å£çº§å¹¶å‘æŠ½å–"):
                file_stem, s_line, e_line = futures[fut]
                try:
                    source_file_stem, s_line, e_line, items, raw_records = fut.result()
                    
                    # æ›´æ–°æ–‡ä»¶ç»Ÿè®¡
                    file_stats[source_file_stem]['items'].extend(items)
                    file_stats[source_file_stem]['raw_records'].extend(raw_records)
                    file_stats[source_file_stem]['stats'].raw_questions += len(items)
                    file_stats[source_file_stem]['stats'].kept_questions += len(items)
                    
                    # å®æ—¶å†™å…¥combinedæ–‡æ¡£
                    if items:
                        # å†™å…¥JSONL
                        with combined_jsonl.open("a", encoding="utf-8") as f:
                            for it in items:
                                f.write(json.dumps(asdict(it), ensure_ascii=False) + "\n")
                        
                        # å†™å…¥CSV
                        with combined_csv.open("a", encoding="utf-8-sig") as f:
                            for it in items:
                                row = [
                                    it.qid, it.source_file, it.type,
                                    it.question[:160].replace("\n", " "),
                                    "1" if it.answer else "0",
                                    "1" if it.explanation else "0", 
                                    "1" if it.knowledge_points else "0"
                                ]
                                f.write(",".join(str(cell) for cell in row) + "\n")
                        
                        # åŒæ—¶ç»´æŠ¤å†…å­˜ä¸­çš„æ±‡æ€»æ•°æ®ï¼ˆç”¨äºæœ€ç»ˆç»Ÿè®¡ï¼‰
                        for it in items:
                            combined_rows.append({
                                "qid": it.qid, "source_file": it.source_file, "type": it.type,
                                "question_preview": it.question[:160].replace("\n", " "),
                                "has_answer": 1 if it.answer else 0,
                                "has_explanation": 1 if it.explanation else 0,
                                "has_kp": 1 if it.knowledge_points else 0,
                            })
                            
                    # æ¯å¤„ç†100ä¸ªçª—å£æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
                    if len(combined_rows) % 100 == 0:
                        print(f"ğŸ’¾ å·²å®æ—¶å†™å…¥combinedæ–‡æ¡£ï¼Œå½“å‰æ€»è®¡ï¼š{len(combined_rows)} é¢˜")
                        
                except Exception as e:
                    print(f"\nâŒ çª—å£å¤„ç†å¤±è´¥ {file_stem} {s_line}-{e_line}: {e}")
        
        # ä¸ºæ¯ä¸ªæ–‡ä»¶çš„é¢˜ç›®åˆ†é…qidå¹¶ä¿å­˜æ–‡ä»¶çº§ç»“æœ
        print("\nğŸ“ ä¿å­˜æ–‡ä»¶çº§ç»“æœ...")
        for base_norm, file_info in file_stats.items():
            if file_info['items']:
                # ä¸ºé¢˜ç›®åˆ†é…qid
                for i, q in enumerate(file_info['items'], start=1):
                    q.qid = f"{base_norm}__Q{i:05d}"
                
                # ä¿å­˜æ–‡ä»¶çº§ç»“æœ
                jsonl_path, csv_path = save_items_per_file(file_info['items'], out_root, base_norm)
                raw_path = None
                if raw_root:
                    raw_path = raw_root / base_norm / "raw_chunks.jsonl"
                    save_raw_chunks(file_info['raw_records'], raw_path)
                
                print(f"âœ… {file_info['path'].name} â†’ æŠ½å– {len(file_info['items'])} é¢˜ï¼›çª—å£æ•°={file_info['stats'].windows}, "
                      f"æ¨¡å‹è¿”å›={file_info['stats'].raw_questions}, ä¿ç•™={file_info['stats'].kept_questions}")
                print(f"ğŸ“ {jsonl_path}")
                print(f"ğŸ§¾ {csv_path}")
                if raw_path:
                    print(f"ğŸ“‚ åŸæ–‡ç‰‡æ®µå¯¹ç…§ï¼š{raw_path}")
    
    else:
        # æ–‡ä»¶çº§å¹¶å‘å¤„ç†
        print(f"ğŸš€ ä½¿ç”¨æ–‡ä»¶çº§å¹¶å‘ï¼Œæœ€å¤§çº¿ç¨‹æ•°ï¼š{args.max_workers}")
        
        with cf.ThreadPoolExecutor(max_workers=args.max_workers) as executor:
            futures = {
                executor.submit(
                    process_one,
                    md_path=md_path,
                    window=args.window,
                    stride=args.stride,
                    max_retries=args.max_retries,
                    timeout=args.timeout,
                    out_root=out_root,
                    raw_root=raw_root,
                    debug_dir=debug_dir,
                    embedding_dedup=embedding_dedup,
                    dedup_logger=dedup_logger
                ): md_path
                for md_path in md_files
            }

            for fut in tqdm(cf.as_completed(futures), total=len(futures), ncols=100, desc="æ–‡ä»¶çº§å¹¶å‘æŠ½å–"):
                md_path = futures[fut]
                try:
                    base_norm, items, stats, raw_records, jsonl_path, csv_path, raw_path = fut.result()
                    print(f"\nâœ… {md_path.name} â†’ æŠ½å– {len(items)} é¢˜ï¼›çª—å£æ•°={stats.windows}, æ¨¡å‹è¿”å›={stats.raw_questions}, "
                          f"ä¿ç•™={stats.kept_questions}, hashå»é‡ä¸¢å¼ƒ={stats.dedup_dropped}, embeddingå»é‡ä¸¢å¼ƒ={stats.embedding_dedup_dropped}")
                    print(f"ğŸ“ {jsonl_path}")
                    print(f"ğŸ§¾ {csv_path}")
                    if raw_path:
                        print(f"ğŸ“‚ åŸæ–‡ç‰‡æ®µå¯¹ç…§ï¼š{raw_path}")

                    # å®æ—¶å†™å…¥combinedæ–‡æ¡£
                    if items:
                        # å†™å…¥JSONL
                        with combined_jsonl.open("a", encoding="utf-8") as f:
                            for it in items:
                                f.write(json.dumps(asdict(it), ensure_ascii=False) + "\n")
                        
                        # å†™å…¥CSV
                        with combined_csv.open("a", encoding="utf-8-sig") as f:
                            for it in items:
                                row = [
                                    it.qid, it.source_file, it.type,
                                    it.question[:160].replace("\n", " "),
                                    "1" if it.answer else "0",
                                    "1" if it.explanation else "0", 
                                    "1" if it.knowledge_points else "0"
                                ]
                                f.write(",".join(str(cell) for cell in row) + "\n")
                        
                        # åŒæ—¶ç»´æŠ¤å†…å­˜ä¸­çš„æ±‡æ€»æ•°æ®ï¼ˆç”¨äºæœ€ç»ˆç»Ÿè®¡ï¼‰
                        for it in items:
                            combined_rows.append({
                                "qid": it.qid, "source_file": it.source_file, "type": it.type,
                                "question_preview": it.question[:160].replace("\n", " "),
                                "has_answer": 1 if it.answer else 0,
                                "has_explanation": 1 if it.explanation else 0,
                                "has_kp": 1 if it.knowledge_points else 0,
                            })
                            
                    print(f"ğŸ’¾ å·²å®æ—¶å†™å…¥combinedæ–‡æ¡£ï¼Œå½“å‰æ€»è®¡ï¼š{len(combined_rows)} é¢˜")
                except Exception as e:
                    print(f"\nâŒ å¤„ç†å¤±è´¥ï¼š{md_path} -> {e}")

    print("\n=== å…¨é‡æ±‡æ€»å®Œæˆ ===")
    print(f"ğŸ—‚ é¢˜ç›®æ±‡æ€» JSONLï¼š{combined_jsonl.resolve()}")
    print(f"ğŸ“Š é¢˜ç›®æ±‡æ€» CSV ï¼š{combined_csv.resolve()}")
    
    # æ˜¾ç¤ºå»é‡ç»Ÿè®¡ä¿¡æ¯
    if args.enable_dedup_log and dedup_logger and dedup_logger.log_file.exists():
        print(f"\nğŸ“ å»é‡æ—¥å¿—æ–‡ä»¶ï¼š{dedup_logger.log_file.resolve()}")
        
        # ç»Ÿè®¡å»é‡æƒ…å†µ
        md5_count = 0
        embedding_count = 0
        total_dedup = 0
        
        with dedup_logger.log_file.open("r", encoding="utf-8") as f:
            for line in f:
                if line.strip():
                    try:
                        log_entry = json.loads(line)
                        total_dedup += 1
                        if log_entry.get("dedup_type") == "md5":
                            md5_count += 1
                        elif log_entry.get("dedup_type") == "embedding":
                            embedding_count += 1
                    except:
                        continue
        
        print(f"ğŸ“Š å»é‡ç»Ÿè®¡ï¼š")
        print(f"   - æ€»å»é‡æ•°é‡ï¼š{total_dedup}")
        print(f"   - MD5å»é‡ï¼š{md5_count}")
        print(f"   - Embeddingå»é‡ï¼š{embedding_count}")
        
        if embedding_count > 0:
            # è®¡ç®—embeddingå»é‡çš„ç›¸ä¼¼åº¦åˆ†å¸ƒ
            similarities = []
            with dedup_logger.log_file.open("r", encoding="utf-8") as f:
                for line in f:
                    if line.strip():
                        try:
                            log_entry = json.loads(line)
                            if log_entry.get("dedup_type") == "embedding" and log_entry.get("similarity_score"):
                                similarities.append(log_entry["similarity_score"])
                        except:
                            continue
            
            if similarities:
                # ç¡®ä¿æ‰€æœ‰ç›¸ä¼¼åº¦éƒ½æ˜¯PythonåŸç”Ÿfloatç±»å‹
                similarities = [float(s.item()) if hasattr(s, 'item') else float(s) for s in similarities]
                print(f"   - Embeddingç›¸ä¼¼åº¦ç»Ÿè®¡ï¼š")
                print(f"     * å¹³å‡ç›¸ä¼¼åº¦ï¼š{np.mean(similarities):.4f}")
                print(f"     * æœ€é«˜ç›¸ä¼¼åº¦ï¼š{np.max(similarities):.4f}")
                print(f"     * æœ€ä½ç›¸ä¼¼åº¦ï¼š{np.min(similarities):.4f}")
                print(f"     * ç›¸ä¼¼åº¦é˜ˆå€¼ï¼š{args.similarity_threshold}")

if __name__ == "__main__":
    main()


# è¿è¡Œç¤ºä¾‹
'''python 3_qa.py \
  /home/wangxi/workspace/gongye/zejun \
  /home/wangxi/workspace/gongye/shiti/classification_results_20250815_110753.jsonl \
  --max-workers 256 \
  --window 80 --stride 40 \
  --timeout 120 \
  --out-dir shiti/out_qas \
  --save-raw-dir shiti/raw_logs \
  --debug-log-dir shiti/debug_logs \
  --enable-embedding-dedup \
  --embedding-model /home/wangxi/workspace/gongye/yijizaojia/Qwen3-Embedding-0.6B \
  --concurrency-level window \
  --similarity-threshold 0.99 \
  --enable-dedup-log'''