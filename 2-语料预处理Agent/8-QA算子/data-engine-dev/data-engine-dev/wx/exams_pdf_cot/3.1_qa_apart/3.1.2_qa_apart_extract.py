#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import re
import json
import time
import random
import argparse
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Set, Any
from dataclasses import dataclass, asdict
from tqdm import tqdm
import concurrent.futures as cf

# ========= Ark é…ç½® =========
API_KEY = "c702676e-a69f-4ff0-a672-718d0d4723ed"
MODEL_ID = "doubao-seed-1-6-250615"

try:
    from volcenginesdkarkruntime import Ark
except Exception:
    raise SystemExit("âŒ è¯·å…ˆå®‰è£… Ark SDK: pip install 'volcengine-python-sdk[ark]'")

# ========= å‚æ•°å¸¸é‡ =========
DEFAULT_WINDOW_LINES = 80
DEFAULT_STRIDE_LINES = 40
MAX_CHARS_PER_PROMPT = 9000
DEFAULT_ARK_TIMEOUT = 120

# ========= System/User Prompt =========
QUESTION_SYSTEM_PROMPT = """\
ä½ æ˜¯ä¸€ä¸ªç²¾ç¡®çš„è¯•é¢˜æŠ½å–åŠ©æ‰‹ã€‚
æˆ‘å°†ç»™ä½ ä¸€æ®µä»è€ƒè¯•çœŸé¢˜è½¬æˆ Markdown çš„æ–‡æœ¬ç‰‡æ®µã€‚
ä½ çš„ä»»åŠ¡ï¼šåªæŠ½å–å››ç§ç±»å‹çš„å®Œæ•´è¯•é¢˜ï¼š
- singleï¼ˆå•é€‰é¢˜ï¼‰
- multipleï¼ˆå¤šé€‰é¢˜ï¼‰
- judgeï¼ˆåˆ¤æ–­é¢˜ï¼‰
- fillï¼ˆå¡«ç©ºé¢˜ï¼‰

è§„åˆ™ï¼š
1) æ¯é“é¢˜å¿…é¡»åŒ…å«ï¼š
   - "question_number"ï¼šé¢˜ç›®ç¼–å·ï¼ˆæ•°å­—ï¼Œå¦‚1ã€2ã€3ç­‰ï¼‰ï¼Œè¿™æ˜¯å¿…é¡»çš„ï¼Œç”¨äºåŒ¹é…ç­”æ¡ˆ
   - "type"ï¼šå–å€¼åªèƒ½æ˜¯ ["single","multiple","judge","fill"]
   - "question"ï¼šé¢˜å¹²çš„å®Œæ•´æ–‡å­—ï¼ŒåŒ…æ‹¬æ‰€æœ‰é€‰é¡¹ï¼ˆä¿ç•™é¢˜å¹²ä¸­çš„å›¾ç‰‡é“¾æ¥æˆ–å…¬å¼ï¼Œé€‰é¡¹æŒ‰åŸé¡ºåºä¿ç•™åœ¨é¢˜ç›®æ–‡æœ¬ä¸­ï¼‰ã€‚
2) åªè¾“å‡ºå®Œæ•´çš„è¯•é¢˜ï¼šå¿…é¡»æœ‰å®Œæ•´çš„é—®é¢˜ã€‚
3) å¦‚æœé¢˜ç›®åœ¨è¯­ä¹‰ä¸Šè¢«æˆªæ–­æˆ–ä¸å®Œæ•´ï¼Œåˆ™ä¸è¦è¾“å‡ºï¼Œå¦‚æœé¢˜ç›®åªæ˜¯ç¼ºå°‘ååŠä¸ªæ‹¬å·è¿™ç§éè¯­ä¹‰çš„å°é—®é¢˜ï¼Œåˆ™éœ€è¦è¡¥å…¨ã€‚
4) ä¸¥æ ¼è¾“å‡º JSON æ ¼å¼ï¼ŒUTF-8 ç¼–ç ï¼Œä¸è¦åŒ…å« Markdownã€ä»£ç å—å›´æ æˆ–å¤šä½™æ–‡å­—ã€‚
5) ä¸è¦ç¼–é€ å†…å®¹ã€‚ç­”æ¡ˆã€è§£ææˆ–çŸ¥è¯†ç‚¹ç¼ºå¤±æ—¶ç”¨ç©ºå­—ç¬¦ä¸²è¡¨ç¤ºã€‚
6) å¦‚æœç‰‡æ®µä¸­æ²¡æœ‰ç¬¦åˆæ¡ä»¶çš„è¯•é¢˜ï¼Œè¿”å›ç©ºæ•°ç»„ []ã€‚
7) å¿…é¡»æå–é¢˜å·ï¼ˆquestion_numberï¼‰ï¼Œè¿™æ˜¯å…³é”®å­—æ®µï¼Œç”¨äºåç»­çš„ç­”æ¡ˆåŒ¹é…ã€‚
8)é¢˜ç›®é‡Œå¦‚æœæœ‰è¡¨æ ¼ï¼Œåˆ™éœ€è¦è§£ææ¯ä¸ª<td>å•å…ƒæ ¼ä¸­çš„å†…å®¹ã€‚

è¾“å‡ºæ ¼å¼ï¼š
JSON æ•°ç»„ï¼Œæ•°ç»„ä¸­æ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªè¯•é¢˜å¯¹è±¡ã€‚
"""

QUESTION_CHUNK_PROMPT_TEMPLATE = """\
ä¸‹é¢æ˜¯å…¨æ–‡çš„ç¬¬ {start_line} è¡Œåˆ°ç¬¬ {end_line} è¡Œçš„ Markdown å†…å®¹ã€‚

--- å¼€å§‹ç‰‡æ®µ ---
{chunk_text}
--- ç»“æŸç‰‡æ®µ ---

è¯·è®°ä½ï¼š
- åªä¿ç•™ ["single","multiple","judge","fill"] è¿™å››ç§é¢˜å‹ã€‚
- å¿…é¡»æå–é¢˜å·ï¼ˆquestion_numberï¼‰ï¼Œè¿™æ˜¯å…³é”®å­—æ®µï¼Œç”¨äºåç»­çš„ç­”æ¡ˆåŒ¹é…ã€‚
- åªè¿”å›ä¸¥æ ¼ JSON æ•°ç»„ï¼Œä¸è¦è¾“å‡ºä»»ä½•é¢å¤–æ–‡å­—ã€‚
"""

ANSWER_SYSTEM_PROMPT = """\
ä½ æ˜¯ä¸€ä¸ªç²¾ç¡®çš„ç­”æ¡ˆæå–åŠ©æ‰‹ã€‚
æˆ‘å°†ç»™ä½ ä¸€æ®µä»è€ƒè¯•ç­”æ¡ˆæ–‡ä»¶è½¬æˆ Markdown çš„æ–‡æœ¬ç‰‡æ®µã€‚
ä½ çš„ä»»åŠ¡ï¼šåªæå–ç­”æ¡ˆä¿¡æ¯ã€‚

è§„åˆ™ï¼š
1) æ¯é“ç­”æ¡ˆå¿…é¡»åŒ…å«ï¼š
   - "question_number"ï¼šé¢˜ç›®ç¼–å·ï¼ˆæ•°å­—ï¼Œå¦‚1ã€2ã€3ç­‰ï¼‰ï¼Œè¿™æ˜¯å¿…é¡»çš„
   - "answer"ï¼šç®€æ´çš„ç­”æ¡ˆæ–‡å­—ï¼›é€‰æ‹©é¢˜ç”¨å­—æ¯ï¼ˆå¦‚ "A" æˆ– "ACD"ï¼‰ï¼›åˆ¤æ–­é¢˜ç”¨"å¯¹/é”™"æˆ–"True/False"ï¼›å¡«ç©ºé¢˜ç”¨å®é™…å¡«ç©ºå†…å®¹
2) åªè¾“å‡ºå®Œæ•´çš„ç­”æ¡ˆï¼šå¿…é¡»æœ‰é¢˜å·å’Œç­”æ¡ˆã€‚
3) å¦‚æœç­”æ¡ˆè¢«æˆªæ–­æˆ–ä¸å®Œæ•´ï¼Œåˆ™ä¸è¦è¾“å‡ºã€‚
4) ä¸¥æ ¼è¾“å‡º JSON æ ¼å¼ï¼ŒUTF-8 ç¼–ç ï¼Œä¸è¦åŒ…å« Markdownã€ä»£ç å—å›´æ æˆ–å¤šä½™æ–‡å­—ã€‚
5) ä¸è¦ç¼–é€ å†…å®¹ã€‚ç­”æ¡ˆç¼ºå¤±æ—¶ç”¨ç©ºå­—ç¬¦ä¸²è¡¨ç¤ºã€‚
6) å¦‚æœç‰‡æ®µä¸­æ²¡æœ‰ç¬¦åˆæ¡ä»¶çš„ç­”æ¡ˆï¼Œè¿”å›ç©ºæ•°ç»„ []ã€‚
7ï¼‰ç‰¹åˆ«æ³¨æ„è¡¨æ ¼æ ¼å¼ï¼šå¦‚æœé‡åˆ°HTMLè¡¨æ ¼ï¼Œè¯·è§£ææ¯ä¸ª<td>å•å…ƒæ ¼ä¸­çš„å†…å®¹ã€‚
    - è¡¨æ ¼æ ¼å¼ç¤ºä¾‹ï¼š<td>1.C(1)â†</td> åº”æå–ä¸º {"question_number": 1, "answer": "C"}
    - å¤„ç†è¡¨æ ¼æ—¶ï¼Œå¿½ç•¥æ‹¬å·å†…çš„åˆ†æ•°å’Œç®­å¤´ç¬¦å·ï¼Œåªæå–é¢˜å·å’Œç­”æ¡ˆå­—æ¯ã€‚
8ï¼‰å¦‚æœé‡åˆ°1-5ï¼šABCDEï¼Œåˆ™æå–ä¸º{"question_number": 1, "answer": "A"}ï¼Œä»¥æ­¤ç±»æ¨ã€‚
9ï¼‰å¦‚æœé‡åˆ°æŸä¸€é¢˜å·æœ‰ä¸¤ä¸ªç­”æ¡ˆé‡å¤åªä¿ç•™ä¸€ä¸ªã€‚

è¾“å‡ºæ ¼å¼ï¼š
JSON æ•°ç»„ï¼Œæ•°ç»„ä¸­æ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªç­”æ¡ˆå¯¹è±¡ã€‚
"""

ANSWER_CHUNK_PROMPT_TEMPLATE = """\
ä¸‹é¢æ˜¯ç­”æ¡ˆæ–‡ä»¶çš„ç¬¬ {start_line} è¡Œåˆ°ç¬¬ {end_line} è¡Œçš„å†…å®¹ã€‚

--- å¼€å§‹ç‰‡æ®µ ---
{chunk_text}
--- ç»“æŸç‰‡æ®µ ---

è¯·è®°ä½ï¼š
- åªæå–ç­”æ¡ˆä¿¡æ¯ï¼Œä¸éœ€è¦é¢˜ç›®å†…å®¹ã€‚
- å¿…é¡»æå–é¢˜å·ï¼ˆquestion_numberï¼‰ï¼Œè¿™æ˜¯å…³é”®å­—æ®µã€‚
- å¿…é¡»æå–ç­”æ¡ˆï¼ˆanswerï¼‰ï¼Œè¿™æ˜¯å…³é”®å­—æ®µã€‚
- åªè¿”å›ä¸¥æ ¼ JSON æ•°ç»„ï¼Œä¸è¦è¾“å‡ºä»»ä½•é¢å¤–æ–‡å­—ã€‚

"""


@dataclass
class Question:
    """é¢˜ç›®æ•°æ®ç»“æ„"""
    question_number: int
    question_type: str
    question_text: str
    answer: str = ""
    explanation: str = ""
    knowledge_points: str = ""
    source_file: str = ""
    source_window: Tuple[int, int] = (0, 0)


@dataclass
class ExamData:
    """è€ƒè¯•æ•°æ®ç»“æ„"""
    exam_name: str
    total_questions: int
    questions: List[Question]
    missing_answers: List[int]  # ç¼ºå°‘ç­”æ¡ˆçš„é¢˜å·
    extra_answers: List[int]    # å¤šä½™ç­”æ¡ˆçš„é¢˜å·


class QAExtractor:
    def __init__(self, base_path: str):
        self.base_path = Path(base_path)
        
    def get_exam_directories(self) -> List[Path]:
        if not self.base_path.exists():
            print(f"è·¯å¾„ä¸å­˜åœ¨: {self.base_path}")
            return []
            
        directories = []
        for item in self.base_path.iterdir():
            if item.is_dir():
                directories.append(item)
                
        return sorted(directories)

    def save_raw_chunks(self, raw_records: List[Dict[str, Any]], path: Path):
        """ä¿å­˜åŸå§‹æŠ½å–è®°å½•"""
        path.parent.mkdir(parents=True, exist_ok=True)
        with path.open("w", encoding="utf-8") as f:
            for rec in raw_records:
                f.write(json.dumps(rec, ensure_ascii=False) + "\n")

    def make_windows(self, lines: List[str], window: int, stride: int) -> List[Tuple[int, int, str]]:
        n = len(lines)
        out = []
        i = 0
        while i < n:
            s = i
            e = min(i + window - 1, n - 1)
            out.append((s+1, e+1, "\n".join(lines[s:e+1])))
            if e == n - 1:
                break
            i += stride
        return out

    def compress_chunk_text(self, s: str, max_chars: int = MAX_CHARS_PER_PROMPT) -> str:
        lines = s.replace("\r", "").splitlines()
        out = []
        blank = 0
        for ln in lines:
            ln = " ".join(ln.split())
            if not ln:
                blank += 1
                if blank > 1:
                    continue
            else:
                blank = 0
            out.append(ln)
        s2 = "\n".join(out).strip()
        return s2[:max_chars] if len(s2) > max_chars else s2

    def call_ark(self, prompt: str, system_prompt: str, temperature: float = 0.0, top_p: float = 0.9,
                 timeout: int = DEFAULT_ARK_TIMEOUT) -> str:
        time.sleep(random.uniform(0.05, 0.20))  # è½»å¾®æŠ–åŠ¨
        client = Ark(api_key=API_KEY, timeout=timeout)
        resp = client.chat.completions.create(
            model=MODEL_ID,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": prompt},
            ],
            temperature=temperature,
            top_p=top_p,
        )
        try:
            return resp.choices[0].message.content
        except Exception:
            return str(resp)

    def write_text(self, path: Path, content: str):
        """å†™å…¥æ–‡æœ¬æ–‡ä»¶"""
        path.parent.mkdir(parents=True, exist_ok=True)
        with path.open("w", encoding="utf-8") as f:
            f.write(content)
    
    def force_json_load_with_sanitize(self, s: str) -> Tuple[Any, bool]:
        s0 = s.strip()
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºç©ºæˆ–æ— æ•ˆ
        if not s0 or s0 == "" or s0 == "[]" or s0 == "{}":
            # print(f"    è¾“å…¥ä¸ºç©ºæˆ–æ— æ•ˆJSONï¼Œè¿”å›ç©ºæ•°ç»„")
            return [], True
        
        # å°è¯•ç›´æ¥è§£æ
        try:
            if s0.startswith('[') and s0.endswith(']'):
                return json.loads(s0), False
        except json.JSONDecodeError as e:
            # å¦‚æœæ˜¯"Extra data"é”™è¯¯ï¼Œå°è¯•æå–æ‰€æœ‰å®Œæ•´çš„JSONæ•°ç»„å¹¶åˆå¹¶
            if "Extra data" in str(e):
                try:
                    all_arrays = []
                    current_pos = 0
                    
                    while current_pos < len(s0):
                        # æ‰¾åˆ°ä¸‹ä¸€ä¸ª [ çš„ä½ç½®
                        start_pos = s0.find('[', current_pos)
                        if start_pos == -1:
                            break
                        
                        # æ‰¾åˆ°å¯¹åº”çš„ ] çš„ä½ç½®
                        bracket_count = 0
                        end_pos = -1
                        for i in range(start_pos, len(s0)):
                            if s0[i] == '[':
                                bracket_count += 1
                            elif s0[i] == ']':
                                bracket_count -= 1
                                if bracket_count == 0:
                                    end_pos = i
                                    break
                        
                        if end_pos != -1:
                            # æå–å®Œæ•´çš„JSONæ•°ç»„
                            json_array = s0[start_pos:end_pos + 1]
                            try:
                                # å°è¯•è§£æè¿™ä¸ªJSONæ•°ç»„
                                parsed_array = json.loads(json_array)
                                if isinstance(parsed_array, list):
                                    all_arrays.extend(parsed_array)
                            except Exception:
                                # å¦‚æœè§£æå¤±è´¥ï¼Œè·³è¿‡è¿™ä¸ªæ•°ç»„
                                pass
                            
                            current_pos = end_pos + 1
                        else:
                            # æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„ ]ï¼Œè·³è¿‡
                            current_pos = start_pos + 1
                    
                    # å¦‚æœæˆåŠŸæå–äº†å¤šä¸ªæ•°ç»„ï¼Œè¿”å›åˆå¹¶ç»“æœ
                    if all_arrays:
                        return all_arrays, True
                except Exception:
                    pass
        except Exception:
            pass

        # æˆªå–é¦–ä¸ª [ åˆ°æœ€åä¸€ä¸ª ] çš„æ ¸å¿ƒ
        first = s0.find('[')
        last = s0.rfind(']')
        core = s0 if (first == -1 or last == -1 or first >= last) else s0[first:last+1]
        
        # å¤„ç†å¯èƒ½å­˜åœ¨çš„å¤šä¸ªJSONæ•°ç»„ï¼šæå–æ‰€æœ‰å®Œæ•´çš„JSONæ•°ç»„å¹¶åˆå¹¶
        if core.count('[') > 1:
            all_arrays = []
            current_pos = 0
            
            while current_pos < len(core):
                # æ‰¾åˆ°ä¸‹ä¸€ä¸ª [ çš„ä½ç½®
                start_pos = core.find('[', current_pos)
                if start_pos == -1:
                    break
                
                # æ‰¾åˆ°å¯¹åº”çš„ ] çš„ä½ç½®
                bracket_count = 0
                end_pos = -1
                for i in range(start_pos, len(core)):
                    if core[i] == '[':
                        bracket_count += 1
                    elif core[i] == ']':
                        bracket_count -= 1
                        if bracket_count == 0:
                            end_pos = i
                            break
                
                if end_pos != -1:
                    # æå–å®Œæ•´çš„JSONæ•°ç»„
                    json_array = core[start_pos:end_pos + 1]
                    try:
                        # å°è¯•è§£æè¿™ä¸ªJSONæ•°ç»„
                        parsed_array = json.loads(json_array)
                        if isinstance(parsed_array, list):
                            all_arrays.extend(parsed_array)
                    except Exception:
                        # å¦‚æœè§£æå¤±è´¥ï¼Œè·³è¿‡è¿™ä¸ªæ•°ç»„
                        pass
                    
                    current_pos = end_pos + 1
                else:
                    # æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„ ]ï¼Œè·³è¿‡
                    current_pos = start_pos + 1
            
            # å¦‚æœæˆåŠŸæå–äº†å¤šä¸ªæ•°ç»„ï¼Œè¿”å›åˆå¹¶ç»“æœ
            if all_arrays:
                return all_arrays, True
        
        # å»é™¤ä»£ç å›´æ 
        fences = ("```json", "```JSON", "```", "~~~json", "~~~JSON", "~~~")
        for f in fences:
            if core.startswith(f):
                core = core[len(f):].strip()
            if core.endswith(f):
                core = core[:-len(f)].strip()

        # å°è¯•è§£ææ¸…æ´—åçš„JSON
        try:
            return json.loads(core), True
        except json.JSONDecodeError as e:
            error_msg = str(e)
            
            # å¤„ç†ç©ºå€¼æˆ–æ— æ•ˆå€¼é—®é¢˜
            if "Expecting value" in error_msg:
                print(f"    âš ï¸ JSONä¸ºç©ºæˆ–æ— æ•ˆï¼Œè·³è¿‡æ­¤çª—å£")
                return [], True
            
            # å¤„ç†è½¬ä¹‰å­—ç¬¦é—®é¢˜
            if "Invalid \\escape" in error_msg or "Invalid \\uXXXX escape" in error_msg:
                try:
                    # å°è¯•ä¿®å¤å¸¸è§çš„è½¬ä¹‰å­—ç¬¦é—®é¢˜
                    fixed_core = self._fix_json_escapes(core)
                    return json.loads(fixed_core), True
                except Exception:
                    pass
                
                # å¦‚æœä¿®å¤å¤±è´¥ï¼Œå°è¯•æ›´æ¿€è¿›çš„æ–¹æ³•ï¼šç§»é™¤æ‰€æœ‰æœ‰é—®é¢˜çš„è½¬ä¹‰
                try:
                    aggressive_core = self._aggressive_json_fix(core)
                    return json.loads(aggressive_core), True
                except Exception:
                    pass
            
            # å¤„ç†æ ¼å¼é—®é¢˜
            if ("Expecting ',' delimiter" in error_msg or 
                "Expecting property name" in error_msg or 
                "Expecting ':' delimiter" in error_msg):
                try:
                    # é¦–å…ˆæ¸…ç†HTMLå†…å®¹
                    cleaned_core = self._clean_html_content(core)
                    # å°è¯•ä¿®å¤JSONæ ¼å¼é—®é¢˜
                    format_fixed_core = self._fix_json_format(cleaned_core)
                    return json.loads(format_fixed_core), True
                except Exception:
                    pass
                
                # å¦‚æœæ ¼å¼ä¿®å¤å¤±è´¥ï¼Œå°è¯•æ™ºèƒ½ä¿®å¤
                try:
                    smart_fixed_core = self._smart_json_fix(cleaned_core)
                    return json.loads(smart_fixed_core), True
                except Exception:
                    pass
                
                # å¦‚æœæ™ºèƒ½ä¿®å¤å¤±è´¥ï¼Œå°è¯•è½¬ä¹‰ä¿®å¤
                try:
                    escape_fixed_core = self._fix_json_escapes(smart_fixed_core)
                    return json.loads(escape_fixed_core), True
                except Exception:
                    pass
            
            # æœ€åçš„å¤‡ç”¨æ–¹æ¡ˆï¼šå°è¯•æå–æœ‰æ•ˆçš„JSONç‰‡æ®µ
            try:
                # é¦–å…ˆæ¸…ç†HTMLå†…å®¹
                cleaned_core = self._clean_html_content(core)
                # å°è¯•æ‰¾åˆ°å®Œæ•´çš„JSONå¯¹è±¡
                import re
                # æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„JSONå¯¹è±¡
                json_objects = re.findall(r'\{[^{}]*"[^"]*"[^{}]*\}', cleaned_core)
                if json_objects:
                    # å°è¯•è§£æç¬¬ä¸€ä¸ªå®Œæ•´çš„å¯¹è±¡
                    for obj in json_objects:
                        try:
                            parsed_obj = json.loads(obj)
                            if isinstance(parsed_obj, dict) and "question_number" in parsed_obj:
                                # æ‰¾åˆ°æœ‰æ•ˆçš„ç­”æ¡ˆå¯¹è±¡ï¼ŒåŒ…è£…æˆæ•°ç»„è¿”å›
                                return [parsed_obj], True
                        except:
                            continue
            except Exception:
                pass
            
            # å¤„ç†"Extra data"é”™è¯¯ï¼šå°è¯•æå–æ‰€æœ‰å®Œæ•´çš„JSONæ•°ç»„å¹¶åˆå¹¶
            if "Extra data" in error_msg:
                try:
                    all_arrays = []
                    current_pos = 0
                    
                    while current_pos < len(core):
                        # æ‰¾åˆ°ä¸‹ä¸€ä¸ª [ çš„ä½ç½®
                        start_pos = core.find('[', current_pos)
                        if start_pos == -1:
                            break
                        
                        # æ‰¾åˆ°å¯¹åº”çš„ ] çš„ä½ç½®
                        bracket_count = 0
                        end_pos = -1
                        for i in range(start_pos, len(core)):
                            if core[i] == '[':
                                bracket_count += 1
                            elif core[i] == ']':
                                bracket_count -= 1
                                if bracket_count == 0:
                                    end_pos = i
                                    break
                        
                        if end_pos != -1:
                            # æå–å®Œæ•´çš„JSONæ•°ç»„
                            json_array = core[start_pos:end_pos + 1]
                            try:
                                # å°è¯•è§£æè¿™ä¸ªJSONæ•°ç»„
                                parsed_array = json.loads(json_array)
                                if isinstance(parsed_array, list):
                                    all_arrays.extend(parsed_array)
                            except Exception:
                                # å¦‚æœè§£æå¤±è´¥ï¼Œè·³è¿‡è¿™ä¸ªæ•°ç»„
                                pass
                            
                            current_pos = end_pos + 1
                        else:
                            # æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„ ]ï¼Œè·³è¿‡
                            current_pos = start_pos + 1
                    
                    # å¦‚æœæˆåŠŸæå–äº†å¤šä¸ªæ•°ç»„ï¼Œè¿”å›åˆå¹¶ç»“æœ
                    if all_arrays:
                        return all_arrays, True
                except Exception:
                    pass
            
            # å¦‚æœè¿˜æ˜¯å¤±è´¥ï¼Œè¿”å›ç©ºåˆ—è¡¨
            print(f"    JSONè§£æå¤±è´¥: {e}")
            if hasattr(e, 'pos'):
                print(f"    é”™è¯¯ä½ç½®: ç¬¬{e.lineno}è¡Œï¼Œç¬¬{e.colno}åˆ—ï¼Œå­—ç¬¦{e.pos}")
                # æ˜¾ç¤ºé”™è¯¯ä½ç½®é™„è¿‘çš„æ–‡æœ¬
                start = max(0, e.pos - 50)
                end = min(len(s0), e.pos + 50)
                print(f"    é”™è¯¯é™„è¿‘æ–‡æœ¬: ...{s0[start:end]}...")
            else:
                print(f"    åŸå§‹æ–‡æœ¬: {s0[:200]}...")
            return [], True
        except Exception as e:
            # å…¶ä»–å¼‚å¸¸
            print(f"    JSONè§£æå¤±è´¥: {e}")
            print(f"    åŸå§‹æ–‡æœ¬: {s0[:200]}...")
            return [], True
    
    def _fix_json_escapes(self, json_str: str) -> str:
        """ä¿®å¤JSONä¸­çš„éæ³•åæ–œæ ï¼šå¯¹éåˆæ³•è½¬ä¹‰å‰çš„åæ–œæ è¿›è¡ŒäºŒæ¬¡è½¬ä¹‰"""
        _HEX = set("0123456789abcdefABCDEF")
        
        out = []
        i = 0
        n = len(json_str)
        while i < n:
            ch = json_str[i]
            if ch != '\\':
                out.append(ch)
                i += 1
                continue
            # ch is backslash
            if i + 1 >= n:
                out.append('\\\\')
                i += 1
                continue
            nxt = json_str[i+1]
            if nxt in '"\\/bfnrt':
                out.append('\\' + nxt)
                i += 2
                continue
            if nxt == 'u':
                # \uXXXX - æ£€æŸ¥æ˜¯å¦æœ‰å®Œæ•´çš„4ä½åå…­è¿›åˆ¶
                if i + 5 < n:
                    hex_part = json_str[i+2:i+6]
                    if len(hex_part) == 4 and all(c in _HEX for c in hex_part):
                        out.append(json_str[i:i+6])
                        i += 6
                        continue
                    else:
                        # ä¸å®Œæ•´çš„Unicodeè½¬ä¹‰ï¼Œè½¬ä¹‰åæ–œæ 
                        out.append('\\\\u')
                        i += 2
                        continue
                else:
                    # å­—ç¬¦ä¸²æœ«å°¾çš„ä¸å®Œæ•´Unicodeè½¬ä¹‰
                    out.append('\\\\u')
                    i += 2
                    continue
            # éæ³•è½¬ä¹‰ -> åŒåæ–œæ 
            out.append('\\\\' + nxt)
            i += 2
        return ''.join(out)
    
    def _aggressive_json_fix(self, json_str: str) -> str:
        """æ¿€è¿›çš„JSONä¿®å¤æ–¹æ³•ï¼šç§»é™¤æ‰€æœ‰æœ‰é—®é¢˜çš„è½¬ä¹‰å­—ç¬¦"""
        import re
        
        # ç§»é™¤æ‰€æœ‰ä¸å®Œæ•´çš„Unicodeè½¬ä¹‰
        json_str = re.sub(r'\\u[0-9a-fA-F]{0,3}(?![0-9a-fA-F])', '', json_str)
        
        # ç§»é™¤æ‰€æœ‰æœªè½¬ä¹‰çš„åæ–œæ ï¼ˆä¿ç•™è½¬ä¹‰å­—ç¬¦ï¼‰
        # å…ˆä¿æŠ¤è½¬ä¹‰å­—ç¬¦
        protected = {}
        protected_count = 0
        
        def protect_escaped(match):
            nonlocal protected_count
            protected_count += 1
            key = f"__PROTECTED_{protected_count}__"
            protected[key] = match.group(0)
            return key
        
        # ä¿æŠ¤è½¬ä¹‰å­—ç¬¦
        json_str = re.sub(r'\\["\\/bfnrt]', protect_escaped, json_str)
        
        # ä¿æŠ¤å®Œæ•´çš„Unicodeè½¬ä¹‰
        json_str = re.sub(r'\\u[0-9a-fA-F]{4}', protect_escaped, json_str)
        
        # ç§»é™¤æ‰€æœ‰å‰©ä½™çš„åæ–œæ 
        json_str = json_str.replace('\\', '')
        
        # æ¢å¤ä¿æŠ¤çš„å­—ç¬¦
        for key, value in protected.items():
            json_str = json_str.replace(key, value)
        
        return json_str
    
    def _fix_json_format(self, json_str: str) -> str:
        """ä¿®å¤JSONæ ¼å¼é—®é¢˜"""
        import re
        
        # ä¿®å¤å¸¸è§çš„æ ¼å¼é—®é¢˜
        
        # 1. ä¿®å¤ç¼ºå°‘å†’å·çš„é—®é¢˜ï¼šåœ¨é”®ååæ·»åŠ å†’å·
        json_str = re.sub(r'"([^"]+)"\s+([^"\s,{}[\]]+)', r'"\1":\2', json_str)
        
        # 2. ä¿®å¤ç¼ºå°‘é€—å·çš„é—®é¢˜ï¼šåœ¨ } å’Œ { ä¹‹é—´æ·»åŠ é€—å·
        json_str = re.sub(r'}\s*{', '},{', json_str)
        
        # 3. ä¿®å¤ç¼ºå°‘é€—å·çš„é—®é¢˜ï¼šåœ¨ } å’Œ [ ä¹‹é—´æ·»åŠ é€—å·
        json_str = re.sub(r'}\s*\[', '},[', json_str)
        
        # 4. ä¿®å¤ç¼ºå°‘é€—å·çš„é—®é¢˜ï¼šåœ¨ ] å’Œ { ä¹‹é—´æ·»åŠ é€—å·
        json_str = re.sub(r'\]\s*{', '},{', json_str)
        
        # 5. ä¿®å¤ç¼ºå°‘é€—å·çš„é—®é¢˜ï¼šåœ¨ ] å’Œ [ ä¹‹é—´æ·»åŠ é€—å·
        json_str = re.sub(r'\]\s*\[', '],[', json_str)
        
        # 6. ä¿®å¤å¤šä½™çš„é€—å·ï¼šåœ¨ } æˆ– ] å‰ç§»é™¤å¤šä½™çš„é€—å·
        json_str = re.sub(r',\s*([}\]])', r'\1', json_str)
        
        # 7. ä¿®å¤å­—ç¬¦ä¸²æœ«å°¾çš„é€—å·
        json_str = re.sub(r',\s*([}\]])', r'\1', json_str)
        
        # 8. ä¿®å¤ç¼ºå°‘å¼•å·çš„é—®é¢˜ï¼šç¡®ä¿é”®åæœ‰å¼•å·
        json_str = re.sub(r'([{,])\s*([a-zA-Z_][a-zA-Z0-9_]*)\s*:', r'\1"\2":', json_str)
        
        # 9. ä¿®å¤å­—ç¬¦ä¸²å€¼ç¼ºå°‘å¼•å·çš„é—®é¢˜ï¼ˆç®€å•æƒ…å†µï¼‰
        # æ³¨æ„ï¼šè¿™ä¸ªæ¯”è¾ƒå±é™©ï¼Œåªåœ¨ç‰¹å®šæƒ…å†µä¸‹ä½¿ç”¨
        json_str = re.sub(r':\s*([a-zA-Z][a-zA-Z0-9_]*)\s*([,}])', r':"\1"\2', json_str)
        
        # 10. ä¿®å¤å¸¸è§çš„é”®å€¼å¯¹æ ¼å¼é—®é¢˜
        # å¤„ç† "key" value æ ¼å¼ï¼ˆç¼ºå°‘å†’å·ï¼‰
        json_str = re.sub(r'"([^"]+)"\s+([^"\s,{}[\]]+)(?=\s*[,}])', r'"\1":\2', json_str)
        
        # 11. ä¿®å¤æ•°å­—å€¼ç¼ºå°‘å¼•å·çš„é—®é¢˜ï¼ˆå¦‚æœåº”è¯¥æ˜¯å­—ç¬¦ä¸²ï¼‰
        # å¤„ç† "question_number" 1 æ ¼å¼
        json_str = re.sub(r'"question_number"\s+(\d+)(?=\s*[,}])', r'"question_number":"\1"', json_str)
        
        return json_str
    
    def _smart_json_fix(self, json_str: str) -> str:
        """æ™ºèƒ½JSONä¿®å¤ï¼šå°è¯•å¤šç§ä¿®å¤ç­–ç•¥"""
        import re
        
        # ç­–ç•¥0ï¼šå¤„ç†HTMLå†…å®¹
        # ç§»é™¤æˆ–è½¬ä¹‰HTMLæ ‡ç­¾
        json_str = re.sub(r'<[^>]+>', '', json_str)  # ç§»é™¤HTMLæ ‡ç­¾
        json_str = re.sub(r'&[a-zA-Z]+;', '', json_str)  # ç§»é™¤HTMLå®ä½“
        
        # ç­–ç•¥1ï¼šå°è¯•ä¿®å¤å¸¸è§çš„é”®å€¼å¯¹æ ¼å¼
        patterns_to_fix = [
            # "key" value -> "key": value
            (r'"([^"]+)"\s+([^"\s,{}[\]]+)(?=\s*[,}])', r'"\1":\2'),
            # "key" "value" -> "key": "value"
            (r'"([^"]+)"\s+"([^"]+)"', r'"\1":"\2"'),
            # key: value -> "key": value
            (r'([a-zA-Z_][a-zA-Z0-9_]*)\s*:\s*([^"\s,{}[\]]+)', r'"\1":\2'),
            # key: "value" -> "key": "value"
            (r'([a-zA-Z_][a-zA-Z0-9_]*)\s*:\s*"([^"]*)"', r'"\1":"\2"'),
        ]
        
        for pattern, replacement in patterns_to_fix:
            json_str = re.sub(pattern, replacement, json_str)
        
        # ç­–ç•¥2ï¼šä¿®å¤æ•°ç»„æ ¼å¼
        # ç¡®ä¿æ•°ç»„å…ƒç´ ä¹‹é—´æœ‰é€—å·
        json_str = re.sub(r'}\s*{', '},{', json_str)
        json_str = re.sub(r'\]\s*\[', '],[', json_str)
        
        # ç­–ç•¥3ï¼šä¿®å¤å¤šä½™çš„é€—å·
        json_str = re.sub(r',\s*([}\]])', r'\1', json_str)
        
        # ç­–ç•¥4ï¼šä¿®å¤å­—ç¬¦ä¸²å€¼
        # å°†æ•°å­—å€¼è½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼ˆå¦‚æœé”®åæ˜¯question_numberï¼‰
        json_str = re.sub(r'"question_number"\s*:\s*(\d+)', r'"question_number":"\1"', json_str)
        
        # ç­–ç•¥5ï¼šæ¸…ç†å¤šä½™çš„ç©ºç™½å­—ç¬¦
        json_str = re.sub(r'\s+', ' ', json_str)
        json_str = json_str.strip()
        
        return json_str
    
    def _clean_html_content(self, json_str: str) -> str:
        """æ¸…ç†JSONä¸­çš„HTMLå†…å®¹"""
        import re
        
        # ç§»é™¤HTMLæ ‡ç­¾
        json_str = re.sub(r'<[^>]+>', '', json_str)
        
        # ç§»é™¤HTMLå®ä½“
        html_entities = {
            '&amp;': '&',
            '&lt;': '<',
            '&gt;': '>',
            '&quot;': '"',
            '&#39;': "'",
            '&nbsp;': ' ',
        }
        
        for entity, replacement in html_entities.items():
            json_str = json_str.replace(entity, replacement)
        
        # ç§»é™¤å…¶ä»–HTMLå®ä½“
        json_str = re.sub(r'&[a-zA-Z]+;', '', json_str)
        json_str = re.sub(r'&#\d+;', '', json_str)
        
        # æ¸…ç†å¤šä½™çš„ç©ºç™½å­—ç¬¦
        json_str = re.sub(r'\s+', ' ', json_str)
        json_str = json_str.strip()
        
        return json_str
    
    def send_chunk_adaptive(self, s_line: int, e_line: int, chunk_text: str,
                           max_retries: int, retry_backoff: float, timeout: int,
                           debug_dir: Optional[Path] = None, base_norm: str = "",
                           prompt_template: str = QUESTION_CHUNK_PROMPT_TEMPLATE,
                           system_prompt: str = QUESTION_SYSTEM_PROMPT,
                           task_type: str = "questions") -> Tuple[str, list, dict]:
        compressed = self.compress_chunk_text(chunk_text, MAX_CHARS_PER_PROMPT)
        prompt = prompt_template.format(
            start_line=s_line, end_line=e_line, chunk_text=compressed
        )
        meta = {
            "prompt_len": len(prompt), "response_len": 0,
            "parse_error": "", "sanitized_used": False,
            "compressed_chunk": compressed
        }

        # è°ƒè¯•ç›®å½• - æ ¹æ®ä»»åŠ¡ç±»å‹åˆ›å»ºå­ç›®å½•
        wdir = None
        if debug_dir:
            wdir = debug_dir / base_norm / task_type / f"{s_line}-{e_line}"
            self.write_text(wdir / "prompt.txt", prompt)  # é€ç»™æ¨¡å‹çš„å®Œæ•´ç”¨æˆ·æ¶ˆæ¯ï¼ˆå«ç‰‡æ®µï¼‰

        # ç›´æ¥é‡è¯•
        attempt = 0
        last_err = None
        while attempt <= max_retries:
            try:
                raw = self.call_ark(prompt, system_prompt=system_prompt, timeout=timeout)
                if debug_dir:
                    self.write_text(wdir / "response.txt", raw)
                meta["response_len"] = len(raw)
                obj, sanitized_used = self.force_json_load_with_sanitize(raw)
                meta["sanitized_used"] = sanitized_used
                if debug_dir and sanitized_used:
                    # ä¿å­˜æ¸…æ´—åçš„ JSON æ–‡æœ¬
                    self.write_text(wdir / "sanitized.json", json.dumps(obj, ensure_ascii=False, indent=2))
                return raw, obj, meta
            except Exception as e:
                last_err = str(e)
                meta["parse_error"] = last_err
                if debug_dir:
                    self.write_text(wdir / "parse_error.txt", last_err)
                if attempt == max_retries:
                    break
                time.sleep((retry_backoff ** attempt) + random.uniform(0.1, 0.5))
                attempt += 1

        return f"<<FAILED {s_line}-{e_line}>> {last_err}", [], meta

    def extract_all_exams(self, window: int = DEFAULT_WINDOW_LINES, stride: int = DEFAULT_STRIDE_LINES,
                         max_retries: int = 3, timeout: int = DEFAULT_ARK_TIMEOUT,
                         max_workers: int = 1,
                         debug_dir: Optional[Path] = None,
                         raw_dir: Optional[Path] = None,
                         output_dir: Optional[Path] = None) -> List[ExamData]:
        exam_dirs = self.get_exam_directories()
        
        if not exam_dirs:
            print("æœªæ‰¾åˆ°è€ƒè¯•ç›®å½•")
            return []
        
        all_exams_data = []
        
        # æ·»åŠ æ€»çš„è¿›åº¦æ¡
        total_pbar = tqdm(total=len(exam_dirs), desc="æ€»ä½“è¿›åº¦", ncols=100, position=0)
        

        with cf.ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = {
                executor.submit(
                    self.process_exam_directory,
                    exam_dir=exam_dir,
                    window=window,
                    stride=stride,
                    max_retries=max_retries,
                    timeout=timeout,
                    debug_dir=debug_dir,
                    raw_dir=raw_dir
                ): exam_dir
                for exam_dir in exam_dirs
            }
            
            for fut in cf.as_completed(futures):
                exam_dir = futures[fut]
                try:
                    exam_data = fut.result()
                    all_exams_data.append(exam_data)
                    
                    # è‡ªåŠ¨ä¿å­˜æ¯ä¸ªè€ƒè¯•çš„ç»“æœ
                    if output_dir:
                        self.save_single_exam(exam_data, output_dir)
                        # åŒæ—¶ä¿å­˜ç´¯ç§¯ç»“æœ
                        self.save_to_json(all_exams_data, str(output_dir / "qa_extracted_incremental.json"))
                        print(f"    ğŸ’¾ è‡ªåŠ¨ä¿å­˜å®Œæˆ: {len(all_exams_data)} ä¸ªè€ƒè¯•")
                        
                except Exception as e:
                    print(f"âŒ å¤„ç† {exam_dir.name} æ—¶å‡ºé”™: {e}")
                finally:
                    total_pbar.update(1)
        
        total_pbar.close()
        return all_exams_data
    
    def process_exam_directory(self, exam_dir: Path, window: int = DEFAULT_WINDOW_LINES,
                              stride: int = DEFAULT_STRIDE_LINES, max_retries: int = 3,
                              timeout: int = DEFAULT_ARK_TIMEOUT,
                              debug_dir: Optional[Path] = None,
                              raw_dir: Optional[Path] = None) -> ExamData:
        exam_name = exam_dir.name
        questions_file = exam_dir / 'questions.md'
        answers_file = exam_dir / 'answers.md'
        
        
        # æå–é¢˜ç›®
        questions, questions_raw_records = self.extract_questions_from_file(
            questions_file, window=window, stride=stride,
            max_retries=max_retries, timeout=timeout, debug_dir=debug_dir,
            exam_name=exam_name
        )
        
        # ä¿å­˜é¢˜ç›®åŸå§‹è®°å½•
        if raw_dir:
            questions_raw_path = raw_dir / exam_name / "questions_raw_chunks.jsonl"
            self.save_raw_chunks(questions_raw_records, questions_raw_path)
            
        answers, answers_raw_records = self.extract_answers_from_file(
            answers_file, window=window, stride=stride,
            max_retries=max_retries, timeout=timeout, debug_dir=debug_dir,
            exam_name=exam_name
        )
        
        # è°ƒè¯•ä¿¡æ¯
        if debug_dir:
            answers_debug_dir = debug_dir / exam_name / "answers"
        
        # ä¿å­˜ç­”æ¡ˆåŸå§‹è®°å½•
        if raw_dir:
            answers_raw_path = raw_dir / exam_name / "answers_raw_chunks.jsonl"
            self.save_raw_chunks(answers_raw_records, answers_raw_path)
        
        # éªŒè¯åŒ¹é…æƒ…å†µ
        questions, missing_answers, extra_answers = self.validate_qa_matching(
            questions, answers, exam_name=exam_name, raw_dir=raw_dir
        )
        
        return ExamData(
            exam_name=exam_name,
            total_questions=len(questions),
            questions=questions,
            missing_answers=missing_answers,
            extra_answers=extra_answers
        )
    
    def extract_questions_from_file(self, file_path: Path, window: int = DEFAULT_WINDOW_LINES,
                                   stride: int = DEFAULT_STRIDE_LINES, max_retries: int = 3,
                                   timeout: int = DEFAULT_ARK_TIMEOUT,
                                   debug_dir: Optional[Path] = None,
                                   exam_name: str = "") -> Tuple[List[Question], List[Dict[str, Any]]]:
        if not file_path.exists():
            print(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return [], []
        
        text = file_path.read_text(encoding='utf-8', errors='ignore')
        lines = text.splitlines()
        windows = self.make_windows(lines, window=window, stride=stride)
        
        all_questions = []
        seen_questions = set()
        raw_records = []
        
        pbar = tqdm(total=len(windows), desc=f"æå–é¢˜ç›® {exam_name} {file_path.name}", ncols=100, position=1)
        
        for s_line, e_line, chunk in windows:
            raw_out, data, meta = self.send_chunk_adaptive(
                s_line=s_line, e_line=e_line, chunk_text=chunk,
                max_retries=max_retries, retry_backoff=1.8, timeout=timeout,
                debug_dir=debug_dir, base_norm=exam_name,
                task_type="questions"
            )
            
            # ä¿å­˜åŸå§‹è®°å½•
            raw_records.append({
                "window": f"{s_line}-{e_line}",
                "chunk_text": chunk,
                "compressed_chunk": meta["compressed_chunk"],
                "prompt_length": meta["prompt_len"],
                "model_output": raw_out,
                "response_length": meta["response_len"],
                "sanitized_used": meta["sanitized_used"],
                "parse_error": meta["parse_error"],
                "parsed_questions": data
            })
            
            for obj in data:
                question_text = str(obj.get("question") or "").strip()
                qtype = str(obj.get("type") or "").strip()
                question_number = int(obj.get("question_number") or 0)
                
                # å»é‡ï¼ˆä½¿ç”¨é¢˜å·+é¢˜ç›®å†…å®¹ï¼‰
                question_hash = hash(f"{question_number}_{question_text}")
                if question_hash in seen_questions:
                    continue
                seen_questions.add(question_hash)
                
                question = Question(
                    question_number=question_number,
                    question_type=qtype,
                    question_text=re.sub(r'\s+', ' ', question_text.strip()),
                    answer="",
                    explanation="",
                    knowledge_points="",
                    source_file=file_path.stem,
                    source_window=(s_line, e_line)
                )
                all_questions.append(question)
            
            pbar.update(1)
        pbar.close()
        
        # æŒ‰é¢˜å·æ’åº
        all_questions.sort(key=lambda x: x.question_number)
        
        return all_questions, raw_records
    
    def extract_answers_from_file(self, answers_file: Path, window: int = DEFAULT_WINDOW_LINES,
                                 stride: int = DEFAULT_STRIDE_LINES, max_retries: int = 3,
                                 timeout: int = DEFAULT_ARK_TIMEOUT,
                                 debug_dir: Optional[Path] = None,
                                 exam_name: str = "") -> Tuple[List[Question], List[Dict[str, Any]]]:
        if not answers_file.exists():
            print(f"ç­”æ¡ˆæ–‡ä»¶ä¸å­˜åœ¨: {answers_file}")
            return [], []
        
        text = answers_file.read_text(encoding='utf-8', errors='ignore')
        lines = text.splitlines()
        windows = self.make_windows(lines, window=window, stride=stride)
        
        all_answers = []
        seen_answers = set()
        raw_records = []
        
        pbar = tqdm(total=len(windows), desc=f"æå–ç­”æ¡ˆ {exam_name} {answers_file.name}", ncols=100, position=1)
        
        for s_line, e_line, chunk in windows:
            raw_out, data, meta = self.send_chunk_adaptive(
                s_line=s_line, e_line=e_line, chunk_text=chunk,
                max_retries=max_retries, retry_backoff=1.8, timeout=timeout,
                debug_dir=debug_dir, base_norm=exam_name,
                prompt_template=ANSWER_CHUNK_PROMPT_TEMPLATE,
                system_prompt=ANSWER_SYSTEM_PROMPT,
                task_type="answers"
            )
            
            # ä¿å­˜åŸå§‹è®°å½•
            raw_records.append({
                "window": f"{s_line}-{e_line}",
                "chunk_text": chunk,
                "compressed_chunk": meta["compressed_chunk"],
                "prompt_length": meta["prompt_len"],
                "model_output": raw_out,
                "response_length": meta["response_len"],
                "sanitized_used": meta["sanitized_used"],
                "parse_error": meta["parse_error"],
                "parsed_answers": data
            })
            
            if not isinstance(data, list):
                data = []
            
            # æ·»åŠ è°ƒè¯•ä¿¡æ¯
            if len(data) > 0:
                print(f"    çª—å£ {s_line}-{e_line}: è§£æåˆ° {len(data)} ä¸ªç­”æ¡ˆå¯¹è±¡")
                if debug_dir:
                    # ä¿å­˜æ¨¡å‹è¿”å›çš„åŸå§‹æ•°æ®ç”¨äºè°ƒè¯•
                    debug_file = debug_dir / exam_name / "answers" / f"{s_line}-{e_line}_model_response.json"
                    debug_file.parent.mkdir(parents=True, exist_ok=True)
                    with debug_file.open("w", encoding="utf-8") as f:
                        json.dump(data, f, ensure_ascii=False, indent=2)
            else:
                print(f"    çª—å£ {s_line}-{e_line}: æœªè§£æåˆ°ç­”æ¡ˆå¯¹è±¡")
            
            for obj in data:
                try:
                    # å®‰å…¨åœ°è·å–é¢˜å·
                    question_number_raw = obj.get("question_number")
                    if question_number_raw is None:
                        print(f"    è­¦å‘Š: è·³è¿‡ç¼ºå°‘é¢˜å·çš„ç­”æ¡ˆå¯¹è±¡: {obj}")
                        continue
                    
                    try:
                        question_number = int(question_number_raw)
                    except (ValueError, TypeError):
                        print(f"    è­¦å‘Š: è·³è¿‡æ— æ•ˆé¢˜å·çš„ç­”æ¡ˆå¯¹è±¡: {obj}")
                        continue
                    
                    if question_number <= 0:
                        print(f"    è­¦å‘Š: è·³è¿‡é¢˜å·å°äºç­‰äº0çš„ç­”æ¡ˆå¯¹è±¡: {obj}")
                        continue
                    
                    answer = str(obj.get("answer") or "").strip()
                    explanation = str(obj.get("explanation") or "").strip()
                    knowledge_points = str(obj.get("knowledge_points") or "").strip()
                    
                    # å»é‡
                    answer_hash = hash(f"{question_number}_{answer}")
                    if answer_hash in seen_answers:
                        continue
                    seen_answers.add(answer_hash)

                    answer_obj = Question(
                        question_number=question_number,
                        question_type="",
                        question_text="",
                        answer=answer,
                        explanation=explanation,
                        knowledge_points=knowledge_points,
                        source_file=answers_file.stem,
                        source_window=(s_line, e_line)
                    )
                    all_answers.append(answer_obj)
                except Exception as e:
                    print(f"    è­¦å‘Š: å¤„ç†ç­”æ¡ˆå¯¹è±¡æ—¶å‡ºé”™: {e}, å¯¹è±¡: {obj}")
                    continue
            
            pbar.update(1)
        pbar.close()
        # æŒ‰é¢˜å·æ’åº
        all_answers.sort(key=lambda x: x.question_number)        
        
        return all_answers, raw_records
    
    def validate_qa_matching(self, questions: List[Question], answers: List[Question], 
                           exam_name: str = "", raw_dir: Optional[Path] = None) -> Tuple[List[Question], List[int], List[int]]:
        # å¤„ç†é‡å¤çš„é¢˜ç›®å’Œç­”æ¡ˆ
        questions_dict = {}
        answers_dict = {}
        
        # å¤„ç†é‡å¤é¢˜ç›®ï¼šéšæœºé€‰æ‹©ä¸€ä¸ªä¿ç•™
        for question in questions:
            if question.question_number in questions_dict:
                if random.random() < 0.5:
                    continue  # ä¿ç•™ç°æœ‰çš„
                else:
                    # æ›¿æ¢ç°æœ‰çš„é¢˜ç›®
                    pass  # ç›´æ¥è¦†ç›–
            questions_dict[question.question_number] = question
        
        # å¤„ç†é‡å¤ç­”æ¡ˆï¼šéšæœºé€‰æ‹©ä¸€ä¸ªä¿ç•™
        for answer in answers:
            if answer.question_number in answers_dict:
                if random.random() < 0.5:
                    continue  # ä¿ç•™ç°æœ‰çš„
                else:
                    # æ›¿æ¢ç°æœ‰çš„ç­”æ¡ˆ
                    pass  # ç›´æ¥è¦†ç›–
            answers_dict[answer.question_number] = answer
        
        # è·å–å»é‡åçš„é¢˜å·å’Œç­”æ¡ˆå·
        question_numbers = set(questions_dict.keys())
        answer_numbers = set(answers_dict.keys())
        
        missing_answers = list(question_numbers - answer_numbers)
        extra_answers = list(answer_numbers - question_numbers)
        
        # ä¸ºé¢˜ç›®æ·»åŠ ç­”æ¡ˆ
        final_questions = []
        for question_number, question in questions_dict.items():
            if question_number in answers_dict:
                answer_obj = answers_dict[question_number]
                question.answer = answer_obj.answer
                question.explanation = answer_obj.explanation
                question.knowledge_points = answer_obj.knowledge_points
            else:
                question.answer = ""
                question.explanation = ""
                question.knowledge_points = ""
            final_questions.append(question)
        
        # æŒ‰é¢˜å·æ’åº
        final_questions.sort(key=lambda x: x.question_number)
        
        # ä¿å­˜å¯¹æ¯”åŸå§‹è¾“å…¥å’Œç»Ÿè®¡ç»“æœ
        if raw_dir:
            validation_stats = {
                "exam_name": exam_name,
                "original_questions_count": len(questions),
                "original_answers_count": len(answers),
                "deduplicated_questions_count": len(questions_dict),
                "deduplicated_answers_count": len(answers_dict),
                "question_number_range": {
                    "min": min(question_numbers) if question_numbers else 0,
                    "max": max(question_numbers) if question_numbers else 0
                },
                "answer_number_range": {
                    "min": min(answer_numbers) if answer_numbers else 0,
                    "max": max(answer_numbers) if answer_numbers else 0
                },
                "matched_questions_count": len(question_numbers & answer_numbers),
                "missing_answers": missing_answers,
                "extra_answers": extra_answers,
                "duplicate_questions_removed": len(questions) - len(questions_dict),
                "duplicate_answers_removed": len(answers) - len(answers_dict),
                "duplicate_questions_details": self._find_duplicates(questions),
                "duplicate_answers_details": self._find_duplicates(answers)
            }
            
            validation_path = raw_dir / exam_name / "validation_stats.json"
            validation_path.parent.mkdir(parents=True, exist_ok=True)
            with validation_path.open("w", encoding="utf-8") as f:
                json.dump(validation_stats, f, ensure_ascii=False, indent=2)
        
        # æ‰“å°åŒ¹é…ç»Ÿè®¡
        print(f"    åŸå§‹é¢˜ç›®æ•°: {len(questions)} -> å»é‡å: {len(questions_dict)}")
        print(f"    åŸå§‹ç­”æ¡ˆæ•°: {len(answers)} -> å»é‡å: {len(answers_dict)}")
        print(f"    é¢˜ç›®é¢˜å·èŒƒå›´: {min(question_numbers) if question_numbers else 0} - {max(question_numbers) if question_numbers else 0}")
        print(f"    ç­”æ¡ˆé¢˜å·èŒƒå›´: {min(answer_numbers) if answer_numbers else 0} - {max(answer_numbers) if answer_numbers else 0}")
        print(f"    åŒ¹é…çš„é¢˜ç›®æ•°: {len(question_numbers & answer_numbers)}")
        print(f"    é‡å¤é¢˜ç›®ç§»é™¤: {len(questions) - len(questions_dict)}")
        print(f"    é‡å¤ç­”æ¡ˆç§»é™¤: {len(answers) - len(answers_dict)}")
        
        return final_questions, missing_answers, extra_answers
    
    def _find_duplicates(self, items: List[Question]) -> Dict[int, List[Dict]]:
        """æŸ¥æ‰¾é‡å¤é¡¹å¹¶è¿”å›è¯¦ç»†ä¿¡æ¯"""
        duplicates = {}
        seen = {}
        
        for item in items:
            if item.question_number in seen:
                if item.question_number not in duplicates:
                    duplicates[item.question_number] = [seen[item.question_number]]
                duplicates[item.question_number].append({
                    "question_number": item.question_number,
                    "question_text": item.question_text[:100] + "..." if len(item.question_text) > 100 else item.question_text,
                    "answer": item.answer,
                    "source_window": item.source_window,
                    "source_file": item.source_file
                })
            else:
                seen[item.question_number] = {
                    "question_number": item.question_number,
                    "question_text": item.question_text[:100] + "..." if len(item.question_text) > 100 else item.question_text,
                    "answer": item.answer,
                    "source_window": item.source_window,
                    "source_file": item.source_file
                }
        
        return duplicates
    
    def save_to_json(self, data: List[ExamData], output_file: str):
        serializable_data = []
        for exam in data:
            exam_dict = {
                'exam_name': exam.exam_name,
                'total_questions': exam.total_questions,
                'missing_answers': exam.missing_answers,
                'extra_answers': exam.extra_answers,
                'questions': []
            }
            
            for question in exam.questions:
                question_dict = {
                    'question_number': question.question_number,
                    'question_type': question.question_type,
                    'question_text': question.question_text,
                    'answer': question.answer,
                    'explanation': question.explanation,
                    'knowledge_points': question.knowledge_points,
                    'source_file': question.source_file,
                    'source_window': question.source_window
                }
                exam_dict['questions'].append(question_dict)
                
            serializable_data.append(exam_dict)
            
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(serializable_data, f, ensure_ascii=False, indent=2)
        print(f"æ•°æ®å·²ä¿å­˜åˆ°: {output_file}")
    
    def save_to_jsonl(self, data: List[ExamData], output_file: str):
        """ä¿å­˜ä¸ºJSONLæ ¼å¼ï¼Œæ¯è¡Œä¸€ä¸ªé¢˜ç›®"""
        with open(output_file, 'w', encoding='utf-8') as f:
            for exam in data:
                for question in exam.questions:
                    # åˆ›å»ºæ–°çš„æ•°æ®ç»“æ„ï¼ŒæŒ‰ç…§ä½ çš„è¦æ±‚é‡æ–°ç»„ç»‡
                    new_question = {
                        'qid': question.question_number,
                        'question': question.question_text,
                        'answer': question.answer,
                        'explanation': question.explanation,
                        'knowledge_points': question.knowledge_points,
                        'source_file': exam.exam_name,  # ä½¿ç”¨exam_nameæ›¿æ¢source_file
                        'source_window': question.source_window
                    }
                    # å†™å…¥JSONLæ ¼å¼ï¼ˆæ¯è¡Œä¸€ä¸ªJSONå¯¹è±¡ï¼‰
                    f.write(json.dumps(new_question, ensure_ascii=False) + '\n')
        print(f"æ•°æ®å·²ä¿å­˜åˆ°: {output_file}")
    
    def save_single_exam(self, exam_data: ExamData, output_dir: Path):
        """ä¿å­˜å•ä¸ªè€ƒè¯•çš„ç»“æœ"""
        exam_file = output_dir / f"{exam_data.exam_name}_qa.json"
        exam_dict = {
            'exam_name': exam_data.exam_name,
            'total_questions': exam_data.total_questions,
            'missing_answers': exam_data.missing_answers,
            'extra_answers': exam_data.extra_answers,
            'questions': []
        }
        
        for question in exam_data.questions:
            question_dict = {
                'question_number': question.question_number,
                'question_type': question.question_type,
                'question_text': question.question_text,
                'answer': question.answer,
                'explanation': question.explanation,
                'knowledge_points': question.knowledge_points,
                'source_file': question.source_file,
                'source_window': question.source_window
            }
            exam_dict['questions'].append(question_dict)
        
        with open(exam_file, 'w', encoding='utf-8') as f:
            json.dump(exam_dict, f, ensure_ascii=False, indent=2)
        print(f"    ğŸ’¾ å•ä¸ªè€ƒè¯•ä¿å­˜å®Œæˆ: {exam_file}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="ä½¿ç”¨AIæ¨¡å‹æå–è€ƒè¯•é¢˜ç›®å’Œç­”æ¡ˆ")
    parser.add_argument("--base-path", type=str, help="è€ƒè¯•çœŸé¢˜ç›®å½•çš„åŸºç¡€è·¯å¾„")
    parser.add_argument("--window", type=int, default=DEFAULT_WINDOW_LINES,
                       help="æ»‘åŠ¨çª—å£å¤§å°ï¼ˆè¡Œæ•°ï¼‰")
    parser.add_argument("--stride", type=int, default=DEFAULT_STRIDE_LINES,
                       help="æ»‘åŠ¨çª—å£æ­¥é•¿ï¼ˆè¡Œæ•°ï¼‰")
    parser.add_argument("--max-retries", type=int, default=3,
                       help="AIè°ƒç”¨æœ€å¤§é‡è¯•æ¬¡æ•°")
    parser.add_argument("--timeout", type=int, default=DEFAULT_ARK_TIMEOUT,
                       help="AIè°ƒç”¨è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰")
    parser.add_argument("--max-workers", type=int, default=256,
                       help="å¹¶å‘å¤„ç†æ•°")
    parser.add_argument("--output-dir", type=str, default=".",
                       help="è¾“å‡ºç›®å½•")
    parser.add_argument("--save-raw-dir", type=str, default="",
                       help="ä¿å­˜åŸæ–‡ç‰‡æ®µå¯¹ç…§çš„ç›®å½•ï¼ˆæ¯æ–‡ä»¶ raw_chunks.jsonlï¼‰")
    parser.add_argument("--debug-log-dir", type=str, default="",
                       help="ä¿å­˜æ¯ä¸ªçª—å£çš„ prompt/response/parse_error/sanitized.json")

    
    args = parser.parse_args()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # åˆ›å»ºè°ƒè¯•å’ŒåŸå§‹è®°å½•ç›®å½•
    debug_dir = Path(args.debug_log_dir) if args.debug_log_dir else None
    raw_dir = Path(args.save_raw_dir) if args.save_raw_dir else None
    
    # åˆ›å»ºæå–å™¨
    extractor = QAExtractor(args.base_path)
    
    # æå–æ‰€æœ‰è€ƒè¯•æ•°æ®
    all_exams_data = extractor.extract_all_exams(
        window=args.window,
        stride=args.stride,
        max_retries=args.max_retries,
        timeout=args.timeout,
        max_workers=args.max_workers,
        debug_dir=debug_dir,
        raw_dir=raw_dir,
        output_dir=output_dir
    )
    
    if not all_exams_data:
        print("æœªæå–åˆ°ä»»ä½•æ•°æ®")
        return
    
    # ä¿å­˜ç»“æœ
    output_file = output_dir / "qa_extracted.jsonl"
    extractor.save_to_jsonl(all_exams_data, str(output_file))


if __name__ == "__main__":
    main()




'''python 3.2_qa_apart.py \
  --base-path /home/wangxi/workspace/gongye/yijizaojia/qna_split \
  --window 80 \
  --stride 60 \
  --max-retries 3 \
  --timeout 120 \
  --max-workers 256 \
  --output-dir yijizaojia/qa_apart_output_0819 \
  --save-raw-dir yijizaojia/raw_logs_0819 \
  --debug-log-dir yijizaojia/debug_logs_0819'''