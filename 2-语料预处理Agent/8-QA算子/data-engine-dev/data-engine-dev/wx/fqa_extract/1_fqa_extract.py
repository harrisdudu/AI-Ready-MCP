import json
import time
import argparse
import random
import requests
from dataclasses import dataclass, asdict
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

from tqdm import tqdm
import concurrent.futures as cf

# ========= Ark é…ç½® =========
API_KEY  = "c702676e-a69f-4ff0-a672-718d0d4723ed"
MODEL_ID = "doubao-seed-1-6-250615"

# ========= Embedding é…ç½® =========
EMBEDDING_API_URLS = [
    "http://localhost:7100/v1/embeddings",
    "http://localhost:7101/v1/embeddings", 
    "http://localhost:7102/v1/embeddings",
    "http://localhost:7103/v1/embeddings",
    "http://localhost:7104/v1/embeddings",
    "http://localhost:7105/v1/embeddings",
    "http://localhost:7106/v1/embeddings",
    "http://localhost:7107/v1/embeddings"
]
EMBEDDING_MODEL_NAME = "qwen3-8b-embd"
SIMILARITY_THRESHOLD = 0.99

try:
    from volcenginesdkarkruntime import Ark
except Exception:
    raise SystemExit("âŒ è¯·å…ˆå®‰è£… Ark SDK: pip install 'volcengine-python-sdk[ark]'")


try:
    from docx import Document
except Exception:
    raise SystemExit("âŒ è¯·å…ˆå®‰è£… python-docx: pip install python-docx")


# ========= å‚æ•°å¸¸é‡ =========
DEFAULT_WINDOW_LINES = 80    # ç¨å¾®ä¿å®ˆï¼Œå‡å°‘é•¿çª—è¶…æ—¶
DEFAULT_STRIDE_LINES = 40
SUPPORTED_TYPES = {"single", "multiple", "judge", "fill"}

# å•æ¬¡è¯·æ±‚æœ€å¤§å­—ç¬¦é¢„ç®—ï¼ˆé¿å…è¶…å¤§çª—å£å¯¼è‡´è¶…æ—¶/è¶…é•¿ï¼‰
MAX_CHARS_PER_PROMPT = 9000
# Ark é»˜è®¤è¯·æ±‚è¶…æ—¶ï¼ˆç§’ï¼‰
DEFAULT_ARK_TIMEOUT = 120

# ========= System/User Prompt =========
SYSTEM_PROMPT = """\
ä½ æ˜¯ä¸€ä¸ªç²¾ç¡®çš„é—®ç­”å¯¹æŠ½å–åŠ©æ‰‹ã€‚
æˆ‘å°†ç»™ä½ ä¸€æ®µåŒ…å«é—®ç­”å’Œå‚è€ƒä¿¡æ¯çš„æ–‡æœ¬ç‰‡æ®µã€‚
ä½ çš„ä»»åŠ¡ï¼šæŠ½å–å…¶ä¸­çš„é—®ç­”å¯¹å’Œå¯¹åº”çš„å‚è€ƒä¿¡æ¯ã€‚

æ–‡æœ¬å¯èƒ½åŒ…å«å¤šç§æ ¼å¼ï¼š
1) æ ‡å‡†é—®ç­”æ ¼å¼ï¼šæ˜ç¡®çš„é—®é¢˜å’Œç­”æ¡ˆ
2) é‡‡è®¿å¯¹è¯æ ¼å¼ï¼šAé—®Bç­”çš„å¯¹è¯å½¢å¼
3) æ•™å­¦é—®ç­”æ ¼å¼ï¼šè€å¸ˆæé—®å­¦ç”Ÿå›ç­”
4) å…¶ä»–å¯¹è¯å½¢å¼ï¼šåŒ…å«é—®ç­”å†…å®¹çš„å¯¹è¯

æŠ½å–è§„åˆ™ï¼š
1) æ¯ä¸ªé—®ç­”å¯¹å¿…é¡»åŒ…å«ï¼š
   - "id"ï¼šåœ¨å½“å‰ç‰‡æ®µå†…çš„å±€éƒ¨é€’å¢ç¼–å·ï¼ˆå­—ç¬¦ä¸²æˆ–æ•°å­—ï¼‰
   - "question"ï¼šé—®é¢˜çš„å®Œæ•´æ–‡å­—ï¼ˆå¯èƒ½æ˜¯ç›´æ¥æé—®ï¼Œä¹Ÿå¯èƒ½æ˜¯å¯¹è¯ä¸­çš„è¯¢é—®ï¼‰
   - "answer"ï¼šå¯¹åº”çš„ç­”æ¡ˆæ–‡å­—ï¼ˆå¯èƒ½æ˜¯ç›´æ¥å›ç­”ï¼Œä¹Ÿå¯èƒ½æ˜¯å¯¹è¯ä¸­çš„å›åº”ï¼‰
   - "reference"ï¼šç›¸å…³çš„å‚è€ƒä¿¡æ¯ã€æ¥æºæˆ–ä¸Šä¸‹æ–‡ï¼ˆå¦‚æ— åˆ™ä¸ºç©ºå­—ç¬¦ä¸²ï¼‰
2) å¯¹äºå¯¹è¯æ ¼å¼ï¼š
   - å°†å¯¹è¯ä¸­çš„è¯¢é—®è¯†åˆ«ä¸ºé—®é¢˜
   - å°†å¯¹è¯ä¸­çš„å›åº”è¯†åˆ«ä¸ºç­”æ¡ˆ
   - ä¿æŒå¯¹è¯çš„åŸå§‹è¯­å¢ƒå’Œå®Œæ•´æ€§
3) åªè¾“å‡ºå®Œæ•´çš„é—®ç­”å¯¹ï¼šå¿…é¡»æœ‰å®Œæ•´çš„é—®é¢˜å’Œç­”æ¡ˆã€‚
4) å¦‚æœé—®ç­”å¯¹è¢«æˆªæ–­æˆ–ä¸å®Œæ•´ï¼Œåˆ™ä¸è¦è¾“å‡ºã€‚
5) ä¸¥æ ¼è¾“å‡º JSON æ ¼å¼ï¼ŒUTF-8 ç¼–ç ï¼Œä¸è¦åŒ…å« Markdownã€ä»£ç å—å›´æ æˆ–å¤šä½™æ–‡å­—ã€‚
6) ä¸è¦ç¼–é€ å†…å®¹ã€‚å‚è€ƒä¿¡æ¯ç¼ºå¤±æ—¶ç”¨ç©ºå­—ç¬¦ä¸²è¡¨ç¤ºã€‚
7) å¦‚æœç‰‡æ®µä¸­æ²¡æœ‰ç¬¦åˆæ¡ä»¶çš„é—®ç­”å¯¹ï¼Œè¿”å›ç©ºæ•°ç»„ []ã€‚

è¾“å‡ºæ ¼å¼ï¼š
JSON æ•°ç»„ï¼Œæ•°ç»„ä¸­æ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªé—®ç­”å¯¹å¯¹è±¡ã€‚
"""

CHUNK_PROMPT_TEMPLATE = """\
ä¸‹é¢æ˜¯å…¨æ–‡çš„ç¬¬ {start_line} è¡Œåˆ°ç¬¬ {end_line} è¡Œçš„æ–‡æœ¬å†…å®¹ã€‚
è¯·ä»”ç»†åˆ†ææ–‡æœ¬ç»“æ„ï¼Œæå–å…¶ä¸­ç¬¦åˆè¦æ±‚ä¸”å®Œæ•´çš„é—®ç­”å¯¹ã€‚

--- å¼€å§‹ç‰‡æ®µ ---
{chunk_text}
--- ç»“æŸç‰‡æ®µ ---

è¯·è®°ä½ï¼š
- è¯†åˆ«å„ç§æ ¼å¼çš„é—®ç­”ï¼šæ ‡å‡†é—®ç­”ã€å¯¹è¯å½¢å¼ã€é‡‡è®¿å½¢å¼ç­‰
- å¯¹äºå¯¹è¯æ ¼å¼ï¼Œå°†è¯¢é—®è¯†åˆ«ä¸ºé—®é¢˜ï¼Œå›åº”è¯†åˆ«ä¸ºç­”æ¡ˆ
- ä¿æŒé—®ç­”çš„å®Œæ•´æ€§å’ŒåŸå§‹è¯­å¢ƒ
- åªè¿”å›ä¸¥æ ¼ JSON æ•°ç»„ï¼Œä¸è¦è¾“å‡ºä»»ä½•é¢å¤–æ–‡å­—
- æ¯ä¸ªé—®ç­”å¯¹å¿…é¡»åŒ…å«å®Œæ•´çš„é—®é¢˜å’Œç­”æ¡ˆ
"""

# ========= æ•°æ®ç»“æ„ =========
@dataclass
class QAItem:
    qid: Optional[str]
    question: str
    answer: str
    reference: str
    source_window: Tuple[int, int]
    window_local_id: Optional[str]
    source_file: str = ""

@dataclass
class ExtractStats:
    windows: int = 0
    raw_questions: int = 0
    kept_questions: int = 0

@dataclass
class ClusterLog:
    cluster_id: int
    representative_question: str
    cluster_size: int
    similarity_threshold: float
    kept_item: QAItem
    merged_items: List[QAItem]
    similarities: List[float]
    all_cluster_items: List[QAItem]  # èšç±»å‰çš„æ‰€æœ‰é—®é¢˜
    answer_groups: Dict[str, List[Tuple[int, str]]]  # ç­”æ¡ˆåˆ†ç»„ä¿¡æ¯

# ========= Embedding æ¨¡å‹ =========
class EmbeddingModel:
    def __init__(self, api_urls: List[str], model_name: str):
        self.api_urls = api_urls
        self.model_name = model_name
        self.working_urls = []
        self._test_apis()
    
    def _test_apis(self):
        """æµ‹è¯•APIè¿æ¥"""
        print(f"ğŸ”„ æµ‹è¯•embedding APIè¿æ¥: {len(self.api_urls)} ä¸ªç«¯ç‚¹")
        for i, url in enumerate(self.api_urls):
            try:
                test_response = requests.post(
                    url,
                    headers={"Content-Type": "application/json"},
                    json={
                        "input": "æµ‹è¯•",
                        "model": self.model_name
                    },
                    timeout=10
                )
                test_response.raise_for_status()
                print(f"  âœ… ç«¯ç‚¹ {i+1} æµ‹è¯•æˆåŠŸ: {url}")
                self.working_urls.append(url)
            except requests.exceptions.RequestException as e:
                print(f"  âŒ ç«¯ç‚¹ {i+1} æµ‹è¯•å¤±è´¥: {url} - {e}")
        
        if not self.working_urls:
            raise Exception("âŒ æ‰€æœ‰embedding APIç«¯ç‚¹éƒ½è¿æ¥å¤±è´¥ï¼Œè¯·ç¡®ä¿embeddingæœåŠ¡æ­£åœ¨è¿è¡Œ")
        
        print(f"âœ… æˆåŠŸè¿æ¥ {len(self.working_urls)} ä¸ªembedding APIç«¯ç‚¹")
    
    def get_embedding(self, text: str, url_index: int = 0) -> List[float]:
        """è·å–å•ä¸ªæ–‡æœ¬çš„embeddingï¼Œæ”¯æŒè½®è¯¢å¤šä¸ªAPIç«¯ç‚¹"""
        if not text.strip():
            return []
        
        # è½®è¯¢å°è¯•æ‰€æœ‰APIç«¯ç‚¹
        for attempt in range(len(self.working_urls)):
            current_url_index = (url_index + attempt) % len(self.working_urls)
            api_url = self.working_urls[current_url_index]
            
            try:
                response = requests.post(
                    api_url,
                    headers={"Content-Type": "application/json"},
                    json={
                        "input": text,
                        "model": self.model_name
                    },
                    timeout=30
                )
                response.raise_for_status()
                
                result = response.json()
                if 'data' in result and len(result['data']) > 0:
                    return result['data'][0]['embedding']
                else:
                    print(f"APIå“åº”æ ¼å¼é”™è¯¯: {result}")
                    continue
                    
            except requests.exceptions.RequestException as e:
                print(f"APIè°ƒç”¨å¤±è´¥ (ç«¯å£ {api_url.split(':')[-1]}): {e}")
                continue
        
        print(f"æ‰€æœ‰APIç«¯ç‚¹éƒ½è°ƒç”¨å¤±è´¥ï¼Œæ–‡æœ¬: {text[:50]}...")
        return []
    
    def encode_texts(self, texts: List[str], batch_size: int = 32) -> np.ndarray:
        """æ‰¹é‡ç¼–ç æ–‡æœ¬ä¸ºembeddingå‘é‡"""
        print(f"ğŸ”¤ ç¼–ç  {len(texts)} ä¸ªé—®é¢˜æ–‡æœ¬...")
        embeddings = []
        
        for i, text in enumerate(tqdm(texts, desc="ç¼–ç æ–‡æœ¬", ncols=100)):
            if i % 10 == 0:  # æ¯10ä¸ªæ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
                print(f"  å¤„ç†ç¬¬ {i+1}/{len(texts)} ä¸ªæ–‡æœ¬...")
            
            # è½®è¯¢ä½¿ç”¨ä¸åŒçš„APIç«¯ç‚¹
            url_index = i % len(self.working_urls)
            embedding = self.get_embedding(text, url_index)
            embeddings.append(embedding)
        
        # è¿‡æ»¤æ‰ç©ºçš„embedding
        valid_embeddings = [emb for emb in embeddings if emb]
        if len(valid_embeddings) != len(embeddings):
            print(f"âš ï¸  è­¦å‘Šï¼š{len(embeddings) - len(valid_embeddings)} ä¸ªæ–‡æœ¬çš„embeddingè·å–å¤±è´¥")
        
        return np.array(valid_embeddings)
    
    def compute_similarity_matrix(self, embeddings: np.ndarray) -> np.ndarray:
        """è®¡ç®—embeddingä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦çŸ©é˜µ"""
        # å½’ä¸€åŒ–embeddingå‘é‡
        normalized_embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        similarity_matrix = cosine_similarity(normalized_embeddings)
        return similarity_matrix

# å…¨å±€embeddingæ¨¡å‹å®ä¾‹
_embedding_model = None

def get_embedding_model() -> EmbeddingModel:
    """è·å–å…¨å±€embeddingæ¨¡å‹å®ä¾‹"""
    global _embedding_model
    if _embedding_model is None:
        _embedding_model = EmbeddingModel(EMBEDDING_API_URLS, EMBEDDING_MODEL_NAME)
    return _embedding_model

# ========= Ark è°ƒç”¨ =========
def call_ark(prompt: str, api_key: str, model_id: str,
             temperature: float = 0.0, top_p: float = 0.9,
             timeout: int = DEFAULT_ARK_TIMEOUT) -> str:
    time.sleep(random.uniform(0.05, 0.20))  # è½»å¾®æŠ–åŠ¨
    client = Ark(api_key=api_key, timeout=timeout)
    resp = client.chat.completions.create(
        model=model_id,
        messages=[
            {"role":"system","content":SYSTEM_PROMPT},
            {"role":"user","content":prompt},
        ],
        temperature=temperature,
        top_p=top_p,
    )
    try:
        return resp.choices[0].message.content
    except Exception:
        return str(resp)

# ========= JSON è§£æ + æ¸…æ´— =========
_HEX = set("0123456789abcdefABCDEF")

def _fix_invalid_backslashes(s: str) -> str:
    """
    ä¿®å¤æ¨¡å‹è¿”å›ä¸­çš„éæ³•åæ–œæ ï¼šå¯¹éåˆæ³•è½¬ä¹‰å‰çš„åæ–œæ è¿›è¡ŒäºŒæ¬¡è½¬ä¹‰ã€‚
    ä»…ç”¨äºè®© JSON å¯è¢«è§£æï¼Œä¸æ”¹åŠ¨å…¶ä»–å†…å®¹ã€‚
    """
    out = []
    i = 0
    n = len(s)
    while i < n:
        ch = s[i]
        if ch != '\\':
            out.append(ch)
            i += 1
            continue
        # ch is backslash
        if i + 1 >= n:
            out.append('\\\\'); i += 1; continue
        nxt = s[i+1]
        if nxt in '"\\/bfnrt':
            out.append('\\' + nxt); i += 2; continue
        if nxt == 'u':
            # \uXXXX
            if i + 5 < n and all(c in _HEX for c in s[i+2:i+6]):
                out.append(s[i:i+6]); i += 6; continue
            else:
                out.append('\\\\u'); i += 2; continue
        # éæ³•è½¬ä¹‰ -> åŒåæ–œæ 
        out.append('\\\\' + nxt)
        i += 2
    return ''.join(out)

def _strip_code_fences(s: str) -> str:
    s = s.strip()
    fences = ("```json", "```JSON", "```", "~~~json", "~~~JSON", "~~~")
    for f in fences:
        if s.startswith(f):
            s = s[len(f):].strip()
        if s.endswith(f):
            s = s[:-len(f)].strip()
    return s

def force_json_load_with_sanitize(s: str) -> Tuple[Any, bool]:
    """
    è¿”å› (obj, sanitized_used)
    å…ˆæŒ‰å¸¸è§„è·¯å¾„è§£æï¼›å¤±è´¥åï¼š
      - æˆªå–é¦–ä¸ª [ åˆ°æœ€åä¸€ä¸ª ] çš„æ ¸å¿ƒ
      - å»ä»£ç å›´æ 
      - ä¿®å¤éæ³•åæ–œæ 
    """
    s0 = s.strip()
    try:
        if s0.startswith('[') and s0.endswith(']'):
            return json.loads(s0), False
    except Exception:
        pass

    first = s0.find('['); last = s0.rfind(']')
    core = s0 if (first == -1 or last == -1 or first >= last) else s0[first:last+1]
    core = _strip_code_fences(core)

    # ç¬¬ä¸€æ¬¡å°è¯•
    try:
        return json.loads(core), False
    except Exception:
        pass

    # æ¸…æ´—éæ³• \
    fixed = _fix_invalid_backslashes(core)
    return json.loads(fixed), True  # å¦‚æœä»å¼‚å¸¸ï¼Œä¼šæŠ›ç»™ä¸Šå±‚

# ========= å·¥å…· =========
def norm_text(s: str) -> str:
    return " ".join(s.replace("\u3000", " ").replace("\r", "").split())

def clean_text_lines(text: str, max_consecutive_empty: int = 1) -> str:
    """
    æ¸…ç†æ–‡æœ¬ä¸­çš„å¤šä½™ç©ºè¡Œ
    Args:
        text: è¾“å…¥æ–‡æœ¬
        max_consecutive_empty: æœ€å¤§è¿ç»­ç©ºè¡Œæ•°ï¼Œé»˜è®¤ä¸º1
    """
    lines = text.splitlines()
    cleaned_lines = []
    empty_count = 0
    
    for line in lines:
        is_empty = not line.strip()
        if is_empty:
            empty_count += 1
            if empty_count <= max_consecutive_empty:
                cleaned_lines.append("")
        else:
            empty_count = 0
            cleaned_lines.append(line)
    
    return "\n".join(cleaned_lines)

def read_word_document(docx_path: Path) -> str:
    """è¯»å–Wordæ–‡æ¡£å¹¶è¿”å›æ–‡æœ¬å†…å®¹"""
    try:
        doc = Document(docx_path)
        text_lines = []
        for paragraph in doc.paragraphs:
            if paragraph.text.strip():  # åªæ·»åŠ éç©ºæ®µè½
                text_lines.append(paragraph.text.strip())
        return "\n".join(text_lines)
    except Exception as e:
        raise Exception(f"è¯»å–Wordæ–‡æ¡£å¤±è´¥: {e}")

def read_markdown_document(md_path: Path) -> str:
    """è¯»å–Markdownæ–‡æ¡£å¹¶è¿”å›æ–‡æœ¬å†…å®¹ï¼Œåˆ é™¤å¤šä½™ç©ºè¡Œ"""
    try:
        content = md_path.read_text(encoding="utf-8", errors="ignore")
        # ä½¿ç”¨é€šç”¨æ¸…ç†å‡½æ•°åˆ é™¤å¤šä½™ç©ºè¡Œ
        return clean_text_lines(content, max_consecutive_empty=1)
    except Exception as e:
        raise Exception(f"è¯»å–Markdownæ–‡æ¡£å¤±è´¥: {e}")

def read_document(file_path: Path) -> str:
    """æ ¹æ®æ–‡ä»¶ç±»å‹è¯»å–æ–‡æ¡£å†…å®¹"""
    suffix = file_path.suffix.lower()
    if suffix in ['.docx', '.doc']:
        return read_word_document(file_path)
    elif suffix in ['.md', '.markdown', '.txt']:
        return read_markdown_document(file_path)
    else:
        raise Exception(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {suffix}")

def make_windows(lines: List[str], window: int, stride: int) -> List[Tuple[int, int, str]]:
    n = len(lines); out = []; i = 0
    while i < n:
        s = i; e = min(i + window - 1, n - 1)
        out.append((s+1, e+1, "\n".join(lines[s:e+1])))
        if e == n - 1: break
        i += stride
    return out

def make_char_windows(text: str, window_chars: int, stride_chars: int) -> List[Tuple[int, int, str]]:
    """
    æŒ‰å­—ç¬¦æ•°åˆ†å‰²æ–‡æœ¬ï¼Œåˆ›å»ºæ»‘åŠ¨çª—å£
    Args:
        text: è¾“å…¥æ–‡æœ¬
        window_chars: çª—å£å­—ç¬¦æ•°
        stride_chars: æ»‘åŠ¨æ­¥é•¿å­—ç¬¦æ•°
    Returns:
        List of (start_pos, end_pos, window_text)
    """
    if not text:
        return []
    
    windows = []
    text_len = len(text)
    start = 0
    
    while start < text_len:
        # è®¡ç®—çª—å£ç»“æŸä½ç½®
        end = min(start + window_chars, text_len)
        
        # è·å–çª—å£æ–‡æœ¬
        window_text = text[start:end]
        
        # è®¡ç®—è¡Œå·èŒƒå›´ï¼ˆè¿‘ä¼¼ï¼‰
        lines_before = text[:start].count('\n')
        lines_in_window = window_text.count('\n')
        start_line = lines_before + 1
        end_line = lines_before + lines_in_window + 1
        
        windows.append((start_line, end_line, window_text))
        
        # å¦‚æœå·²ç»åˆ°è¾¾æ–‡æœ¬æœ«å°¾ï¼Œé€€å‡º
        if end >= text_len:
            break
            
        # è®¡ç®—ä¸‹ä¸€ä¸ªçª—å£çš„èµ·å§‹ä½ç½®
        start += stride_chars
    
    return windows

def compress_chunk_text(s: str, max_chars: int = MAX_CHARS_PER_PROMPT) -> str:
    lines = s.replace("\r", "").splitlines()
    out = []
    for ln in lines:
        ln = " ".join(ln.split())
        if ln:  # åªä¿ç•™éç©ºè¡Œ
            out.append(ln)
    s2 = "\n".join(out).strip()
    return s2[:max_chars] if len(s2) > max_chars else s2

# ========= Debug è½ç›˜ =========
def write_text(path: Path, content: str):
    path.parent.mkdir(parents=True, exist_ok=True)
    with path.open("w", encoding="utf-8") as f:
        f.write(content)

# ========= å•çª—å£è‡ªé€‚åº”è°ƒç”¨ï¼ˆå¸¦æ—¥å¿—ï¼‰ =========
def send_chunk_adaptive(s_line: int, e_line: int, chunk_text: str,
                        max_retries: int, retry_backoff: float, timeout: int,
                        debug_dir: Optional[Path], base_norm: str):
    """
    è¿”å›ï¼šraw_out, data(list), meta(dict)
    meta: {prompt_len, response_len, parse_error, sanitized_used, compressed_chunk}
    """
    compressed = compress_chunk_text(chunk_text, MAX_CHARS_PER_PROMPT)
    prompt = CHUNK_PROMPT_TEMPLATE.format(
        start_line=s_line, end_line=e_line, chunk_text=compressed
    )
    meta = {
        "prompt_len": len(prompt), "response_len": 0,
        "parse_error": "", "sanitized_used": False,
        "compressed_chunk": compressed
    }

    # è°ƒè¯•ç›®å½•
    wdir = None
    if debug_dir:
        wdir = debug_dir / base_norm / f"{s_line}-{e_line}"
        write_text(wdir / "prompt.txt", prompt)  # é€ç»™æ¨¡å‹çš„å®Œæ•´ç”¨æˆ·æ¶ˆæ¯ï¼ˆå«ç‰‡æ®µï¼‰

    # ç›´æ¥é‡è¯•
    attempt = 0
    last_err = None
    while attempt <= max_retries:
        try:
            raw = call_ark(prompt, api_key=API_KEY, model_id=MODEL_ID, timeout=timeout)
            if debug_dir:
                write_text(wdir / "response.txt", raw)
            meta["response_len"] = len(raw)
            obj, sanitized_used = force_json_load_with_sanitize(raw)
            meta["sanitized_used"] = sanitized_used
            if debug_dir and sanitized_used:
                # ä¿å­˜æ¸…æ´—åçš„ JSON æ–‡æœ¬
                write_text(wdir / "sanitized.json", json.dumps(obj, ensure_ascii=False, indent=2))
            return raw, obj, meta
        except Exception as e:
            last_err = str(e)
            meta["parse_error"] = last_err
            if debug_dir:
                write_text(wdir / "parse_error.txt", last_err)
            if attempt == max_retries:
                break
            time.sleep((retry_backoff ** attempt) + random.uniform(0.1, 0.5))
            attempt += 1

    # å®åœ¨å¤±è´¥
    return f"<<FAILED {s_line}-{e_line}>> {last_err}", [], meta

# ========= ä¿å­˜åŸæ–‡ç‰‡æ®µå¯¹ç…§ =========
def save_raw_chunks(raw_records: List[Dict[str, Any]], path: Path):
    path.parent.mkdir(parents=True, exist_ok=True)
    with path.open("w", encoding="utf-8") as f:
        for rec in raw_records:
            f.write(json.dumps(rec, ensure_ascii=False) + "\n")

# ========= å»é‡é€»è¾‘ =========
@dataclass
class DedupLog:
    question: str
    total_count: int
    duplicate_groups: List[Dict[str, Any]]
    kept_item: QAItem
    removed_items: List[QAItem]

def deduplicate_qa_items(items: List[QAItem], log_file: Optional[Path] = None) -> Tuple[List[QAItem], List[DedupLog]]:
    """
    å¯¹é—®ç­”å¯¹è¿›è¡Œå»é‡å¤„ç†
    Args:
        items: é—®ç­”å¯¹åˆ—è¡¨
        log_file: å»é‡æ—¥å¿—æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
    Returns:
        (å»é‡åçš„é—®ç­”å¯¹åˆ—è¡¨, å»é‡æ—¥å¿—åˆ—è¡¨)
    """
    print("ğŸ”„ å¼€å§‹å»é‡å¤„ç†...")
    
    # æŒ‰questionåˆ†ç»„
    question_groups = {}
    for item in items:
        question_key = norm_text(item.question)
        if question_key not in question_groups:
            question_groups[question_key] = []
        question_groups[question_key].append(item)
    
    print(f"ğŸ“Š åŸå§‹é¢˜ç›®æ•°ï¼š{len(items)}ï¼Œå»é‡å‰åˆ†ç»„æ•°ï¼š{len(question_groups)}")
    
    deduped_items = []
    dedup_logs = []
    total_removed = 0
    
    for question_key, group_items in question_groups.items():
        if len(group_items) == 1:
            # åªæœ‰ä¸€ä¸ªï¼Œç›´æ¥ä¿ç•™
            deduped_items.append(group_items[0])
            continue
        
        # å¤šä¸ªç›¸åŒé—®é¢˜çš„ï¼Œéœ€è¦æ¯”è¾ƒç­”æ¡ˆ
        # æŒ‰answerå‰3ä¸ªå­—ç¬¦åˆ†ç»„
        answer_groups = {}
        for item in group_items:
            answer_prefix = item.answer[:3] if len(item.answer) >= 3 else item.answer
            if answer_prefix not in answer_groups:
                answer_groups[answer_prefix] = []
            answer_groups[answer_prefix].append(item)
        
        # å¤„ç†æ¯ä¸ªç­”æ¡ˆå‰ç¼€ç»„
        duplicate_groups = []
        kept_item = None
        removed_items = []
        
        for answer_prefix, answer_items in answer_groups.items():
            if len(answer_items) == 1:
                # è¯¥ç­”æ¡ˆå‰ç¼€åªæœ‰ä¸€ä¸ªï¼Œç›´æ¥ä¿ç•™
                if kept_item is None or len(answer_items[0].answer) > len(kept_item.answer):
                    if kept_item is not None:
                        removed_items.append(kept_item)
                    kept_item = answer_items[0]
                else:
                    removed_items.append(answer_items[0])
            else:
                # è¯¥ç­”æ¡ˆå‰ç¼€æœ‰å¤šä¸ªï¼Œä¿ç•™æœ€é•¿çš„
                # æŒ‰ç­”æ¡ˆé•¿åº¦æ’åºï¼Œä¿ç•™æœ€é•¿çš„
                answer_items.sort(key=lambda x: len(x.answer), reverse=True)
                kept_in_group = answer_items[0]
                removed_in_group = answer_items[1:]
                
                if kept_item is None or len(kept_in_group.answer) > len(kept_item.answer):
                    if kept_item is not None:
                        removed_items.append(kept_item)
                    kept_item = kept_in_group
                else:
                    removed_items.append(kept_in_group)
                
                removed_items.extend(removed_in_group)
                
                duplicate_groups.append({
                    "answer_prefix": answer_prefix,
                    "count": len(answer_items),
                    "kept_item": {
                        "qid": kept_in_group.qid,
                        "answer_length": len(kept_in_group.answer),
                        "answer_preview": kept_in_group.answer[:100]
                    },
                    "removed_items": [
                        {
                            "qid": item.qid,
                            "answer_length": len(item.answer),
                            "answer_preview": item.answer[:100]
                        } for item in removed_in_group
                    ]
                })
        
        # è®°å½•å»é‡æ—¥å¿—
        if duplicate_groups or len(removed_items) > 0:
            dedup_log = DedupLog(
                question=question_key,
                total_count=len(group_items),
                duplicate_groups=duplicate_groups,
                kept_item=kept_item,
                removed_items=removed_items
            )
            dedup_logs.append(dedup_log)
            total_removed += len(removed_items)
        
        deduped_items.append(kept_item)
    
    print(f"ğŸ¯ å»é‡å®Œæˆï¼šåŸå§‹ {len(items)} é¢˜ â†’ å»é‡å {len(deduped_items)} é¢˜ï¼Œåˆ é™¤ {total_removed} ä¸ªé‡å¤é¡¹")
    
    # ä¿å­˜å»é‡æ—¥å¿—
    if log_file and dedup_logs:
        log_file.parent.mkdir(parents=True, exist_ok=True)
        with log_file.open("w", encoding="utf-8") as f:
            f.write("# å»é‡å¤„ç†æ—¥å¿—\n\n")
            f.write(f"## æ€»ä½“ç»Ÿè®¡\n")
            f.write(f"- åŸå§‹é¢˜ç›®æ•°ï¼š{len(items)}\n")
            f.write(f"- å»é‡åé¢˜ç›®æ•°ï¼š{len(deduped_items)}\n")
            f.write(f"- åˆ é™¤é‡å¤é¡¹æ•°ï¼š{total_removed}\n")
            f.write(f"- æ¶‰åŠé‡å¤çš„é—®é¢˜æ•°ï¼š{len(dedup_logs)}\n\n")
            
            for i, log in enumerate(dedup_logs, 1):
                f.write(f"## é‡å¤é—®é¢˜ {i}\n")
                f.write(f"**é—®é¢˜ï¼š** {log.question[:100]}...\n")
                f.write(f"**ä¿ç•™ï¼š** {log.kept_item.qid} (ç­”æ¡ˆé•¿åº¦: {len(log.kept_item.answer)})\n")
                f.write(f"**åˆ é™¤ï¼š** {len(log.removed_items)} ä¸ªé‡å¤é¡¹\n\n")
        
        print(f"ğŸ“ å»é‡æ—¥å¿—å·²ä¿å­˜ï¼š{log_file}")
    
    return deduped_items, dedup_logs

# ========= ç›¸ä¼¼åº¦èšç±»å»é‡ =========
def cluster_and_deduplicate_questions(items: List[QAItem], similarity_threshold: float = SIMILARITY_THRESHOLD, 
                                    log_file: Optional[Path] = None) -> Tuple[List[QAItem], List[ClusterLog]]:
    """
    åŸºäºembeddingç›¸ä¼¼åº¦å¯¹é—®é¢˜è¿›è¡Œèšç±»ï¼Œç„¶ååœ¨èšç±»å†…è¿›è¡Œanswerå»é‡
    Args:
        items: é—®ç­”å¯¹åˆ—è¡¨
        similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
        log_file: èšç±»æ—¥å¿—æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
    Returns:
        (èšç±»å’Œå»é‡åçš„é—®ç­”å¯¹åˆ—è¡¨, èšç±»æ—¥å¿—åˆ—è¡¨)
    """
    if not items:
        return items, []
    
    print(f"ğŸ”„ å¼€å§‹ç›¸ä¼¼åº¦èšç±»å’Œå»é‡ï¼Œç›¸ä¼¼åº¦é˜ˆå€¼ï¼š{similarity_threshold}")
    print(f"ğŸ“Š è¾“å…¥é¢˜ç›®æ•°ï¼š{len(items)}")
    
    # è·å–embeddingæ¨¡å‹
    embedding_model = get_embedding_model()
    
    # æå–æ‰€æœ‰é—®é¢˜æ–‡æœ¬
    questions = [item.question for item in items]
    
    # ç¼–ç æ‰€æœ‰é—®é¢˜
    embeddings = embedding_model.encode_texts(questions)
    
    # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
    print("ğŸ“ è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ...")
    similarity_matrix = embedding_model.compute_similarity_matrix(embeddings)
    
    # èšç±»ç®—æ³•ï¼šä½¿ç”¨ç›¸ä¼¼åº¦é˜ˆå€¼è¿›è¡Œèšç±»
    print("ğŸ¯ æ‰§è¡Œèšç±»ç®—æ³•...")
    clusters = []
    visited = set()
    
    for i in range(len(items)):
        if i in visited:
            continue
        
        # åˆ›å»ºæ–°èšç±»
        cluster = [i]
        visited.add(i)
        
        # æ‰¾åˆ°æ‰€æœ‰ä¸å½“å‰é—®é¢˜ç›¸ä¼¼çš„é—®é¢˜
        for j in range(i + 1, len(items)):
            if j in visited:
                continue
            
            similarity = similarity_matrix[i][j]
            if similarity >= similarity_threshold:
                cluster.append(j)
                visited.add(j)
        
        clusters.append(cluster)
    
    print(f"ğŸ“Š èšç±»ç»“æœï¼š{len(clusters)} ä¸ªèšç±»")
    
    # å¤„ç†æ¯ä¸ªèšç±»ï¼šé€‰æ‹©ä»£è¡¨é—®é¢˜ï¼Œç„¶åå¯¹ç­”æ¡ˆè¿›è¡Œå»é‡
    final_items = []
    cluster_logs = []
    total_merged = 0
    
    for cluster_id, cluster_indices in enumerate(clusters):
        if len(cluster_indices) == 1:
            # å•å…ƒç´ èšç±»ï¼Œç›´æ¥ä¿ç•™
            final_items.append(items[cluster_indices[0]])
            continue
        
        # å¤šå…ƒç´ èšç±»ï¼Œéœ€è¦é€‰æ‹©ä»£è¡¨é—®é¢˜å¹¶å¯¹ç­”æ¡ˆå»é‡
        cluster_items = [items[i] for i in cluster_indices]
        
        # 1. é€‰æ‹©ä»£è¡¨é—®é¢˜ï¼ˆé€‰æ‹©é—®é¢˜æ–‡æœ¬æœ€é•¿çš„ï¼Œæˆ–è€…ç¬¬ä¸€ä¸ªï¼‰
        representative_item = max(cluster_items, key=lambda x: len(x.question))
        
        # 2. æ”¶é›†èšç±»å†…æ‰€æœ‰ç­”æ¡ˆ
        all_answers = [item.answer for item in cluster_items]
        
        # 3. å¯¹ç­”æ¡ˆè¿›è¡Œå»é‡å¤„ç†ï¼ˆæŒ‰å‰3ä¸ªå­—ç¬¦åˆ†ç»„ï¼‰
        answer_groups = {}
        for i, answer in enumerate(all_answers):
            answer_prefix = answer[:3] if len(answer) >= 3 else answer
            if answer_prefix not in answer_groups:
                answer_groups[answer_prefix] = []
            answer_groups[answer_prefix].append((i, answer))
        
        # 4. åœ¨æ¯ä¸ªç­”æ¡ˆå‰ç¼€ç»„å†…é€‰æ‹©æœ€é•¿çš„ç­”æ¡ˆ
        final_answer = None
        merged_items = []
        similarities = []
        
        for answer_prefix, answer_list in answer_groups.items():
            if len(answer_list) == 1:
                # è¯¥ç­”æ¡ˆå‰ç¼€åªæœ‰ä¸€ä¸ªï¼Œç›´æ¥ä½¿ç”¨
                if final_answer is None or len(answer_list[0][1]) > len(final_answer):
                    if final_answer is not None:
                        # æ‰¾åˆ°è¢«æ›¿æ¢çš„ç­”æ¡ˆå¯¹åº”çš„item
                        old_item_idx = next(i for i, item in enumerate(cluster_items) 
                                          if item.answer == final_answer)
                        merged_items.append(cluster_items[old_item_idx])
                    final_answer = answer_list[0][1]
                else:
                    # å½“å‰ç­”æ¡ˆè¾ƒçŸ­ï¼Œæ ‡è®°ä¸ºåˆå¹¶
                    item_idx = answer_list[0][0]
                    merged_items.append(cluster_items[item_idx])
            else:
                # è¯¥ç­”æ¡ˆå‰ç¼€æœ‰å¤šä¸ªï¼Œé€‰æ‹©æœ€é•¿çš„
                # æŒ‰ç­”æ¡ˆé•¿åº¦æ’åº
                answer_list.sort(key=lambda x: len(x[1]), reverse=True)
                best_answer = answer_list[0][1]
                other_answers = answer_list[1:]
                
                if final_answer is None or len(best_answer) > len(final_answer):
                    if final_answer is not None:
                        # æ‰¾åˆ°è¢«æ›¿æ¢çš„ç­”æ¡ˆå¯¹åº”çš„item
                        old_item_idx = next(i for i, item in enumerate(cluster_items) 
                                          if item.answer == final_answer)
                        merged_items.append(cluster_items[old_item_idx])
                    final_answer = best_answer
                else:
                    # å½“å‰æœ€ä½³ç­”æ¡ˆè¾ƒçŸ­ï¼Œæ ‡è®°ä¸ºåˆå¹¶
                    item_idx = answer_list[0][0]
                    merged_items.append(cluster_items[item_idx])
                
                # æ ‡è®°å…¶ä»–ç­”æ¡ˆä¸ºåˆå¹¶
                for item_idx, _ in other_answers:
                    merged_items.append(cluster_items[item_idx])
        
        # 5. åˆ›å»ºæœ€ç»ˆçš„QAItem
        final_item = QAItem(
            qid=representative_item.qid,
            question=representative_item.question,
            answer=final_answer,
            reference=representative_item.reference,
            source_window=representative_item.source_window,
            window_local_id=representative_item.window_local_id,
            source_file=representative_item.source_file
        )
        
        # 6. è®¡ç®—ç›¸ä¼¼åº¦ï¼ˆè¢«åˆå¹¶çš„itemä¸ä»£è¡¨é—®é¢˜ä¹‹é—´çš„ç›¸ä¼¼åº¦ï¼‰
        representative_idx = next(i for i, item in enumerate(items) if item == representative_item)
        for merged_item in merged_items:
            merged_idx = next(i for i, item in enumerate(items) if item == merged_item)
            similarity = similarity_matrix[representative_idx][merged_idx]
            similarities.append(similarity)
        
        # 7. è®°å½•èšç±»æ—¥å¿— - åªä¿ç•™èšç±»å¤§å°>1æˆ–ç­”æ¡ˆæ•°é‡>1çš„èšç±»
        if len(cluster_items) > 1 or len(answer_groups) > 1:
            cluster_log = ClusterLog(
                cluster_id=cluster_id + 1,
                representative_question=representative_item.question,
                cluster_size=len(cluster_items),
                similarity_threshold=similarity_threshold,
                kept_item=final_item,
                merged_items=merged_items,
                similarities=similarities,
                all_cluster_items=cluster_items,  # èšç±»å‰çš„æ‰€æœ‰é—®é¢˜
                answer_groups=answer_groups  # ç­”æ¡ˆåˆ†ç»„ä¿¡æ¯
            )
            cluster_logs.append(cluster_log)
            total_merged += len(merged_items)
        
        final_items.append(final_item)
    
    print(f"ğŸ¯ èšç±»å’Œå»é‡å®Œæˆï¼šåŸå§‹ {len(items)} é¢˜ â†’ æœ€ç»ˆ {len(final_items)} é¢˜ï¼Œåˆå¹¶ {total_merged} ä¸ªç›¸ä¼¼é—®é¢˜")
    
    # ä¿å­˜èšç±»æ—¥å¿—
    if log_file and cluster_logs:
        log_file.parent.mkdir(parents=True, exist_ok=True)
        with log_file.open("w", encoding="utf-8") as f:
            f.write("# ç›¸ä¼¼åº¦èšç±»å’Œå»é‡æ—¥å¿—\n\n")
            f.write(f"## æ€»ä½“ç»Ÿè®¡\n")
            f.write(f"- åŸå§‹é¢˜ç›®æ•°ï¼š{len(items)}\n")
            f.write(f"- æœ€ç»ˆé¢˜ç›®æ•°ï¼š{len(final_items)}\n")
            f.write(f"- åˆå¹¶ç›¸ä¼¼é—®é¢˜æ•°ï¼š{total_merged}\n")
            f.write(f"- èšç±»æ•°é‡ï¼š{len(clusters)}\n")
            f.write(f"- ç›¸ä¼¼åº¦é˜ˆå€¼ï¼š{similarity_threshold}\n\n")
            
            for log in cluster_logs:
                f.write(f"## èšç±» {log.cluster_id}\n")
                f.write(f"**èšç±»å¤§å°ï¼š** {log.cluster_size}\n")
                f.write(f"**ä»£è¡¨é—®é¢˜ï¼š** {log.representative_question}...\n")
                f.write(f"**ä¿ç•™é¡¹ï¼š** {log.kept_item.qid} (ç­”æ¡ˆé•¿åº¦: {len(log.kept_item.answer)})\n")
                f.write(f"**åˆå¹¶é¡¹ï¼š** {len(log.merged_items)} ä¸ª\n")
                
                # æ˜¾ç¤ºèšç±»å‰çš„æ‰€æœ‰é—®é¢˜å’Œç­”æ¡ˆå¯¹æ¯”ï¼ˆç®€åŒ–ç‰ˆï¼‰
                f.write(f"\n**èšç±»å‰çš„é—®é¢˜å’Œç­”æ¡ˆï¼š**\n")
                for i, item in enumerate(log.all_cluster_items, 1):
                    f.write(f"{i}. {item.qid}: {item.question}...\n")
                    f.write(f"   ç­”æ¡ˆ: {item.answer}... (é•¿åº¦: {len(item.answer)})\n")
                
                f.write(f"\n**ç›¸ä¼¼åº¦ï¼š** {', '.join([f'{s:.3f}' for s in log.similarities])}\n")
                f.write(f"**ç­”æ¡ˆåˆ†ç»„ï¼š** {len(log.answer_groups)} ä¸ªä¸åŒç­”æ¡ˆå‰ç¼€\n\n")
                f.write("---\n\n")
        
        print(f"ğŸ“ èšç±»æ—¥å¿—å·²ä¿å­˜ï¼š{log_file}")
    
    return final_items, cluster_logs

# ========= æŠ½å–å•æ–‡ä»¶ =========
def extract_from_md(md_path: Path, window: int, stride: int,
                    max_retries: int, retry_backoff: float, timeout: int,
                    source_file_stem: str,
                    debug_dir: Optional[Path]) -> Tuple[List[QAItem], ExtractStats, List[Dict[str, Any]]]:
    text = md_path.read_text(encoding="utf-8", errors="ignore")
    lines = text.splitlines()
    windows = make_windows(lines, window=window, stride=stride)

    all_items: List[QAItem] = []
    stats = ExtractStats(windows=len(windows))
    raw_records: List[Dict[str, Any]] = []

    pbar = tqdm(total=len(windows), desc=f"æŠ½å– {md_path.name}", ncols=100)
    for (s_line, e_line, chunk) in windows:
        raw_out, data, meta = send_chunk_adaptive(
            s_line=s_line, e_line=e_line, chunk_text=chunk,
            max_retries=max_retries, retry_backoff=retry_backoff, timeout=timeout,
            debug_dir=debug_dir, base_norm=source_file_stem
        )

        raw_records.append({
            "window": f"{s_line}-{e_line}",
            "chunk_text": chunk,
            "compressed_chunk": meta["compressed_chunk"],
            "prompt_length": meta["prompt_len"],
            "model_output": raw_out,
            "response_length": meta["response_len"],
            "sanitized_used": meta["sanitized_used"],
            "parse_error": meta["parse_error"],
            "parsed_questions": data
        })

        stats.raw_questions += len(data)
        for obj in data:
            question = norm_text(str(obj.get("question") or ""))
            answer = norm_text(str(obj.get("answer") or ""))
            reference = norm_text(str(obj.get("reference") or ""))

            # ======= å…³é”®æ”¹åŠ¨ï¼šç­”æ¡ˆä¸ºç©ºç›´æ¥å‰”é™¤ =======
            if not answer:
                continue
            # =======================================

            window_position = (s_line, e_line)
            window_range = f"{s_line}-{e_line}"

            all_items.append(QAItem(
                qid=None, question=question, answer=answer, reference=reference,
                source_window=(s_line, e_line), window_local_id=str(obj.get("id") or ""),
                source_file=source_file_stem
            ))
            stats.kept_questions += 1

        pbar.update(1)
    pbar.close()

    for i, q in enumerate(all_items, start=1):
        q.qid = f"{source_file_stem}__Q{i:05d}"
    return all_items, stats, raw_records

# ========= IO è¾…åŠ© =========
def save_items_per_file(items: List[QAItem], out_dir: Path, base: str):
    out_dir_file = out_dir / base
    out_dir_file.mkdir(parents=True, exist_ok=True)
    jsonl_path = out_dir_file / "questions.jsonl"
    with jsonl_path.open("w", encoding="utf-8") as f:
        for it in items:
            f.write(json.dumps(asdict(it), ensure_ascii=False) + "\n")
    return jsonl_path

# ========= å•çª—å£å¤„ç†ï¼ˆç»™å¹¶å‘è°ƒç”¨ï¼‰ =========
def process_window(md_path: Path, s_line: int, e_line: int, chunk_text: str,
                   window: int, stride: int, max_retries: int, timeout: int,
                   out_root: Path, raw_root: Optional[Path], debug_dir: Optional[Path]) -> Tuple[str, int, int, List[QAItem], int, List[Dict[str, Any]]]:
    """
    å¤„ç†å•ä¸ªçª—å£ï¼Œè¿”å› (source_file_stem, s_line, e_line, items, raw_questions_count, raw_records)
    """
    base_norm = md_path.stem
    compressed = compress_chunk_text(chunk_text, MAX_CHARS_PER_PROMPT)
    prompt = CHUNK_PROMPT_TEMPLATE.format(
        start_line=s_line, end_line=e_line, chunk_text=compressed
    )
    
    # è°ƒè¯•ç›®å½•
    wdir = None
    if debug_dir:
        wdir = debug_dir / base_norm / f"{s_line}-{e_line}"
        write_text(wdir / "prompt.txt", prompt)

    # è°ƒç”¨Arkå¤„ç†çª—å£
    raw_out, data, meta = send_chunk_adaptive(
        s_line=s_line, e_line=e_line, chunk_text=chunk_text,
        max_retries=max_retries, retry_backoff=1.8, timeout=timeout,
        debug_dir=debug_dir, base_norm=base_norm
    )

    # å¤„ç†è¿”å›çš„æ•°æ®
    items = []
    
    for obj in data:
        question = norm_text(str(obj.get("question") or ""))
        answer = norm_text(str(obj.get("answer") or ""))
        reference = norm_text(str(obj.get("reference") or ""))

        # ç­”æ¡ˆä¸ºç©ºç›´æ¥å‰”é™¤
        if not answer:
            continue

        window_position = (s_line, e_line)
        window_range = f"{s_line}-{e_line}"

        items.append(QAItem(
            qid=None, question=question, answer=answer, reference=reference,
            source_window=(s_line, e_line), window_local_id=str(obj.get("id") or ""),
            source_file=base_norm
        ))

    return base_norm, s_line, e_line, items, len(data), [{
        "window": f"{s_line}-{e_line}",
        "chunk_text": chunk_text,
        "compressed_chunk": meta["compressed_chunk"],
        "prompt_length": meta["prompt_len"],
        "model_output": raw_out,
        "response_length": meta["response_len"],
        "sanitized_used": meta["sanitized_used"],
        "parse_error": meta["parse_error"],
        "parsed_questions": data
    }]


# ========= ä¸»ç¨‹åº =========
def main():
    ap = argparse.ArgumentParser(description="é—®ç­”å¯¹æŠ½å–ï¼šå¤„ç†Wordæ–‡æ¡£æˆ–Markdownæ–‡æ¡£ï¼ŒæŠ½å–é—®ç­”å¯¹")
    ap.add_argument("input_file", type=str, help="è¾“å…¥çš„æ–‡æ¡£è·¯å¾„ï¼ˆæ”¯æŒ.docx, .doc, .md, .markdown, .txtï¼‰")
    ap.add_argument("--out-dir", type=str, default="out_qas", help="è¾“å‡ºæ ¹ç›®å½•")
    ap.add_argument("--save-raw-dir", type=str, default="", help="ä¿å­˜åŸæ–‡ç‰‡æ®µå¯¹ç…§çš„ç›®å½•ï¼ˆæ¯æ–‡ä»¶ raw_chunks.jsonlï¼‰")
    ap.add_argument("--debug-log-dir", type=str, default="", help="ä¿å­˜æ¯ä¸ªçª—å£çš„ prompt/response/parse_error/sanitized.json")
    ap.add_argument("--window", type=int, default=DEFAULT_WINDOW_LINES, help="æ»‘çª—å¤§å°ï¼ˆè¡Œï¼‰")
    ap.add_argument("--stride", type=int, default=DEFAULT_STRIDE_LINES, help="æ»‘çª—æ­¥é•¿ï¼ˆè¡Œï¼‰")
    ap.add_argument("--window-chars", type=int, default=3000, help="å­—ç¬¦çª—å£å¤§å°ï¼ˆå­—ç¬¦æ•°ï¼‰")
    ap.add_argument("--stride-chars", type=int, default=1800, help="å­—ç¬¦çª—å£æ­¥é•¿ï¼ˆå­—ç¬¦æ•°ï¼‰")
    ap.add_argument("--use-char-window", action="store_true", help="ä½¿ç”¨å­—ç¬¦çª—å£è€Œä¸æ˜¯è¡Œçª—å£")
    ap.add_argument("--max-retries", type=int, default=3, help="Ark è°ƒç”¨å¤±è´¥æœ€å¤§é‡è¯•æ¬¡æ•°")
    ap.add_argument("--timeout", type=int, default=DEFAULT_ARK_TIMEOUT, help="å•æ¬¡ Ark è¯·æ±‚è¶…æ—¶ï¼ˆç§’ï¼‰")
    ap.add_argument("--max-workers", type=int, default=256, help="å¹¶å‘çº¿ç¨‹æ•°")
    args = ap.parse_args()

    input_file = Path(args.input_file)
    out_root = Path(args.out_dir)
    raw_root = Path(args.save_raw_dir) if args.save_raw_dir else None
    debug_dir = Path(args.debug_log_dir) if args.debug_log_dir else None

    if not input_file.exists():
        print(f"âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨ï¼š{input_file}"); return
    
    supported_formats = ['.docx', '.doc', '.md', '.markdown', '.txt']
    if not input_file.suffix.lower() in supported_formats:
        print(f"âŒ è¾“å…¥æ–‡ä»¶æ ¼å¼ä¸æ”¯æŒï¼Œæ”¯æŒæ ¼å¼ï¼š{supported_formats}ï¼Œå½“å‰æ–‡ä»¶ï¼š{input_file}"); return

    print(f"ğŸ“„ å¤„ç†æ–‡ä»¶ï¼š{input_file}")
    
    # è¯»å–æ–‡æ¡£
    print("ğŸ“¥ è¯»å–æ–‡æ¡£...")
    try:
        text_content = read_document(input_file)
        # æ¸…ç†å¤šä½™ç©ºè¡Œ
        cleaned_content = clean_text_lines(text_content, max_consecutive_empty=1)
        
        if args.use_char_window:
            # ä½¿ç”¨å­—ç¬¦çª—å£
            windows = make_char_windows(cleaned_content, args.window_chars, args.stride_chars)
            print(f"âœ… æ–‡æ¡£å­—ç¬¦æ•°ï¼š{len(cleaned_content)}ï¼Œçª—å£æ•°ï¼š{len(windows)}ï¼ˆå­—ç¬¦çª—å£ï¼‰")
        else:
            # ä½¿ç”¨è¡Œçª—å£
            lines = cleaned_content.splitlines()
            windows = make_windows(lines, window=args.window, stride=args.stride)
            print(f"âœ… æ–‡æ¡£è¡Œæ•°ï¼š{len(lines)}ï¼Œçª—å£æ•°ï¼š{len(windows)}ï¼ˆè¡Œçª—å£ï¼‰")
    except Exception as e:
        print(f"âŒ è¯»å–æ–‡æ¡£å¤±è´¥ï¼š{e}")
        return

    out_root.mkdir(parents=True, exist_ok=True)
    combined_jsonl = out_root / "combined_questions.jsonl"
    combined_jsonl_before_dedup = out_root / "combined_questions_before_dedup.jsonl"

    # çª—å£çº§å¹¶å‘å¤„ç†
    print(f"ğŸš€ ä½¿ç”¨çª—å£çº§å¹¶å‘ï¼Œæœ€å¤§çº¿ç¨‹æ•°ï¼š{args.max_workers}")
    
    # å‡†å¤‡çª—å£ä»»åŠ¡
    base_norm = input_file.stem
    
    file_stats = {
        base_norm: {
            'path': input_file,
            'windows': len(windows),
            'items': [],
            'raw_records': [],
            'stats': ExtractStats(windows=len(windows))
        }
    }
    
    all_window_tasks = []
    for s_line, e_line, chunk in windows:
        all_window_tasks.append((input_file, s_line, e_line, chunk))
    
    print(f"ğŸ“Š æ€»çª—å£æ•°ï¼š{len(all_window_tasks)}")
    
    # å…¨å±€çª—å£çº§å¹¶å‘å¤„ç†
    with cf.ThreadPoolExecutor(max_workers=args.max_workers) as executor:
        futures = {
                            executor.submit(
                    process_window,
                    md_path=task[0], s_line=task[1], e_line=task[2], chunk_text=task[3],
                    window=args.window, stride=args.stride, max_retries=args.max_retries, timeout=args.timeout,
                    out_root=out_root, raw_root=raw_root, debug_dir=debug_dir
                ): (task[0].stem, task[1], task[2])  # ç”¨ (æ–‡ä»¶å, èµ·å§‹è¡Œ, ç»“æŸè¡Œ) ä½œä¸ºkey
            for task in all_window_tasks
        }

        # å¤„ç†å®Œæˆçš„çª—å£
        for fut in tqdm(cf.as_completed(futures), total=len(futures), ncols=100, desc="çª—å£çº§å¹¶å‘æŠ½å–"):
            file_stem, s_line, e_line = futures[fut]
            try:
                source_file_stem, s_line, e_line, items, raw_questions_count, raw_records = fut.result()
                
                # æ›´æ–°æ–‡ä»¶ç»Ÿè®¡
                file_stats[source_file_stem]['items'].extend(items)
                file_stats[source_file_stem]['raw_records'].extend(raw_records)
                file_stats[source_file_stem]['stats'].raw_questions += raw_questions_count
                file_stats[source_file_stem]['stats'].kept_questions += len(items)
                
            except Exception as e:
                print(f"\nâŒ çª—å£å¤„ç†å¤±è´¥ {file_stem} {s_line}-{e_line}: {e}")
    
    # æ”¶é›†æ‰€æœ‰é¢˜ç›®ï¼Œå‡†å¤‡å…¨å±€é‡æ–°åˆ†é…qid
    print("\nğŸ“ æ”¶é›†æ‰€æœ‰é¢˜ç›®å¹¶å‡†å¤‡é‡æ–°åˆ†é…qid...")
    all_items_for_qid = []
    combined_rows: List[dict] = []
    
    for base_norm, file_info in file_stats.items():
        if file_info['items']:
            all_items_for_qid.extend(file_info['items'])
    
    # å…ˆä¿å­˜å»é‡å‰çš„æ•°æ®
    print(f"\nğŸ’¾ ä¿å­˜å»é‡å‰çš„æ•°æ®...")
    if all_items_for_qid:
        # ä¸ºå»é‡å‰çš„æ•°æ®åˆ†é…ä¸´æ—¶qid
        source_file = all_items_for_qid[0].source_file if all_items_for_qid else "é—®ç­”æ•´ç†"
        for i, item in enumerate(all_items_for_qid, start=1):
            item.qid = f"{source_file}__Q{i:05d}"
        
        # ä¿å­˜å»é‡å‰çš„combinedæ–‡ä»¶
        with combined_jsonl_before_dedup.open("w", encoding="utf-8") as f:
            for it in all_items_for_qid:
                f.write(json.dumps(asdict(it), ensure_ascii=False) + "\n")
        print(f"âœ… å»é‡å‰æ•°æ®å·²ä¿å­˜ï¼š{combined_jsonl_before_dedup} ({len(all_items_for_qid)} é¢˜)")
    
    # è¿›è¡Œç›¸ä¼¼åº¦èšç±»å’Œå»é‡å¤„ç†
    print(f"\nğŸ”„ å¼€å§‹ç›¸ä¼¼åº¦èšç±»å’Œå»é‡å¤„ç†ï¼ŒåŸå§‹é¢˜ç›®æ•°ï¼š{len(all_items_for_qid)}")
    cluster_log_file = out_root / "cluster_log.md"
    clustered_items, cluster_logs = cluster_and_deduplicate_questions(all_items_for_qid, SIMILARITY_THRESHOLD, cluster_log_file)
    
    # å¯¹èšç±»åçš„æ•°æ®é‡æ–°åˆ†é…qidï¼ˆæŒ‰æ–‡ä»¶é¡ºåºï¼Œç„¶åæŒ‰é¡¹ç›®é¡ºåºï¼‰
    print("ğŸ”¢ å¯¹èšç±»åçš„æ•°æ®é‡æ–°åˆ†é…qid...")
    source_file = clustered_items[0].source_file if clustered_items else "é—®ç­”æ•´ç†"
    for i, item in enumerate(clustered_items, start=1):
        item.qid = f"{source_file}__Q{i:05d}"
    
    # ä¿å­˜æ–‡ä»¶çº§ç»“æœï¼ˆä½¿ç”¨èšç±»åçš„æ•°æ®ï¼‰
    print("ğŸ’¾ ä¿å­˜æ–‡ä»¶çº§ç»“æœ...")
    for base_norm, file_info in file_stats.items():
        if file_info['items']:
            # ä»èšç±»åçš„æ•°æ®ä¸­ç­›é€‰å±äºå½“å‰æ–‡ä»¶çš„é¢˜ç›®
            file_clustered_items = [item for item in clustered_items if item.source_file == base_norm]
            
            # ä¿å­˜æ–‡ä»¶çº§ç»“æœ
            jsonl_path = save_items_per_file(file_clustered_items, out_root, base_norm)
            raw_path = None
            if raw_root:
                raw_path = raw_root / base_norm / "raw_chunks.jsonl"
                save_raw_chunks(file_info['raw_records'], raw_path)
            
            print(f"âœ… {file_info['path'].name} â†’ æŠ½å– {len(file_info['items'])} é¢˜ â†’ èšç±»å {len(file_clustered_items)} é¢˜")
            if raw_path:
                print(f"ğŸ“‚ åŸæ–‡ç‰‡æ®µå¯¹ç…§ï¼š{raw_path}")
            
            # æ”¶é›†æ‰€æœ‰é¢˜ç›®ç”¨äºæ‰¹é‡å†™å…¥combinedæ–‡æ¡£
            for it in file_clustered_items:
                combined_rows.append({
                    "qid": it.qid, "source_file": it.source_file,
                    "question_preview": it.question[:160].replace("\n", " "),
                    "has_answer": 1 if it.answer else 0,
                    "has_reference": 1 if it.reference else 0,
                })
    
    
    # æ‰¹é‡å†™å…¥combinedæ–‡æ¡£ï¼ˆèšç±»åï¼‰
    print("\nğŸ’¾ æ‰¹é‡å†™å…¥combinedæ–‡æ¡£ï¼ˆèšç±»åï¼‰...")
    if clustered_items:
        # å†™å…¥èšç±»åçš„JSONL
        with combined_jsonl.open("w", encoding="utf-8") as f:
            for it in clustered_items:
                f.write(json.dumps(asdict(it), ensure_ascii=False) + "\n")
        
        print(f"âœ… å·²æ‰¹é‡å†™å…¥combinedæ–‡æ¡£ï¼Œæ€»è®¡ï¼š{len(clustered_items)} é¢˜ï¼ˆèšç±»å»é‡åï¼‰")

    print("\n=== å¤„ç†å®Œæˆ ===")
    print(f"ğŸ“Š ç»Ÿè®¡ï¼šåŸå§‹ {len(all_items_for_qid)} é¢˜ â†’ èšç±»å {len(clustered_items)} é¢˜")
    print(f"ğŸ—‚ ç»“æœæ–‡ä»¶ï¼š{combined_jsonl.resolve()}")
    if cluster_logs:
        print(f"ğŸ“ èšç±»æ—¥å¿—ï¼š{cluster_log_file.resolve()}")
    

if __name__ == "__main__":
    main()


# è¿è¡Œç¤ºä¾‹
'''
æŒ‰ç…§å­—ç¬¦çª—å£æŠ½å–
python /home/wangxi/workspace/data-engine/wx/fqa_extract/1_fqa_extract.py \
  /home/wangxi/workspace/xiaofang/fqa/é—®ç­”æ•´ç†.docx \
  --use-char-window --window-chars 3000 --stride-chars 1800 \
  --max-workers 256 \
  --timeout 120 \
  --out-dir out_qas_$(date +%m%d%H%M) \
  --save-raw-dir raw_logs_$(date +%m%d%H%M) \
  --debug-log-dir debug_logs_$(date +%m%d%H%M)
'''

'''
æŒ‰ç…§è¡Œçª—å£æŠ½å–ï¼ˆä¸æ¨èï¼‰
python /home/wangxi/workspace/data-engine/wx/fqa_extract/1_fqa_extract.py \
  /home/wangxi/workspace/xiaofang/fqa/é—®ç­”æ•´ç†.md \
  --window 80 --stride 40 \
  --max-workers 256 \
  --timeout 120 \
  --out-dir out_qas_$(date +%m%d%H%M) \
  --save-raw-dir raw_logs_$(date +%m%d%H%M) \
  --debug-log-dir debug_logs_$(date +%m%d%H%M)
'''