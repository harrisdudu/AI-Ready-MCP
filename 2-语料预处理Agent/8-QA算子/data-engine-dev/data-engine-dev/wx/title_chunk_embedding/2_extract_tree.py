#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import json
import time
import argparse
import random
import os
import shutil
from dataclasses import dataclass, asdict
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
import re
import concurrent.futures as cf


# ========= Ark é…ç½® =========
API_KEY  = "c702676e-a69f-4ff0-a672-718d0d4723ed"
MODEL_ID = "doubao-seed-1-6-250615"

try:
    from volcenginesdkarkruntime import Ark
except Exception:
    raise SystemExit("âŒ è¯·å…ˆå®‰è£… Ark SDK: pip install 'volcengine-python-sdk[ark]'")

# ========= å‚æ•°å¸¸é‡ =========
DEFAULT_WINDOW_LINES = 80
DEFAULT_STRIDE_LINES = 60

# å•æ¬¡è¯·æ±‚æœ€å¤§å­—ç¬¦é¢„ç®—
MAX_CHARS_PER_PROMPT = 9000
# Ark é»˜è®¤è¯·æ±‚è¶…æ—¶ï¼ˆç§’ï¼‰
DEFAULT_ARK_TIMEOUT = 120
# é»˜è®¤å¹¶å‘çº¿ç¨‹æ•°ï¼ˆé™ä½ä»¥é¿å…æ–‡ä»¶æè¿°ç¬¦é—®é¢˜ï¼‰
DEFAULT_MAX_WORKERS = 8

# ========= System/User Prompt =========
TOC_EXTRACT_PROMPT = """\
ä½ æ˜¯ä¸€ä¸ªç²¾ç¡®çš„æ–‡æ¡£ç›®å½•ç»“æ„æå–åŠ©æ‰‹ã€‚
æˆ‘å°†ç»™ä½ ä¸€æ®µä»æ–‡æ¡£ PDF è½¬æˆ Markdown çš„æ–‡æœ¬ç‰‡æ®µã€‚
ä½ çš„ä»»åŠ¡ï¼šä»æ­£æ–‡ä¸­æå–æ ‡é¢˜ç»“æ„ã€‚

è¦æ±‚ï¼š
1) è¦ä»¥è¡Œä¸ºå•ä½åˆ¤æ–­æ ‡é¢˜ï¼Œæ•´è¡Œä½œä¸ºæ•´ä½“åˆ¤æ–­ï¼Œæ ‡é¢˜ä¸€èˆ¬éƒ½æ¯”è¾ƒçŸ­ï¼Œä¸è¦æå–é•¿æ–‡æœ¬
2) è¯†åˆ«æ‰€æœ‰æ ‡é¢˜ï¼Œæå–å®Œæ•´çš„æ ‡é¢˜æ–‡æœ¬ï¼ŒåŒ…æ‹¬ç¼–å·å’Œæ ‡é¢˜å†…å®¹
    - æ ‡é¢˜éƒ½æ˜¯æ ‡é¢˜ç¼–å·æ•°å­—+æ ‡é¢˜å†…å®¹çš„ç»“æ„ï¼Œä¸è¦æå–æ²¡æœ‰æ•°å­—çš„æ ‡é¢˜
    - æ ‡é¢˜ç¼–å·æ•°å­—æ¯”å¦‚ï¼š1.ã€1.1ã€1.1.1ã€ç¬¬ä¸€ç« ã€ç¬¬ä¸€èŠ‚ã€ä¸€ã€äºŒã€ä¸‰ã€1ã€2ã€3ã€(1)ã€(2)ã€(3)ã€ï¼ˆä¸€ï¼‰ã€ï¼ˆäºŒï¼‰ã€ï¼ˆä¸‰ï¼‰ã€[1]ã€[2]ã€[3]ã€ã€ä¸€ã€‘ã€ã€äºŒã€‘ã€ã€ä¸‰ã€‘ã€A.ã€B.ã€C.ã€a.ã€b.ã€c.ã€I.ã€II.ã€III.ã€i.ã€ii.ã€iii.ç­‰
3) è¡¨æ ¼é‡Œçš„æ ‡é¢˜ä¸è¦æå–
4) ä¸è¦æå–ç›®å½•ï¼Œåªæå–æ­£æ–‡ä¸­çš„å®é™…ç« èŠ‚æ ‡é¢˜ï¼Œå¦‚æœé‡åˆ°ç›®å½•éƒ¨åˆ†ï¼Œç›´æ¥è·³è¿‡ï¼Œä¸æå–ä»»ä½•å†…å®¹
5) å¦‚æœæ£€æµ‹åˆ°é™„ä»¶ã€å‚è€ƒæ–‡çŒ®ã€é™„å½•ç­‰éæ­£æ–‡å†…å®¹ï¼Œè¿”å›ç‰¹æ®Šæ ‡è®° {"has_attachment": true}

è¾“å‡ºæ ¼å¼ï¼š
- æ­£å¸¸æƒ…å†µä¸‹è¿”å›JSONæ•°ç»„ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«ï¼š
  {"title": å®Œæ•´çš„è¿™è¡Œæ ‡é¢˜æ–‡æœ¬ï¼ˆåŒ…å«ç¼–å·å’Œæ ‡é¢˜å†…å®¹ï¼‰}
- å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ ‡é¢˜ï¼Œè¿”å›ç©ºæ•°ç»„ []
- å¦‚æœæ£€æµ‹åˆ°é™„ä»¶ï¼Œè¿”å› {"has_attachment": true}

åªè¾“å‡ºä¸¥æ ¼çš„JSONæ ¼å¼ï¼Œä¸è¦åŒ…å«ä»»ä½•é¢å¤–æ–‡å­—ã€‚
ä¸è¦ç¼–é€ å†…å®¹ï¼Œåªæå–å®é™…å­˜åœ¨çš„æ ‡é¢˜ã€‚
"""

TOC_VERIFY_PROMPT = """\
ä½ æ˜¯ä¸€ä¸ªç²¾ç¡®çš„æ–‡æ¡£ç›®å½•éªŒè¯åŠ©æ‰‹ã€‚
æˆ‘å°†ç»™ä½ ä¸€ä¸ªå·²æå–çš„æ ‡é¢˜ç»“æ„åˆ—è¡¨å’Œä¸€æ®µæ–‡æ¡£æ–‡æœ¬ç‰‡æ®µã€‚
ä½ çš„ä»»åŠ¡ï¼šæ£€æŸ¥è¿™ä¸ªç›®å½•ç»“æ„æ˜¯å¦å®Œæ•´å’Œå‡†ç¡®ï¼Œå¹¶è¾“å‡ºæœ€ç»ˆç›®å½•ç»“æ„ã€‚

è¦æ±‚ï¼š
1) ä»”ç»†æ£€æŸ¥æ–‡æ¡£ç‰‡æ®µä¸­æ˜¯å¦è¿˜æœ‰é—æ¼çš„æ ‡é¢˜
2) æ£€æŸ¥å·²æå–çš„æ ‡é¢˜æ˜¯å¦çœŸçš„å­˜åœ¨ï¼ˆä¸æ˜¯è¯¯åˆ¤ï¼‰
3) æ ‡é¢˜æ˜¯æ»‘åŠ¨çª—å£æ‹¼æ¥èµ·æ¥çš„ï¼Œæ£€æŸ¥æ ‡é¢˜æ˜¯å¦è¿è´¯ï¼Œå¦‚æœå­˜åœ¨é‡å¤ï¼Œåªä¿ç•™ä¸€ä¸ª
3) é‡ç‚¹å…³æ³¨ï¼š
   - æ˜¯å¦æœ‰æ–°çš„æ ‡é¢˜è¢«é—æ¼
   - æ˜¯å¦æœ‰éæ ‡é¢˜å†…å®¹è¢«è¯¯åˆ¤ä¸ºæ ‡é¢˜
4) åªå…³æ³¨æ­£æ–‡ä¸­çš„å®é™…ç« èŠ‚æ ‡é¢˜ï¼Œå¿½ç•¥ï¼š
   - ç›®å½•éƒ¨åˆ†
   - è¡¨æ ¼ä¸­çš„æ ‡é¢˜
   - é™„ä»¶ã€å‚è€ƒæ–‡çŒ®ç­‰éæ­£æ–‡éƒ¨åˆ†

è¾“å‡ºæ ¼å¼ï¼š
- è¿”å›JSONæ•°ç»„ï¼ŒåŒ…å«æœ€ç»ˆçš„å‡†ç¡®å®Œæ•´çš„æ ‡é¢˜åˆ—è¡¨
- æ¯ä¸ªæ ‡é¢˜é¡¹åŒ…å«ï¼š
  {"title": å®Œæ•´çš„æ ‡é¢˜æ–‡æœ¬}

åªè¾“å‡ºä¸¥æ ¼çš„JSONæ ¼å¼ï¼Œä¸è¦åŒ…å«ä»»ä½•é¢å¤–æ–‡å­—ã€‚
ç›´æ¥è¾“å‡ºæœ€ç»ˆçš„æ ‡é¢˜æ•°ç»„ï¼Œä¸è¦åˆ†ç±»ã€‚
"""

# ========= æ•°æ®ç»“æ„ =========
@dataclass
class TOCItem:
    """ç›®å½•é¡¹"""
    title: str

@dataclass
class Section:
    """æ–‡æ¡£ç« èŠ‚"""
    section_title: str
    content: str
    source_file: str

@dataclass
class ExtractStats:
    files_processed: int = 0
    files_copied: int = 0
    files_with_titles: int = 0
    title_items: int = 0
    sections_extracted: int = 0
    # æ–°å¢éªŒè¯ç›¸å…³ç»Ÿè®¡
    missing_titles_found: int = 0
    extra_titles_found: int = 0
    correct_titles_found: int = 0
    # æ–°å¢é™„ä»¶æ£€æµ‹ç›¸å…³ç»Ÿè®¡
    files_with_attachments: int = 0

# ========= Ark è°ƒç”¨ =========
def call_ark(system_prompt: str, user_prompt: str, api_key: str, model_id: str,
             temperature: float = 0.0, top_p: float = 0.9,
             timeout: int = DEFAULT_ARK_TIMEOUT) -> str:
    time.sleep(random.uniform(0.05, 0.20))  # è½»å¾®æŠ–åŠ¨
    client = Ark(api_key=api_key, timeout=timeout)
    resp = client.chat.completions.create(
        model=model_id,
        messages=[
            {"role":"system","content":system_prompt},
            {"role":"user","content":user_prompt},
        ],
        temperature=temperature,
        top_p=top_p,
    )
    try:
        return resp.choices[0].message.content
    except Exception:
        return str(resp)

# ========= JSON è§£æ + æ¸…æ´— =========
def _strip_code_fences(s: str) -> str:
    s = s.strip()
    fences = ("```json", "```JSON", "```", "~~~json", "~~~JSON", "~~~")
    for f in fences:
        if s.startswith(f):
            s = s[len(f):].strip()
        if s.endswith(f):
            s = s[:-len(f)].strip()
    return s

def _clean_unicode_escapes(s: str) -> str:
    """
    æ¸…ç†Unicodeè½¬ä¹‰åºåˆ—ï¼Œå°†\\u3001ç­‰è½¬æ¢ä¸ºå®é™…å­—ç¬¦
    """
    # å¤„ç†å¸¸è§çš„Unicodeè½¬ä¹‰åºåˆ—
    unicode_mappings = {
        '\\u3001': 'ã€',  # ä¸­æ–‡é¡¿å·
        '\\u3002': 'ã€‚',  # ä¸­æ–‡å¥å·
        '\\u300a': 'ã€Š',  # ä¸­æ–‡å·¦ä¹¦åå·
        '\\u300b': 'ã€‹',  # ä¸­æ–‡å³ä¹¦åå·
        '\\u2014': 'â€”',  # ç ´æŠ˜å·
        '\\u2018': ''',  # å·¦å•å¼•å·
        '\\u2019': ''',  # å³å•å¼•å·
        '\\u201c': '"',  # å·¦åŒå¼•å·
        '\\u201d': '"',  # å³åŒå¼•å·
    }
    
    for escape, char in unicode_mappings.items():
        s = s.replace(escape, char)
    
    return s

def force_json_load_with_sanitize(s: str) -> Tuple[Any, bool]:
    """
    è¿”å› (obj, sanitized_used)
    """
    s0 = s.strip()
    
    # æ¸…ç†Unicodeè½¬ä¹‰åºåˆ—
    s0 = _clean_unicode_escapes(s0)
    
    try:
        if s0.startswith('[') and s0.endswith(']'):
            return json.loads(s0), False
    except Exception:
        pass

    first = s0.find('['); last = s0.rfind(']')
    core = s0 if (first == -1 or last == -1 or first >= last) else s0[first:last+1]
    core = _strip_code_fences(core)

    try:
        return json.loads(core), False
    except Exception:
        return [], True  # å¦‚æœè§£æå¤±è´¥ï¼Œè¿”å›ç©ºæ•°ç»„

# ========= å·¥å…·å‡½æ•° =========
def make_windows(lines: List[str], window: int, stride: int) -> List[Tuple[int, int, str]]:
    # ç¡®ä¿å‚æ•°æ˜¯æ•´æ•°
    window = int(window)
    stride = int(stride)
    
    n = len(lines); out = []; i = 0
    while i < n:
        s = i; e = min(i + window - 1, n - 1)
        out.append((s+1, e+1, "\n".join(lines[s:e+1])))
        if e == n - 1: break
        i += stride
    return out

def split_document_by_titles(lines: List[str], toc_items: List[TOCItem], source_file: str) -> List[Section]:
    """
    æ ¹æ®æ ‡é¢˜ç»“æ„æ‹†åˆ†æ–‡æ¡£å†…å®¹ï¼ˆä½¿ç”¨æ¸…æ´—åçš„æ–‡æœ¬ï¼‰
    ä»ç¬¬ä¸€ä¸ªæ ‡é¢˜åœ¨å…¨æ–‡ä¸­çš„æœ€åä¸€ä¸ªåŒ¹é…ä½ç½®å¼€å§‹ä½œä¸ºå…¨æ–‡èµ·å§‹ç‚¹ï¼Œé¿å…ç›®å½•å¹²æ‰°
    """
    if not toc_items:
        return []
    
    sections = []
    full_text = "\n".join(lines)
    
    # æ‰¾åˆ°ç¬¬ä¸€ä¸ªæ ‡é¢˜åœ¨å…¨æ–‡ä¸­çš„æœ€åä¸€ä¸ªåŒ¹é…ä½ç½®ï¼Œä½œä¸ºæ­£æ–‡èµ·å§‹ç‚¹
    first_title = toc_items[0].title
    first_title_pos = full_text.find(first_title)
    if first_title_pos == -1:
        # å¦‚æœæ‰¾ä¸åˆ°ç¬¬ä¸€ä¸ªæ ‡é¢˜ï¼Œè¿”å›ç©ºç»“æœ
        return []
    
    # ä»ç¬¬ä¸€ä¸ªæ ‡é¢˜çš„æœ€åä¸€ä¸ªåŒ¹é…ä½ç½®å¼€å§‹
    # å¦‚æœç¬¬ä¸€ä¸ªæ ‡é¢˜åœ¨æ–‡æ¡£ä¸­å‡ºç°å¤šæ¬¡ï¼Œå–æœ€åä¸€ä¸ªä½ç½®
    last_occurrence_pos = first_title_pos
    current_pos = first_title_pos
    while True:
        next_pos = full_text.find(first_title, current_pos + 1)
        if next_pos == -1:
            break
        last_occurrence_pos = next_pos
        current_pos = next_pos
    
    # ä»ç¬¬ä¸€ä¸ªæ ‡é¢˜çš„æœ€åä¸€ä¸ªåŒ¹é…ä½ç½®å¼€å§‹æˆªå–æ­£æ–‡
    body_text_start = last_occurrence_pos
    body_text = full_text[body_text_start:]
    
    # ä¸ºæ¯ä¸ªæ ‡é¢˜åˆ›å»ºç« èŠ‚
    for i, toc_item in enumerate(toc_items):       
        # åœ¨æ­£æ–‡éƒ¨åˆ†ä¸­æŸ¥æ‰¾æ ‡é¢˜ï¼ˆä½¿ç”¨ç›¸å¯¹ä½ç½®ï¼‰
        title_pos = body_text.find(toc_item.title)
        if title_pos == -1:
            # å¦‚æœæ‰¾ä¸åˆ°ï¼Œè·³è¿‡è¿™ä¸ªæ ‡é¢˜
            continue
        
        # æ‰¾åˆ°ä¸‹ä¸€ä¸ªæ ‡é¢˜ä½ç½®
        next_title_pos = len(body_text)
        if i + 1 < len(toc_items):
            # åœ¨å½“å‰æ ‡é¢˜ä¹‹åæŸ¥æ‰¾ä¸‹ä¸€ä¸ªæ ‡é¢˜
            for j in range(i + 1, len(toc_items)):
                next_candidate_pos = body_text.find(toc_items[j].title, title_pos + 1)
                
                if next_candidate_pos != -1:
                    next_title_pos = next_candidate_pos
                    break
        
        # æå–å†…å®¹
        content_start = title_pos + len(toc_item.title)
        # è·³è¿‡æ ‡é¢˜åçš„æ ‡ç‚¹ç¬¦å·ï¼ˆå¦‚å¥å·ã€é¡¿å·ç­‰ï¼‰
        while content_start < next_title_pos and body_text[content_start] in 'ã€‚ã€ï¼Œï¼›ï¼š':
            content_start += 1
        content = body_text[content_start:next_title_pos].strip()
        
        # åˆ›å»ºç« èŠ‚å¯¹è±¡
        section = Section(
            section_title=toc_item.title,
            content=content,
            source_file=source_file
        )
        sections.append(section)
    
    return sections

# ========= ç›®å½•æå– =========
def extract_toc_from_chunk(s_line: int, e_line: int, chunk_text: str,
                          max_retries: int, timeout: int,
                          debug_dir: Optional[Path], base_norm: str) -> Tuple[List[TOCItem], Dict[str, Any]]:
    """
    ä»æ–‡æœ¬ç‰‡æ®µä¸­æå–ç›®å½•ç»“æ„
    """    
    lines_with_numbers = []
    for i, line in enumerate(chunk_text.split('\n')):
        # ä½¿ç”¨ç»å¯¹è¡Œå·ï¼ŒåŸºäºåŸå§‹æ–‡æ¡£çš„è¡Œå·
        absolute_line_num = s_line + i
        lines_with_numbers.append(f"{absolute_line_num:06d}â”‚{line}")
    
    numbered_text = '\n'.join(lines_with_numbers)
    
    user_prompt = f"""è¯·ä»”ç»†é˜…è¯»ä¸‹æ–¹ç»™å‡ºçš„æ–‡æ¡£ç‰‡æ®µï¼ˆç¬¬ {s_line} è¡Œåˆ°ç¬¬ {e_line} è¡Œï¼‰ï¼Œå¹¶ä»ä¸­æå–å‡ºæ¸…æ™°çš„ç›®å½•ç»“æ„ã€‚

--- æ–‡æ¡£ç‰‡æ®µå¼€å§‹ ---
{numbered_text}
--- æ–‡æ¡£ç‰‡æ®µç»“æŸ ---
"""

    meta = {
        "prompt_len": len(user_prompt), "response_len": 0,
        "parse_error": "", "sanitized_used": False,
        "has_attachment": False
    }

    # è°ƒè¯•ç›®å½•
    wdir = None
    if debug_dir:
        wdir = debug_dir / base_norm / f"toc_{s_line}-{e_line}"
        wdir.mkdir(parents=True, exist_ok=True)
        try:
            with (wdir / "system_prompt.txt").open("w", encoding="utf-8") as f:
                f.write(TOC_EXTRACT_PROMPT)
            with (wdir / "user_prompt.txt").open("w", encoding="utf-8") as f:
                f.write(user_prompt)
        except Exception as e:
            print(f"âš ï¸ å†™å…¥è°ƒè¯•æ–‡ä»¶å¤±è´¥ï¼š{e}")

    # é‡è¯•æœºåˆ¶
    attempt = 0
    last_err = None
    while attempt <= max_retries:
        try:
            raw = call_ark(TOC_EXTRACT_PROMPT, user_prompt, api_key=API_KEY, model_id=MODEL_ID, timeout=timeout)
            if debug_dir:
                try:
                    with (wdir / "response.txt").open("w", encoding="utf-8") as f:
                        f.write(raw)
                except Exception as e:
                    print(f"âš ï¸ å†™å…¥å“åº”è°ƒè¯•æ–‡ä»¶å¤±è´¥ï¼š{e}")
            meta["response_len"] = len(raw)
            
            # æ£€æŸ¥æ˜¯å¦æ£€æµ‹åˆ°é™„ä»¶
            try:
                obj = json.loads(raw.strip())
                if isinstance(obj, dict) and obj.get("has_attachment", False):
                    meta["has_attachment"] = True
                    return [], meta
            except Exception:
                pass
            
            obj, sanitized_used = force_json_load_with_sanitize(raw)
            meta["sanitized_used"] = sanitized_used
            
            # è½¬æ¢ä¸ºTOCItemå¯¹è±¡
            toc_items = []
            for item in obj:
                if isinstance(item, dict):
                    toc_items.append(TOCItem(
                        title=item.get("title", "")
                    ))
            
            if debug_dir and sanitized_used:
                try:
                    with (wdir / "sanitized.json").open("w", encoding="utf-8") as f:
                        json.dump(obj, f, ensure_ascii=False, indent=2)
                except Exception as e:
                    print(f"âš ï¸ å†™å…¥sanitizedè°ƒè¯•æ–‡ä»¶å¤±è´¥ï¼š{e}")
            
            return toc_items, meta
            
        except Exception as e:
            last_err = str(e)
            meta["parse_error"] = last_err
            if debug_dir:
                try:
                    with (wdir / "parse_error.txt").open("w", encoding="utf-8") as f:
                        f.write(last_err)
                except Exception as write_err:
                    print(f"âš ï¸ å†™å…¥é”™è¯¯è°ƒè¯•æ–‡ä»¶å¤±è´¥ï¼š{write_err}")
            if attempt == max_retries:
                break
            time.sleep((1.8 ** attempt) + random.uniform(0.1, 0.5))
            attempt += 1

    return [], meta

def verify_toc_structure(s_line: int, e_line: int, chunk_text: str, 
                        existing_toc_items: List[TOCItem],
                        max_retries: int, timeout: int,
                        debug_dir: Optional[Path], base_norm: str) -> Tuple[List[TOCItem], Dict[str, Any]]:
    """
    éªŒè¯å·²æå–çš„æ ‡é¢˜ç»“æ„ï¼Œè¾“å‡ºæœ€ç»ˆçš„å®Œæ•´æ ‡é¢˜åˆ—è¡¨
    """    
    # ä¸ºæ¯ä¸€è¡Œæ·»åŠ ç»å¯¹è¡Œå·
    lines_with_numbers = []
    for i, line in enumerate(chunk_text.split('\n')):
        # ä½¿ç”¨ç»å¯¹è¡Œå·ï¼ŒåŸºäºåŸå§‹æ–‡æ¡£çš„è¡Œå·
        absolute_line_num = s_line + i
        lines_with_numbers.append(f"{absolute_line_num:06d}â”‚{line}")
    
    numbered_text = '\n'.join(lines_with_numbers)
    
    # å‡†å¤‡ç°æœ‰æ ‡é¢˜ç»“æ„
    existing_titles_text = ""
    for item in existing_toc_items:
        existing_titles_text += f"{item.title}\n"
    
    user_prompt = f"""è¯·è¾“å‡ºæœ€ç»ˆçš„å®Œæ•´æ ‡é¢˜ç»“æ„ã€‚

--- å·²æå–çš„æ ‡é¢˜ç»“æ„ ---
{existing_titles_text.strip()}

--- æ–‡æ¡£ç‰‡æ®µï¼ˆç¬¬ {s_line} è¡Œåˆ°ç¬¬ {e_line} è¡Œï¼‰ ---
{numbered_text}
--- æ–‡æ¡£ç‰‡æ®µç»“æŸ ---

è¯·ä»”ç»†æ£€æŸ¥å¹¶è¾“å‡ºæœ€ç»ˆçš„å®Œæ•´æ ‡é¢˜åˆ—è¡¨ã€‚
"""

    meta = {
        "prompt_len": len(user_prompt), "response_len": 0,
        "parse_error": "", "sanitized_used": False,
    }

    # è°ƒè¯•ç›®å½•
    wdir = None
    if debug_dir:
        wdir = debug_dir / base_norm / f"verify_{s_line}-{e_line}"
        wdir.mkdir(parents=True, exist_ok=True)
        try:
            with (wdir / "system_prompt.txt").open("w", encoding="utf-8") as f:
                f.write(TOC_VERIFY_PROMPT)
            with (wdir / "user_prompt.txt").open("w", encoding="utf-8") as f:
                f.write(user_prompt)
        except Exception as e:
            print(f"âš ï¸ å†™å…¥è°ƒè¯•æ–‡ä»¶å¤±è´¥ï¼š{e}")

    # é‡è¯•æœºåˆ¶
    attempt = 0
    last_err = None
    while attempt <= max_retries:
        try:
            raw = call_ark(TOC_VERIFY_PROMPT, user_prompt, api_key=API_KEY, model_id=MODEL_ID, timeout=timeout)
            if debug_dir:
                try:
                    with (wdir / "response.txt").open("w", encoding="utf-8") as f:
                        f.write(raw)
                except Exception as e:
                    print(f"âš ï¸ å†™å…¥å“åº”è°ƒè¯•æ–‡ä»¶å¤±è´¥ï¼š{e}")
            meta["response_len"] = len(raw)
            
            # è§£æJSONå“åº”
            try:
                obj = json.loads(raw.strip())
            except Exception:
                # å°è¯•æ¸…ç†å’Œé‡æ–°è§£æ
                obj, sanitized_used = force_json_load_with_sanitize(raw)
                meta["sanitized_used"] = sanitized_used
            else:
                meta["sanitized_used"] = False
            
            # è½¬æ¢ä¸ºTOCItemå¯¹è±¡
            final_toc_items = []
            if isinstance(obj, list):
                for item in obj:
                    if isinstance(item, dict):
                        final_toc_items.append(TOCItem(
                            title=item.get("title", "")
                        ))
            
            if debug_dir and meta["sanitized_used"]:
                try:
                    with (wdir / "sanitized.json").open("w", encoding="utf-8") as f:
                        json.dump(obj, f, ensure_ascii=False, indent=2)
                except Exception as e:
                    print(f"âš ï¸ å†™å…¥sanitizedè°ƒè¯•æ–‡ä»¶å¤±è´¥ï¼š{e}")
            
            return final_toc_items, meta
            
        except Exception as e:
            last_err = str(e)
            meta["parse_error"] = last_err
            if debug_dir:
                try:
                    with (wdir / "parse_error.txt").open("w", encoding="utf-8") as f:
                        f.write(last_err)
                except Exception as write_err:
                    print(f"âš ï¸ å†™å…¥é”™è¯¯è°ƒè¯•æ–‡ä»¶å¤±è´¥ï¼š{write_err}")
            if attempt == max_retries:
                break
            time.sleep((1.8 ** attempt) + random.uniform(0.1, 0.5))
            attempt += 1

    # å¦‚æœéªŒè¯å¤±è´¥ï¼Œè¿”å›ç©ºç»“æœ
    return [], meta

# ========= å•æ–‡ä»¶å¤„ç† =========
def process_single_file(md_path: Path,
                       toc_window: int, toc_stride: int,
                       max_retries: int, timeout: int,
                       debug_dir: Optional[Path],
                       out_dir: Path, input_dir: Path) -> Tuple[str, List[TOCItem], List[Section], ExtractStats]:
    """
    å¤„ç†å•ä¸ªæ–‡ä»¶ï¼šå¤åˆ¶æ–‡ä»¶å¹¶æå–æ ‡é¢˜ç»“æ„å’Œæ‹†åˆ†æ–‡æ¡£
    """
    base_norm = md_path.stem
    
    # è·å–*_outputæ–‡ä»¶å¤¹åç§°
    output_folder_name = input_dir.name
    
    # åˆ›å»ºè¾“å‡ºç›®å½•ï¼ˆåªæŒ‰*_outputæ–‡ä»¶å¤¹åç§°åˆ†ç±»ï¼‰
    out_dir_file = out_dir / output_folder_name / base_norm
    out_dir_file.mkdir(parents=True, exist_ok=True)
    
    # å¤åˆ¶åŸå§‹markdownæ–‡ä»¶åˆ°è¾“å‡ºç›®å½•
    dest_file = out_dir_file / md_path.name
    copy_success = False
    try:
        shutil.copy2(md_path, dest_file)
        print(f"ğŸ“ å·²å¤åˆ¶åŸå§‹æ–‡ä»¶ï¼š{md_path.name} -> {dest_file}")
        copy_success = True
    except Exception as e:
        print(f"âŒ å¤åˆ¶åŸå§‹æ–‡ä»¶å¤±è´¥ {md_path}: {e}")
    
    # ä½¿ç”¨åŸå§‹æ–‡ä»¶è¿›è¡Œå¤„ç†
    text = md_path.read_text(encoding="utf-8", errors="ignore")
    lines = text.splitlines()
    
    # æ¸…æ´—æ–‡æœ¬ï¼šå»æ‰æ¯è¡Œå¼€å¤´çš„#å·å’Œä¸­é—´çš„ç©ºæ ¼
    cleaned_lines = []
    for line in lines:
        # å»æ‰å¼€å¤´çš„#å·
        cleaned_line = re.sub(r'^#+\s*', '', line)
        # å»æ‰ä¸­é—´çš„ç©ºæ ¼
        cleaned_line = re.sub(r'\s+', '', cleaned_line)
        # è·³è¿‡ç©ºè¡Œ
        if cleaned_line:
            cleaned_lines.append(cleaned_line)
    
    lines = cleaned_lines
    
    stats = ExtractStats(files_processed=1, files_copied=1 if copy_success else 0)
    
    # ç¬¬ä¸€æ­¥ï¼šæå–æ–‡æ¡£ç»“æ„
    print(f"ğŸ” æå–æ–‡æ¡£ç»“æ„ï¼š{md_path.name}")
    toc_windows = make_windows(lines, toc_window, toc_stride)
    
    all_toc_items = []
    attachment_detected = False
    # æå–ç»“æ„è¿›åº¦è·Ÿè¸ª
    total_windows = len(toc_windows)
    for i, (s_line, e_line, chunk) in enumerate(toc_windows):
        if i % 5 == 0 or i == total_windows - 1:  # æ¯5ä¸ªæˆ–æœ€åä¸€ä¸ªæ˜¾ç¤ºè¿›åº¦
            print(f"ğŸ” æå–ç»“æ„è¿›åº¦: {i+1}/{total_windows} ({(i+1)/total_windows*100:.1f}%)")
        toc_items, meta = extract_toc_from_chunk(
            s_line, e_line, chunk, max_retries, timeout, debug_dir, base_norm
        )
        
        # æ£€æŸ¥æ˜¯å¦æ£€æµ‹åˆ°é™„ä»¶
        if meta.get("has_attachment", False):
            attachment_detected = True
            print(f"ğŸ“ åœ¨ç¬¬ {s_line}-{e_line} è¡Œæ£€æµ‹åˆ°é™„ä»¶ï¼Œåœæ­¢åç»­æ ‡é¢˜æå–")
            stats.files_with_attachments = 1
            break
        
        all_toc_items.extend(toc_items)
    
    # ä¿ç•™æ‰€æœ‰æ ‡é¢˜é¡¹ï¼ˆä¸å»é‡ï¼Œå…è®¸ç›¸åŒæ ‡é¢˜åœ¨ä¸åŒä½ç½®å‡ºç°ï¼‰
    unique_toc_items = all_toc_items  # ç›´æ¥ä½¿ç”¨æ‰€æœ‰æ ‡é¢˜ï¼Œä¸å»é‡ï¼ˆå¦åˆ™åŒæ ·æ ‡é¢˜ä¸åŒçº§å‡ºç°å¯èƒ½ä¼šè¢«å½“ä½œé‡å¤ï¼‰
    
    stats.title_items = len(unique_toc_items)
    
    if unique_toc_items:
        stats.files_with_titles = 1
        print(f"âœ… æ‰¾åˆ° {len(unique_toc_items)} ä¸ªæ ‡é¢˜")
        
        # ç¬¬äºŒæ­¥ï¼šéªŒè¯æ ‡é¢˜ç»“æ„ï¼ˆæ–°å¢æ­¥éª¤ï¼‰
        print(f"ğŸ” éªŒè¯æ ‡é¢˜ç»“æ„ï¼š{md_path.name}")
        # éªŒè¯çª—å£å¢å¤§ä¸€å€
        verify_window = int(toc_window * 1.5)
        verify_stride = int(toc_stride * 1.5)
        verify_windows = make_windows(lines, verify_window, verify_stride)
        
        all_verified_titles = []
        
        # éªŒè¯ç»“æ„è¿›åº¦è·Ÿè¸ª
        total_verify_windows = len(verify_windows)
        for i, (s_line, e_line, chunk) in enumerate(verify_windows):
            if i % 5 == 0 or i == total_verify_windows - 1:  # æ¯5ä¸ªæˆ–æœ€åä¸€ä¸ªæ˜¾ç¤ºè¿›åº¦
                print(f"ğŸ” éªŒè¯ç»“æ„è¿›åº¦: {i+1}/{total_verify_windows} ({(i+1)/total_verify_windows*100:.1f}%)")
            # ä½¿ç”¨æ‰€æœ‰æ ‡é¢˜è¿›è¡ŒéªŒè¯
            window_toc_items = unique_toc_items
            
            # éªŒè¯æ‰€æœ‰çª—å£ï¼Œä¸ç®¡æ˜¯å¦æœ‰æ ‡é¢˜
            verified_titles, meta = verify_toc_structure(
                s_line, e_line, chunk, window_toc_items,
                max_retries, timeout, debug_dir, base_norm
            )
            
            all_verified_titles.extend(verified_titles)
        
        # ä¿ç•™æ‰€æœ‰éªŒè¯åçš„æ ‡é¢˜ï¼ˆä¸å»é‡ï¼‰
        final_toc_items = all_verified_titles  # ç›´æ¥ä½¿ç”¨æ‰€æœ‰éªŒè¯åçš„æ ‡é¢˜ï¼Œä¸å»é‡
        
        print(f"âœ… éªŒè¯åæœ€ç»ˆæ ‡é¢˜æ•°ï¼š{len(final_toc_items)} ä¸ª")
        
        # ç¬¬ä¸‰æ­¥ï¼šæ ¹æ®æ ‡é¢˜ç»“æ„æ‹†åˆ†æ–‡æ¡£
        print(f"ğŸ“„ æ‹†åˆ†æ–‡æ¡£å†…å®¹ï¼š{md_path.name}")
        sections = split_document_by_titles(lines, final_toc_items, md_path.name)
        
        # å»é‡ç« èŠ‚ï¼ˆåŸºäºæ ‡é¢˜ï¼Œä¼˜å…ˆä¿ç•™æœ‰å†…å®¹çš„ï¼‰
        title_to_sections = {}
        for section in sections:
            if section.section_title not in title_to_sections:
                title_to_sections[section.section_title] = []
            title_to_sections[section.section_title].append(section)
        
        unique_sections = []
        for title, section_list in title_to_sections.items():
            if len(section_list) == 1:
                # åªæœ‰ä¸€ä¸ªç« èŠ‚ï¼Œç›´æ¥ä¿ç•™
                unique_sections.append(section_list[0])
            else:
                # å¤šä¸ªç« èŠ‚ï¼Œä¼˜å…ˆä¿ç•™æœ‰å†…å®¹çš„
                sections_with_content = [s for s in section_list if s.content.strip()]
                if sections_with_content:
                    # æœ‰å†…å®¹çš„ç« èŠ‚ï¼Œä¿ç•™ç¬¬ä¸€ä¸ª
                    best_section = sections_with_content[0]
                else:
                    # éƒ½æ²¡æœ‰å†…å®¹ï¼Œä¿ç•™ç¬¬ä¸€ä¸ª
                    best_section = section_list[0]
                unique_sections.append(best_section)
        
        stats.sections_extracted = len(unique_sections)
        print(f"âœ… æ‹†åˆ†ä¸º {len(unique_sections)} ä¸ªç« èŠ‚ï¼ˆå»é‡åï¼‰")
        
        return base_norm, final_toc_items, unique_sections, stats
    else:
        print(f"âŒ æœªæ‰¾åˆ°æ–‡æ¡£ç»“æ„ï¼š{md_path.name}")
        return base_norm, [], [], stats

# ========= ä¿å­˜ç»“æœ =========
def save_results(base_norm: str, toc_items: List[TOCItem], sections: List,
                out_dir: Path, relative_path: Path = None, output_folder_name: str = None) -> Tuple[Path, Path]:
    """
    ä¿å­˜æå–çš„æ ‡é¢˜ç»“æ„
    """
    # å¦‚æœæä¾›äº†è¾“å‡ºæ–‡ä»¶å¤¹åç§°ï¼ŒåªæŒ‰*_outputæ–‡ä»¶å¤¹åç§°åˆ†ç±»
    if output_folder_name:
        out_dir_file = out_dir / output_folder_name / base_norm
    else:
        out_dir_file = out_dir / base_norm
    
    # ä¿å­˜æ–‡æ¡£ç»“æ„
    toc_path = out_dir_file / "structure.json"
    toc_data = []
    for item in toc_items:
            toc_data.append({
                "title": item.title
            })
    try:
        with toc_path.open("w", encoding="utf-8") as f:
            json.dump(toc_data, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"âš ï¸ ä¿å­˜æ–‡æ¡£ç»“æ„å¤±è´¥ï¼š{e}")
        toc_path = None
    
    # ä¿å­˜ç« èŠ‚å†…å®¹
    sections_path = out_dir_file / "sections.jsonl"
    try:
        with sections_path.open("w", encoding="utf-8") as f:
            for section in sections:
                f.write(json.dumps(asdict(section), ensure_ascii=False) + "\n")
    except Exception as e:
        print(f"âš ï¸ ä¿å­˜ç« èŠ‚å†…å®¹å¤±è´¥ï¼š{e}")
        sections_path = None
    
    return toc_path, sections_path

# ========= ä¸»ç¨‹åº =========
def main():
    ap = argparse.ArgumentParser(description="ä»æ–‡æ¡£ä¸­æå–æ ‡é¢˜ç»“æ„")
    ap.add_argument("input_dir", type=str, help="åŸºç¡€ç›®å½•ï¼ˆå°†å¤„ç†å…¶ä¸­æ‰€æœ‰ä»¥*_outputç»“å°¾çš„æ–‡ä»¶å¤¹ï¼‰")
    ap.add_argument("--out-dir", type=str, default="out_tree", help="è¾“å‡ºç›®å½•")
    ap.add_argument("--debug-log-dir", type=str, default="", help="è°ƒè¯•æ—¥å¿—ç›®å½•")
    ap.add_argument("--toc-window", type=int, default=DEFAULT_WINDOW_LINES, help="ç»“æ„æå–çª—å£å¤§å°ï¼ˆè¡Œï¼‰")
    ap.add_argument("--toc-stride", type=int, default=DEFAULT_STRIDE_LINES, help="ç»“æ„æå–æ­¥é•¿ï¼ˆè¡Œï¼‰")
    ap.add_argument("--max-retries", type=int, default=3, help="æœ€å¤§é‡è¯•æ¬¡æ•°")
    ap.add_argument("--timeout", type=int, default=DEFAULT_ARK_TIMEOUT, help="è¯·æ±‚è¶…æ—¶ï¼ˆç§’ï¼‰")
    ap.add_argument("--max-workers", type=int, default=DEFAULT_MAX_WORKERS, help="å¹¶å‘çº¿ç¨‹æ•°")
    args = ap.parse_args()

    # å¤„ç†è¾“å…¥ç›®å½•ï¼šæŸ¥æ‰¾æŒ‡å®šç›®å½•ä¸‹æ‰€æœ‰ä»¥*_outputç»“å°¾çš„æ–‡ä»¶å¤¹
    base_dir = Path(args.input_dir)
    if not base_dir.exists():
        print(f"âŒ åŸºç¡€ç›®å½•ä¸å­˜åœ¨ï¼š{base_dir}")
        return
    
    # æŸ¥æ‰¾æ‰€æœ‰ä»¥*_outputç»“å°¾çš„æ–‡ä»¶å¤¹
    import glob
    output_pattern = str(base_dir / "*_output")
    input_dirs = glob.glob(output_pattern)
    
    # è¿‡æ»¤å‡ºç›®å½•ï¼ˆæ’é™¤æ–‡ä»¶ï¼‰
    input_dirs = [d for d in input_dirs if Path(d).is_dir()]
    
    if not input_dirs:
        print(f"âŒ åœ¨ {base_dir} ä¸­æœªæ‰¾åˆ°ä»¥*_outputç»“å°¾çš„æ–‡ä»¶å¤¹")
        return
    
    print(f"ğŸ“‚ æ‰¾åˆ°åŒ¹é…çš„æ–‡ä»¶å¤¹ï¼š{input_dirs}")
    
    out_dir = Path(args.out_dir)
    debug_dir = Path(args.debug_log_dir) if args.debug_log_dir else None

    # æŸ¥æ‰¾æ‰€æœ‰.mdæ–‡ä»¶
    md_files = []
    for input_dir_path in input_dirs:
        input_dir = Path(input_dir_path)
        dir_md_files = list(input_dir.rglob("*.md"))
        md_files.extend(dir_md_files)
        print(f"ğŸ“„ åœ¨ {input_dir} ä¸­æ‰¾åˆ° {len(dir_md_files)} ä¸ª.mdæ–‡ä»¶")
    
    if not md_files:
        print(f"âŒ æœªæ‰¾åˆ°.mdæ–‡ä»¶")
        return
    print(f"ğŸ“„ æ€»å…±æ‰¾åˆ° {len(md_files)} ä¸ª.mdæ–‡ä»¶")

    out_dir.mkdir(parents=True, exist_ok=True)
    if debug_dir:
        debug_dir.mkdir(parents=True, exist_ok=True)

    # å¤„ç†æ–‡ä»¶
    all_toc_items = []
    all_sections = []
    total_stats = ExtractStats()

    # å¹¶å‘å¤„ç†
    print(f"ğŸš€ ä½¿ç”¨å¹¶å‘å¤„ç†ï¼Œçº¿ç¨‹æ•°ï¼š{args.max_workers}")
    with cf.ThreadPoolExecutor(max_workers=args.max_workers) as executor:
        futures = {}
        for md_path in md_files:
            # ç¡®å®šæ–‡ä»¶å±äºå“ªä¸ªè¾“å…¥ç›®å½•
            file_input_dir = None
            for input_dir_path in input_dirs:
                input_dir = Path(input_dir_path)
                try:
                    md_path.relative_to(input_dir)
                    file_input_dir = input_dir
                    break
                except ValueError:
                    continue
            
            if file_input_dir is None:
                print(f"âš ï¸ æ— æ³•ç¡®å®šæ–‡ä»¶ {md_path} çš„è¾“å…¥ç›®å½•")
                continue
            
            future = executor.submit(
                process_single_file,
                md_path=md_path,
                toc_window=args.toc_window,
                toc_stride=args.toc_stride,
                max_retries=args.max_retries,
                timeout=args.timeout,
                debug_dir=debug_dir,
                out_dir=out_dir,
                input_dir=file_input_dir
            )
            futures[future] = (md_path, file_input_dir)

        # ä½¿ç”¨æ›´å…¼å®¹çš„æ–¹å¼å¤„ç†è¿›åº¦æ¡
        completed = 0
        for fut in cf.as_completed(futures):
            completed += 1
            if completed % 10 == 0 or completed == len(futures):  # æ¯10ä¸ªæˆ–æœ€åä¸€ä¸ªæ˜¾ç¤ºè¿›åº¦
                print(f"ğŸ“Š å¤„ç†è¿›åº¦: {completed}/{len(futures)} ({completed/len(futures)*100:.1f}%)")
            md_path, file_input_dir = futures[fut]
            try:
                base_norm, toc_items, sections, stats = fut.result()
                
                # æ›´æ–°ç»Ÿè®¡
                total_stats.files_processed += stats.files_processed
                total_stats.files_copied += stats.files_copied
                total_stats.files_with_titles += stats.files_with_titles
                total_stats.title_items += stats.title_items
                total_stats.sections_extracted += stats.sections_extracted
                total_stats.missing_titles_found += stats.missing_titles_found
                total_stats.extra_titles_found += stats.extra_titles_found
                total_stats.correct_titles_found += stats.correct_titles_found
                total_stats.files_with_attachments += stats.files_with_attachments
                
                # ä¿å­˜ç»“æœ
                if toc_items or sections:
                    # è®¡ç®—ç›¸å¯¹è·¯å¾„ä»¥ä¿æŒæ–‡ä»¶å¤¹ç»“æ„
                    relative_path = md_path.relative_to(file_input_dir)
                    output_folder_name = file_input_dir.name
                    toc_path, sections_path = save_results(
                        base_norm, toc_items, sections, out_dir, relative_path, output_folder_name
                    )
                    print(f"âœ… {md_path.name} â†’ ç›®å½•é¡¹={len(toc_items)}, ç« èŠ‚={len(sections)}")
                    print(f"ğŸ“ ç›®å½•ï¼š{toc_path}")
                    print(f"ğŸ“„ ç« èŠ‚ï¼š{sections_path}")
                
                all_toc_items.extend(toc_items)
                all_sections.extend(sections)
                
            except Exception as e:
                print(f"âŒ å¤„ç†å¤±è´¥ï¼š{md_path} -> {e}")

    # è¾“å‡ºç»Ÿè®¡
    print("\n=== å¤„ç†å®Œæˆ ===")
    print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯ï¼š")
    print(f"   - å¤„ç†æ–‡ä»¶æ•°ï¼š{total_stats.files_processed}")
    print(f"   - æˆåŠŸå¤åˆ¶æ–‡ä»¶æ•°ï¼š{total_stats.files_copied}")
    print(f"   - æœ‰æ ‡é¢˜æ–‡ä»¶æ•°ï¼š{total_stats.files_with_titles}")
    print(f"   - æ£€æµ‹åˆ°é™„ä»¶æ–‡ä»¶æ•°ï¼š{total_stats.files_with_attachments}")
    print(f"âœ… æ‰€æœ‰æ–‡ä»¶å¤„ç†å®Œæˆï¼Œç»“æœä¿å­˜åœ¨å„æ–‡æ¡£æ–‡ä»¶å¤¹ä¸­")

if __name__ == "__main__":
    main()

# è¿è¡Œç¤ºä¾‹
'''
# ä½¿ç”¨ nohup åœ¨åå°è¿è¡Œï¼Œè¾“å‡ºé‡å®šå‘åˆ°æ—¥å¿—æ–‡ä»¶
nohup python 2_extract_tree.py \
  /mnt/data/projects/tmp \
  --out-dir /mnt/data/wx/xiaofang/out_xiaofang_title_$(date +%m%d%H%M) \
  --debug-log-dir /mnt/data/wx/xiaofang/debug_tree_logs_$(date +%m%d%H%M) \
  --toc-window 80 \
  --toc-stride 60 \
  --max-retries 3 \
  --timeout 120 \
  --max-workers 100 \
  > extract_tree_$(date +%m%d%H%M).log 2>&1 &

# æŸ¥çœ‹è¿è¡ŒçŠ¶æ€
tail -f extract_tree_$(date +%m%d%H%M).log

# æŸ¥çœ‹è¿›ç¨‹
ps aux | grep extract_tree
'''
